[
  {
    "objectID": "practicals/22_scms/index.html",
    "href": "practicals/22_scms/index.html",
    "title": "Practical: Structural Causal Models and Meta-learners",
    "section": "",
    "text": "In this practical you’ll learn more about identification and counterfactuals using the Structural Causal Model approach, and meta-learners\n\nIdentification\n\n\n\n\n\n\nRemember the definition of identification in the lecture on SCMs:\n\n\n\n\n\nLet \\(Q(M)\\) be any computable quantity of a model \\(M\\). We say that \\(Q\\) is identifiable in a class \\(\\mathbb{M}\\) of models if, for any pairs of models \\(M_1\\) and \\(M_2\\) from \\(\\mathbb{M}\\), \\(Q(M_1) = Q(M_2)\\) whenever \\(P_{M_1} (y) = P_{M_2} (y)\\). If our observations are limited and permit only a partial set \\(F_M\\) of features (of \\(P_M(y)\\)) to be estimated, we define \\(Q\\) to be identifiable from \\(F_M\\) if \\(Q(M_1) = Q(M_2)\\) whenever \\(F_{M_1} = F_{M_2}\\).\n\n\n\nWe have two different datasets, for which we know they came from the following DAG:\n\n\n\n\n\n\nFigure 1: DAG U\n\n\n\n\n\nCode\nrequired_pkgs &lt;- c(\"marginaleffects\", \"ggplot2\", \"data.table\")\ncran_repo &lt;- \"https://mirror.lyrahosting.com/CRAN/\" # &lt;- a CRAN mirror in the Netherlands, can select another one from here https://cran.r-project.org/mirrors.html\n\nfor (pkg in required_pkgs) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, repos=cran_repo)\n  }\n}\n\nsuppressPackageStartupMessages({\n  # Load packages\n  library(marginaleffects)\n  library(ggplot2)\n  library(data.table)\n})\n\nsource(here::here(\"practicals\", \"22_scms\", \"_makedatas.R\"))\ndatas &lt;- make_datas()\n\ndata1 &lt;- datas[[\"data1\"]]\ndata2 &lt;- datas[[\"data2\"]]\n\n\n\n\n\n\n\n\nThe datasets can alternatively be downloaded here:\n\n\n\n\n\n\n\n\ndata\nlink\n\n\n\n\ndata1\ndata1.csv\n\n\ndata2\ndata2.csv\n\n\n\n\n\n\n\n\n\n\n\n\nstate the ATE in terms of expected values of ‘do’ expressions\n\n\n\n\n\nanswer: \\[\\text{ATE} = E[Y|\\text{do}(X=1)] - E[Y|\\text{do}(X=0)] \\tag{1}\\]\n\n\n\n\n\n\n\n\n\nwe did not measure \\(U\\), can we estimate this target query based on the DAG, using the observed data\n\n\n\n\n\nanswer: no, there is an open back-door path through \\(U\\) which we cannot block as we did not observe that variable\n\n\n\ndata1 and data2 come from the same DAG but from different SCMs\n\n\n\n\n\n\nHow can this be? What does this mean?\n\n\n\n\n\nanswer: the endogenous variables have the same parents, so the DAG is the same. The structural equations are different\n\n\n\nWe can estimate four features of the observed distribution: \\(P(Y=1|X=0),P(Y=1|X=1),P(Y=1),P(X=1)\\). Observe that for data1 and data2, these are approximately the same (up to sampling variation)\n\n\n\n\n\n\nEstimate them from the observed data\n\n\n\n\n\n\n\nCode\nmean(data1[data1$x==0,\"y\"])\n\n\n[1] 0.3381743\n\n\nCode\nmean(data1[data1$x==1,\"y\"])\n\n\n[1] 0.6949807\n\n\nCode\nmean(data1[,\"y\"])\n\n\n[1] 0.523\n\n\nCode\nmean(data1[,\"x\"])\n\n\n[1] 0.518\n\n\nCode\nmean(data2[data2$x==0,\"y\"])\n\n\n[1] 0.2944664\n\n\nCode\nmean(data2[data2$x==1,\"y\"])\n\n\n[1] 0.6862348\n\n\nCode\nmean(data2[,\"y\"])\n\n\n[1] 0.488\n\n\nCode\nmean(data2[,\"x\"])\n\n\n[1] 0.494\n\n\n\n\n\n\n\n\n\n\n\nUse the fact that u is in the data to calculate the actual effect in both datasets, what are the answers?\n\n\n\n\n\n\n\nCode\nfit1 &lt;- glm(y~x*u, data=data1, family=binomial)\nfit2 &lt;- glm(y~x*u, data=data2, family=binomial)\navg_comparisons(fit1, variables=\"x\")\n\n\n\n Term          Contrast Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n    x mean(1) - mean(0) -0.00897     0.0675 -0.133    0.894 0.2 -0.141  0.123\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nCode\navg_comparisons(fit2, variables=\"x\")\n\n\n\n Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    x mean(1) - mean(0)    0.393      0.029 13.5   &lt;0.001 136.3 0.336   0.45\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\n\n\n\n\n\n\n\n\n\nExplain how this proves (to statistical error) that our target query was not identified\n\n\n\n\n\nanswer: there were two datasets with two different underlying models. Both yielded the same distribution in terms of observed variables \\(X,Y\\), but when using the unobserved variable \\(U\\), we could see both models had different answers to our query.\n\n\n\n\n\nCounterfactual computations\nUse the following information on patient John:\n\nage: 60\nhypertension: true\ndiabetes: true\nintervention: weight-loss program\nsurvival-time: 10\n\nIn addition to the following structural equation, where u denotes an (unobserved) exogenous noise variable, such that \\(E[u] = 0\\) (i.e. the mean is 0):\n\\[\\text{survival-time} = 120 - \\text{age} - 10*\\text{hypertension} - 15*\\text{diabetes} + 5*\\text{weight-loss-program} + u\\]\n\n\n\n\n\n\ncalculate the expected survival time for patients like John with and without the weight-loss program\n\n\n\n\n\nanswer:\n\\[\\begin{align}\n  E[\\text{survival-time}|\\text{do}(\\text{program}=0),...] &= E[120 - 60 - 10 - 15 + u] \\\\\n                                                   &= 120 - 60 - 10 - 15 + E[u] \\\\\n                                                   &= 35 + E[u] \\\\\n                                                   &= 35 + 0 \\\\\n                                                   &= 35 \\\\\n\\end{align}\\]\n\\[\\begin{align}\n  E[\\text{survival-time}|\\text{do}(\\text{program}=1),...] &= E[120 - 60 - 10 - 15 + 5 + u] \\\\\n                                                   &= 120 - 60 - 10 - 15 + 5 + E[u] \\\\\n                                                   &= 40 + E[u] \\\\\n                                                   &= 40 + 0 \\\\\n                                                   &= 40 \\\\\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nCalculate the survival time for John, given that he took the weight-loss-program and survived 10 year, if he would not have taken the weigth-loss-program\n\n\n\n\n\nanswer:\n\nstep 1. abduction: infer John’s u\nJohn’s expected survival time with the program (which he had) was 35 years. He lived for 10 years. We can infer that his \\(u=-25\\)\n\n\nstep 2. action: modify the treatment\nWe update his treatment status to ‘no weight-loss-program’, the formula is now the second answer to the previous question\n\n\nstep 3. predict:\nGiven John’s \\(u=-25\\) and his other observed values, we can now calculate that his expected survival time was \\(5\\) years if he would not have taken the weight-loss program.\n\n\n\n\n\n\n\n\n\nMeta-learners\n\n\n\n\n\n\nRemember the definition of the conditional average treatment effect (CATE) from lecture 4\n\n\n\n\n\n\\(\\text{CATE}(w) = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]\\)\n\n\n\n\n\n\n\n\n\nRembember the definition of the T-learner and the S-learner from lecture 4:\n\n\n\n\n\n\ndenote \\(\\tau(w) = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]\\)\nT-learner: model \\(T=0\\) and \\(T=1\\) separately (e.g. regression separetely for treated and untreated): \\[\\begin{align}\n  \\mu_0(w) &= E[Y|\\text{do}(T=0),W=w] \\\\\n  \\mu_1(w) &= E[Y|\\text{do}(T=1),W=w] \\\\\n  \\tau(w)  &= \\mu_1(w) - \\mu_0(w)\n\\end{align}\\]\nS-learner: use \\(T\\) as just another feature (assuming \\(W\\) is a sufficient set) \\[\\begin{align}\n  \\mu(t,w) &= E[Y|T=t,W=w] \\\\\n  \\tau(w)  &= \\mu(1,w) - \\mu(0,w)\n\\end{align}\\]\n\n\n\n\nWith the following datasets:\n\n\n\n\n\n\n\n\nFigure 2: Three datasets with the same DAG\n\n\n\n\n\n\n\n\n\n\n\nwhat learning-approach would you recommend for estimating the CATE?\n\n\n\n\n\n\nS-learner with simple basemodel and no interaction (e.g. linear regression)\nS-learner with non-linear base model and no interaction term (e.g. splines / boosting / …)\nT-learner\n\nNOTE: we typically have data with multi-dimensional features and/or confounders. Having the above plot to decide on the right meta-learning approach is almost never possible."
  },
  {
    "objectID": "practicals/drafts/draft.html",
    "href": "practicals/drafts/draft.html",
    "title": "Draft practical:",
    "section": "",
    "text": "In this practical, …\nFirst we load a package\nlibrary(survival)\nIn this practical, we will also use the following two packages:"
  },
  {
    "objectID": "practicals/drafts/draft.html#non-coding-questions",
    "href": "practicals/drafts/draft.html#non-coding-questions",
    "title": "Draft practical:",
    "section": "Non-coding questions",
    "text": "Non-coding questions\n\n\n\n\n\n\nQuestion Foo\n\n\n\n\n\nanswer: bar"
  },
  {
    "objectID": "practicals/drafts/draft.html#my-goals",
    "href": "practicals/drafts/draft.html#my-goals",
    "title": "Draft practical:",
    "section": "my goals",
    "text": "my goals"
  },
  {
    "objectID": "practicals/drafts/draft.html#conclusion",
    "href": "practicals/drafts/draft.html#conclusion",
    "title": "Draft practical:",
    "section": "Conclusion",
    "text": "Conclusion\nYou learned ABC"
  },
  {
    "objectID": "practicals/drafts/draft.html#further-reading",
    "href": "practicals/drafts/draft.html#further-reading",
    "title": "Draft practical:",
    "section": "Further reading",
    "text": "Further reading\nRead Chapter 9 of Pearls Causality (Pearl 2009)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#how-to-find-adjustment-sets",
    "href": "lectures/day2-scms/lec4.html#how-to-find-adjustment-sets",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "How to find adjustment sets?",
    "text": "How to find adjustment sets?\n\nadjustment sets:\n\nthe back-door criterion states that any set \\(Z\\) that blocks all backdoor paths from \\(X\\) to \\(Y\\) is a sufficient adjustment set for causal effect estimation of \\(P(Y|\\text{do}(X))\\) using the backdoor formula.\nhow do we find these sufficient sets?\nwhat if there are multiple?\n\nadjustment: how to do this?\n\nstratification\nwhat is regression adjustment?\nT-learner vs S-learner"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#valid-adjustment-sets",
    "href": "lectures/day2-scms/lec4.html#valid-adjustment-sets",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Valid adjustment sets",
    "text": "Valid adjustment sets\n\n\n\n\n\n\n\nin general:\n\n\\(PA_T\\) (the direct parents of treatment \\(T\\): \\(Z_1\\)) are a valid adjustment set\n\\(PA_Y\\) (the direct parents of outcome \\(Y\\): \\(Z_2\\)) are a valid adjustment set\n\nin this case:\n\n\\(W\\) is a valid adjustment set"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#valid-adjustment-sets-picking-one",
    "href": "lectures/day2-scms/lec4.html#valid-adjustment-sets-picking-one",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Valid adjustment sets: picking one",
    "text": "Valid adjustment sets: picking one\n\nwebsites like dagitty.net and causalfusion.net provide user-friendly interfaces for creating and exporting DAGs, in addition:\n\nvalid adjustment sets (if they exist)\ntestable conditional indepdencies"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#what-not-to-do",
    "href": "lectures/day2-scms/lec4.html#what-not-to-do",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "What not to do",
    "text": "What not to do\n\ndo univariable pre-screening against outcome (and / or treatment)\n\n\nthis should maybe never be done\nespecially not in the context of causal inference"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#adjustment-formula",
    "href": "lectures/day2-scms/lec4.html#adjustment-formula",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Adjustment formula",
    "text": "Adjustment formula\n\\[P(y|\\text{do}(x)) = \\sum_z P(y|x,z)P(z)\\]\n\nentails summing over all possible values of \\(Z\\)\nsay \\(Z\\) is 5 categorical variables with each 3 categories, this means \\(4^5=1024\\) estimates of:\n\n\\(P(y|x,z)\\) for each value of \\(x\\)\n\nwhat if \\(Z\\) is continuous?\nin practice, researchers rely on smoothness assumptions (e.g. regression) to estimate \\(P(Y|x,z)\\) with a parametric model\nthis assumption can be based on substantive causal knowledge, but often seems inspired rather pragmatism or necessity\nmisspecification of this estimator leads to biased results (even if you know all the confounders)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#target-queries",
    "href": "lectures/day2-scms/lec4.html#target-queries",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Target queries",
    "text": "Target queries\n\nup to now we’ve worked exclusively with \\(P(y|\\text{do}(t))\\): the probability of observing outcome \\(y\\) when setting treatment \\(T\\) to \\(t\\)\nthis is not typically what is of most interest, say there are two treatment options \\(T \\in \\{0,1\\}\\) (control and ‘treatment’)\n\naverage treatment effect \\[\\text{ATE} = E[y|\\text{do}(t=1)] - E[y|\\text{do}(t=0)]\\]\nconditional average treatment effect \\[\\text{CATE} = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]\\]\nprediction-under-intervention \\(P(y|\\text{do}(t),w)\\) (more on this on day 4)\n\nthese can be computed from \\(P(y|\\text{do}(t),w)\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#the-simplest-case-linear-regression",
    "href": "lectures/day2-scms/lec4.html#the-simplest-case-linear-regression",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "The simplest case: linear regression",
    "text": "The simplest case: linear regression\n\nassume the following structural causal model (\\(z\\) is confounder, \\(u\\) is exogenous noise): \\[f_y(t,z,u) = \\beta_t t + \\beta_z z + \\beta_u u\\]\nthen: \\[\\begin{align}\n  \\text{ATE} &= E[Y|\\text{do}(t=1)] - E[Y|\\text{do}(t=0)] \\\\\n             &\\class{fragment}{= E_{z,u}[\\beta_t * 1+ \\beta_z z + \\beta_u u] - E_{z,u}[\\beta_t * 0 + \\beta_z z + \\beta_u u]} \\\\\n             &\\class{fragment}{= \\beta_t + E_{z,u}[\\beta_z z + \\beta_u u] - E_{z,u}[\\beta_z z + \\beta_u u]} \\\\\n             &\\class{fragment}{= \\beta_t}\n\\end{align}\\]\ni.e. the ATE collapses to the the regression parameter \\(\\beta_t\\) in a linear regression model of \\(y\\) on \\(t,z\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#sec-metalearners",
    "href": "lectures/day2-scms/lec4.html#sec-metalearners",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "General estimators for the ATE and the CATE (meta-learners)",
    "text": "General estimators for the ATE and the CATE (meta-learners)\n\ndenote \\(\\tau(w) = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]\\)\nT-learner: model \\(T=0\\) and \\(T=1\\) separately (e.g. regression separetely for treated and untreated): \\[\\begin{align}\n  \\mu_0(w) &= E[Y|\\text{do}(T=0),W=w] \\\\\n  \\mu_1(w) &= E[Y|\\text{do}(T=1),W=w] \\\\\n  \\tau(w)  &= \\mu_1(w) - \\mu_0(w)\n\\end{align}\\]\nS-learner: use \\(T\\) as just another feature (assuming \\(W\\) is a sufficient set) \\[\\begin{align}\n  \\mu(t,w) &= E[Y|T=t,W=w] \\\\\n  \\tau(w)  &= \\mu(1,w) - \\mu(0,w)\n\\end{align}\\]\n(many other variants combinations: this is a whole literature)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#intuitive-way-pointers",
    "href": "lectures/day2-scms/lec4.html#intuitive-way-pointers",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Intuitive way-pointers:",
    "text": "Intuitive way-pointers:\n\nwhere does the complexity come from?\n\nvariance in outcome under control: \\(E[y|\\text{do}(T=0),w]\\)\nvariance CATE: \\(\\tau(w)\\) (in statistics: interaction between treatment and covariate)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#where-does-the-variance-come-from",
    "href": "lectures/day2-scms/lec4.html#where-does-the-variance-come-from",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Where does the variance come from?",
    "text": "Where does the variance come from?\n\n\n\n\n\n\n\n\n\nDAG\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Three datasets with the same DAG\n\n\n\n\n\n\n\n\n\\(Y = T + 0.5 (X - \\pi) + \\epsilon\\) (linear)\n\\(Y = T + \\sin(X) + \\epsilon\\) (non-linear additive)\n\\(Y = T * \\sin(X) - (1-T) \\sin(x) + \\epsilon\\) (non-linear + interaction)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#making-dags",
    "href": "lectures/day2-scms/lec4.html#making-dags",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Making DAGs",
    "text": "Making DAGs\n\nhow do you get a DAG? up to now we assumed we had one\nbased on prior evidence, expert knowledge\n“no causes in, no causes out”"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#a003024-the-death-of-dags",
    "href": "lectures/day2-scms/lec4.html#a003024-the-death-of-dags",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "A003024: The death of DAGs?",
    "text": "A003024: The death of DAGs?\nThe number of possible DAGs grows super-exponentially in the number of nodes\n\n\n\nn_nodes\nn_dags\ntime at 1 sec / DAG\n\n\n\n\n1\n1\n\n\n\n2\n3\n\n\n\n3\n25\n\n\n\n4\n543\n\n\n\n5\n29281\n&gt; an hour\n\n\n6\n3781503\n&gt; a day\n\n\n7\n1138779265\n&gt; a year\n\n\n8\n783702329343\n\n\n\n9\n1213442454842881\n&gt; human species\n\n\n10\n4175098976430598143\n&gt; age of universe"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#do-we-need-to-consider-all-dags",
    "href": "lectures/day2-scms/lec4.html#do-we-need-to-consider-all-dags",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Do we need to consider all DAGs?",
    "text": "Do we need to consider all DAGs?\n\na single sufficient set suffices\nadjusting for all direct causes of the treatment or all direct causes of the outcome are always sufficent sets\ncan we judge these without specifying all covariate-covariate relationships?\npotential approach:\n\nput all potential confounders in a cluster (e.g Anand et al. 2023)\nignore covariate-covariate relationships in that cluster\nwhat happens when (partial) missing data?"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#references",
    "href": "lectures/day2-scms/lec4.html#references",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "References",
    "text": "References\n\n\n\n\nWouter van Amsterdam — WvanAmsterdam — vanamsterdam.github.io\n\n\n\n\nAnand, Tara V., Adele H. Ribeiro, Jin Tian, and Elias Bareinboim. 2023. “Causal Effect Identification in Cluster DAGs.” Proceedings of the AAAI Conference on Artificial Intelligence 37 (10): 12172–79. https://doi.org/10.1609/aaai.v37i10.26435."
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#in-past-lectures-on-dags",
    "href": "lectures/day2-scms/lec3-scms.html#in-past-lectures-on-dags",
    "title": "Structural Causal Models",
    "section": "In past lectures on DAGs",
    "text": "In past lectures on DAGs\n\ncausal directed acyclic graphs (DAGs) encode assumptions on what variables cause what\nan intervention is defined as a mutilation of this DAG where the treatment variable no longer ‘listens’ to its parents\na causal effect is the effect of an intervention\nDAG patterns:\n\nfork (confounding)\nchain (mediation)\ncollider\n\ntypically:\n\n\ncondition on confounders, don’t condition on mediators or colliders\n\n\nin more complex DAGs, use d-separation to check identifyability\nbackdoor criterion"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#in-this-lecture-structural-causal-models-scms",
    "href": "lectures/day2-scms/lec3-scms.html#in-this-lecture-structural-causal-models-scms",
    "title": "Structural Causal Models",
    "section": "In this lecture: structural causal models (SCMs)",
    "text": "In this lecture: structural causal models (SCMs)\n\n\n\n\n\n\n\\[\\begin{align}\n  U_Z, U_T, U_Y &\\sim p(U) \\\\\n  Z &= f_Z(U_Z) \\\\\n  T &= f_T(Z,U_T) \\\\\n  Y &= f_Y(T,Z,U_Y)\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#why-scms",
    "href": "lectures/day2-scms/lec3-scms.html#why-scms",
    "title": "Structural Causal Models",
    "section": "Why SCMs?",
    "text": "Why SCMs?\n\nWith DAGs we can:\n\nexpress (non-parametric) prior knowledge\nunderstand that seeing \\(\\neq\\) doing\nknow what variables to condition on for estimating treatment effect\n\nHowever,\n\nDAGs and RCTs do not cover all causal questions\nSCMs go a level deeper than DAGs\nDAGs naturally ‘arise’ from SCMs\nsome questions are not identified when only specifying a DAG, but we may have additional information that can lead to identification\nunderstand ‘identifyability’\nSCM thinking aligns [^according to me] with physical thinking about the world and is a natural way to think about causality"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#topics-of-today",
    "href": "lectures/day2-scms/lec3-scms.html#topics-of-today",
    "title": "Structural Causal Models",
    "section": "Topics of today",
    "text": "Topics of today\n\nSCMs: the world as computer programs\ninterventions are submodels\nbonus queries:\n\ncounterfactuals\n\nPearl Causal Hierarchy\nother uses of DAGs: missing data, selection\nreflections on DAGs, limitations"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#think-of-the-world-as-a-computer-program-with-a-set-of",
    "href": "lectures/day2-scms/lec3-scms.html#think-of-the-world-as-a-computer-program-with-a-set-of",
    "title": "Structural Causal Models",
    "section": "Think of the world as a computer program with a set of",
    "text": "Think of the world as a computer program with a set of\n\n(endogenous) variables:\n\nsurgery = duration of surgery (hours)\nlos = length of stay in hospital post surgery (days)\nsurvival = survival time (years)\n\nbackground variables (exogenous):\n\nu_surgery, u_los, u_survival\n\nfunctions f_ for each variable which depend on its parents pa_ and its own background u_:\n\nsurgery = f_surgery(pa_surgery,u_surgery)\nlos = f_los(pa_los, u_los)\nsurvival = f_survival(pa_survival, u_survival)\n\n\n\n\nTogether these define a Structural Causal Model (see definition 7.1.1 in Pearl 2009, and further) (notation: \\(M=&lt;U,V,F&gt;\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#structural-causal-model-1",
    "href": "lectures/day2-scms/lec3-scms.html#structural-causal-model-1",
    "title": "Structural Causal Models",
    "section": "Structural Causal Model 1",
    "text": "Structural Causal Model 1\n\nf_surgery &lt;- function(u_surgery) { # pa_surgery = {}\n  u_surgery\n}\nf_los &lt;- function(surgery, u_los) { # pa_los = {surgery}\n  surgery + u_los\n}\nf_survival &lt;- function(surgery, los, u_survival) { # pa_survival = {sugery, los}\n  survival = los - 2 * surgery + u_survival\n}\n\nscm1 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery  = f_surgery(u_surgery)\n  los      = f_los(surgery, u_los)\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}\nscm1(2, 1, 5)\n\n\n\n surgery      los survival \n       2        3        4"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph",
    "href": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph",
    "title": "Structural Causal Models",
    "section": "Recursive Structural Causal Models imply a Directed Acyclic Graph",
    "text": "Recursive Structural Causal Models imply a Directed Acyclic Graph\nAn SCM is recursive, i.e. acyclic when following the chain of parents, you never end up at the same variable twice\n\n\n\nscm1 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery  = f_surgery(u_surgery)\n  los      = f_los(surgery, u_los)\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph-1",
    "href": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph-1",
    "title": "Structural Causal Models",
    "section": "Recursive Structural Causal Models imply a Directed Acyclic Graph",
    "text": "Recursive Structural Causal Models imply a Directed Acyclic Graph\nAn SCM is recursive, i.e. acyclic when following the chain of parents, you never end up at the same variable twice\n\n\n\nscm1 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery  = f_surgery(u_surgery)\n  los      = f_los(surgery, u_los)\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph-2",
    "href": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph-2",
    "title": "Structural Causal Models",
    "section": "Recursive Structural Causal Models imply a Directed Acyclic Graph",
    "text": "Recursive Structural Causal Models imply a Directed Acyclic Graph\nAn SCM is recursive, i.e. acyclic when following the chain of parents, you never end up at the same variable twice\n\n\n\nscm1 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery  = f_surgery(u_surgery)\n  los      = f_los(surgery, u_los)\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}\n\n\n\n\n\nscm1 (without specifying the f_s) and the DAG are equivalent (they describe the same knowledge of the world)\nfor the remainder, we assume recursiveness"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#submodel-and-effect-of-action",
    "href": "lectures/day2-scms/lec3-scms.html#submodel-and-effect-of-action",
    "title": "Structural Causal Models",
    "section": "Submodel and Effect of Action",
    "text": "Submodel and Effect of Action\n\nsubmodel: in scm1 replace f_los with a specific value, e.g. 7 days \n\n\nsubmodel7 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery = f_surgery(u_surgery)\n  los = 7\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}\n\nsubmodel7(2, 1, 5)\n\n\n\n surgery      los survival \n       2        7        8 \n\n\n\neffect of action: resulting SCM of submodel (notation: \\(M_x=&lt;U,V,F_x&gt;\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#submodel-and-effect-of-action-as-a-mutilated-dag",
    "href": "lectures/day2-scms/lec3-scms.html#submodel-and-effect-of-action-as-a-mutilated-dag",
    "title": "Structural Causal Models",
    "section": "Submodel and Effect of Action as a mutilated DAG",
    "text": "Submodel and Effect of Action as a mutilated DAG\nIn scm1 replace f_los with a specific value, e.g. 7 days (notation: \\(M_x\\))\n\n\n\nsubmodel7 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery = f_surgery(u_surgery)\n  los = 7\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}\n\nsubmodel7(2, 1, 5)\n\n surgery      los survival \n       2        7        8 \n\n\n\n\n\n\n\n\n\nThe DAG describes a submodel where \\(T\\) no longer ‘listens’ to any variables but is controlled to be equal to a specific value (e.g. 7)\nThe Effect of Action \\(do(X=x)\\) is defined as the submodel \\(M_x\\)."
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#specifying-a-distribution-for-exogenous-variables-u",
    "href": "lectures/day2-scms/lec3-scms.html#specifying-a-distribution-for-exogenous-variables-u",
    "title": "Structural Causal Models",
    "section": "Specifying a distribution for exogenous variables U",
    "text": "Specifying a distribution for exogenous variables U\n\nExogenous variables U represent random variation in the world.\nWe can specify a distribution for them (e.g. Gaussian, Uniform)\n\n\n\nsample_u &lt;- function() {\n    u_surgery  = runif(1,  2,  8)\n    u_los      = runif(1, -1,  7)\n    u_survival = runif(1,  8, 13)\n    c(u_surgery=u_surgery, u_los=u_los, u_survival=u_survival)\n}\nsample_u()\n\n\n\n u_surgery      u_los u_survival \n  4.317931   3.909789  11.331566 \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: 1000 random samples of U"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#a-probabilistic-causal-model-is-a-scm-with-a-distribution-over-u",
    "href": "lectures/day2-scms/lec3-scms.html#a-probabilistic-causal-model-is-a-scm-with-a-distribution-over-u",
    "title": "Structural Causal Models",
    "section": "A Probabilistic Causal Model is a SCM with a distribution over U",
    "text": "A Probabilistic Causal Model is a SCM with a distribution over U\n\nsample_pcm &lt;- function() {\n  U &lt;- sample_u()\n  V &lt;- scm1(U[['u_surgery']], U[['u_los']], U[['u_survival']])\n  c(U, V)\n}\n  \nsample_pcm()\n\n\n\n u_surgery      u_los u_survival    surgery        los   survival \n  2.034069   5.222650  12.690868   2.034069   7.256719  15.879449 \n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Realisations of endogenous variables V over random samples of U in Figure 1"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#calculating-a-treatment-effect-in-a-fully-specified-probabilistic-causal-model",
    "href": "lectures/day2-scms/lec3-scms.html#calculating-a-treatment-effect-in-a-fully-specified-probabilistic-causal-model",
    "title": "Structural Causal Models",
    "section": "Calculating a treatment effect in a fully specified probabilistic causal model",
    "text": "Calculating a treatment effect in a fully specified probabilistic causal model\n\ntake random samples from U, push forward through submodel7 and submodel3\n\n\n# N = 1e3\n# us &lt;- map(1:N, ~sample_u())\n\nv3s &lt;- map(us, ~do.call(submodel3, as.list(.x)))\nv7s &lt;- map(us, ~do.call(submodel7, as.list(.x)))\n\nv3df &lt;- v3s |&gt; map(~data.table(t(.x))) |&gt; rbindlist()\nv7df &lt;- v7s |&gt; map(~data.table(t(.x))) |&gt; rbindlist()\nv3df[, idx:=.I]\nv7df[, idx:=.I]\n\ndfa &lt;- rbindlist(list(\n  scm1=vdf,\n  submodel3=v3df,\n  submodel7=v7df\n), idcol='model')\n\ndfa[, list(mean_survival=mean(survival)), by=\"model\"]\n\n\n\n       model mean_survival\n      &lt;char&gt;         &lt;num&gt;\n1:      scm1      8.613519\n2: submodel3      3.585969\n3: submodel7      7.585969"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#recap-of-definitions",
    "href": "lectures/day2-scms/lec3-scms.html#recap-of-definitions",
    "title": "Structural Causal Models",
    "section": "Recap of definitions",
    "text": "Recap of definitions\n\nStructural Causal model:\n\nendogenous variables \\(V\\)\nexogenous (noise) variables \\(U\\)\ndeterministic functions f_i(pa_i,u_i)\n\nEffect of Action do\\((T=t)\\): submodel where f_T replaced with fixed value t\nProbabilistic Causal Model: SCM + distribution over U"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#in-the-real-world",
    "href": "lectures/day2-scms/lec3-scms.html#in-the-real-world",
    "title": "Structural Causal Models",
    "section": "In the real world",
    "text": "In the real world\n\nknowing the SCM is a super-power: you basically know everything revelant about the system, but in the real world:\nwe do not observe \\(U\\)\nwe typically do not know f_\n\nwe may be willing to place assumptions on f_ (e.g. generalized linear models)\n\nwe are presented with realizations \\(V_i\\) of this SCM over a random sample of U\n\nthis is another assumption on the sampling but this is largely orthogonal to causal inference\n\nwe may be interest in knowing:\n\nwhat is the expected survival time if we always admit patients for exactly 7 days?\n\n\n\nWhen and how might we learn the answer to such questions?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#sec-identification",
    "href": "lectures/day2-scms/lec3-scms.html#sec-identification",
    "title": "Structural Causal Models",
    "section": "Identification",
    "text": "Identification\nCausal effect identification:"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#definition-3.2.3-identifiability",
    "href": "lectures/day2-scms/lec3-scms.html#definition-3.2.3-identifiability",
    "title": "Structural Causal Models",
    "section": "Definition 3.2.3 (Identifiability)",
    "text": "Definition 3.2.3 (Identifiability)\nLet \\(Q(M)\\) be any computable quantity of a model \\(M\\).\n\nWe say that \\(Q\\) is identifiable in a class \\(\\mathbb{M}\\) of models if, for any pairs of models \\(M_1\\) and \\(M_2\\) from \\(\\mathbb{M}\\),\n\n\n\\(Q(M_1) = Q(M_2)\\) whenever \\(P_{M_1} (y) = P_{M_2} (y)\\).\n\n\nIf our observations are limited and permit only a partial set \\(F_M\\) of features (of \\(P_M(y)\\)) to be estimated,\n\n\nwe define \\(Q\\) to be identifiable from \\(F_M\\) if \\(Q(M_1) = Q(M_2)\\) whenever \\(F_{M_1} = F_{M_2}\\)."
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#idenfitication-in-pictures",
    "href": "lectures/day2-scms/lec3-scms.html#idenfitication-in-pictures",
    "title": "Structural Causal Models",
    "section": "Idenfitication in pictures",
    "text": "Idenfitication in pictures\n\nSomeone killed the priest (†), we want to know who-dunnit (\\(=Q\\))\n\nBased on prior knowledge on 5 suspects (all the SCMs compatible with our DAG)\n\n\n\n\n\nIf we had full data, we would know it was \\(M_3\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#idenfitication-in-pictures-1",
    "href": "lectures/day2-scms/lec3-scms.html#idenfitication-in-pictures-1",
    "title": "Structural Causal Models",
    "section": "Idenfitication in pictures",
    "text": "Idenfitication in pictures\nSomeone killed the priest {{&lt; iconify ph:knife &gt;}} , we want to know who-dunnit (\\(=Q\\))\nBased on prior knowledge on 5 suspects (all the SCMs compatible with our DAG)\n\nIf we had full data, we would have know it was \\(M_3\\)\nUnfortunately, it was dark an we only got a gray-scale image of the perpetrator\n\nAll our suspects (models) lead to the same partial observations\n\n\nBased on observed data and assumptions we cannot identify the answer to our question \\(Q\\),\n\n\ni.e. multiple models with different answers for \\(Q\\) fit the observed data equally well"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#not-identified-vs-estimand",
    "href": "lectures/day2-scms/lec3-scms.html#not-identified-vs-estimand",
    "title": "Structural Causal Models",
    "section": "Not identified vs estimand",
    "text": "Not identified vs estimand\n\nThe backdoor adjustment in this DAG means the correct estimand is:\n\\[\\begin{align}\n  P(Y|\\text{do}(T)) &= \\sum_{z} P(Y|T,z)P(Z=z)\n\\end{align}\\]\n\nIf we did not observe \\(Z\\), we could still come up with a latent-variable model for \\(Z\\) and a model for \\(Y|T,Z\\) and get a value.\nHowever, we can formulate multiple distinct latent variable models that each yield a different treatment effect (i.e. the output of the estimand)\nBut these latent variable models all fit the observed data equally well\nSo we cannot identify the treatment effect"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#seeing-is-not-doing",
    "href": "lectures/day2-scms/lec3-scms.html#seeing-is-not-doing",
    "title": "Structural Causal Models",
    "section": "Seeing is not doing",
    "text": "Seeing is not doing\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: \\[\\begin{align}\n  P(Y|T) &= \\sum_{z} P(Y|T,z)P(Z=z|T)\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n  P(Y|T) &= \\sum_{z} P(Y|T,z)P(Z=z|T) \\\\\n         &=^2 \\sum_{z} P(Y|T,z)P(Z=z)\n\\end{align}\\]\n\n\nFigure 4: \\(^2\\) because in the intervened DAG, \\(Z\\) is independent of \\(T\\)\n\n\n\n\n\n\n\n\n\\(P(Y|\\text{do}(T)) \\neq P(Y|T)\\) is Pearl’s definition of confounding (def 6.2.1)\nthis shows why RCTs are special (i.e. no backdoor paths into \\(T\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#another-path-to-identification-parametric-assumptions",
    "href": "lectures/day2-scms/lec3-scms.html#another-path-to-identification-parametric-assumptions",
    "title": "Structural Causal Models",
    "section": "Another path to identification: parametric assumptions",
    "text": "Another path to identification: parametric assumptions\n\nfor example:\n\nassumption 1: \\(\\mathbb{M}_1\\), all SCMs with same DAG\nassumption 2: \\(\\mathbb{M}_2\\) SCMs with linear functions and Gaussian error terms\nassumption 1+2: \\(\\mathbb{M} = \\mathbb{M_1} \\cap \\mathbb{M_2}\\) (DAG + linear gaussian)\n\nmany more effects are identified in this setting\n‘works’ with unobserved confounding, positivity violations\ncaveats:\n\nmuch harder to determine identifyability (no backdoor-rule)\nprefer weaker assumptions over stronger assumption"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#counterfactuals",
    "href": "lectures/day2-scms/lec3-scms.html#counterfactuals",
    "title": "Structural Causal Models",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nall of the above can be achieved with DAGs, but we haven’t used SCMs super-power yet: counterfactuals\nRCT / DAG questions: What is the expected survival if we keep all patients in the hospital for 7 days?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#take-it-one-level-higher-counterfactuals",
    "href": "lectures/day2-scms/lec3-scms.html#take-it-one-level-higher-counterfactuals",
    "title": "Structural Causal Models",
    "section": "Take it one level higher: counterfactuals",
    "text": "Take it one level higher: counterfactuals\n\n\n\n\n\n\nFor patient Adam we had this data:\n\nsurgery duration: 4 hours\nlength of stay: 3 days\nsurvival: 4 years\n\n\nFor patient Zoe we had this data:\n\nsurgery duration: 4 hours\nlength of stay: 3 days\nsurvival: 7.5 years\n\n\n\n\nwe do not observe Adam’s/Zoe’s U\nWhat would the expected survival have been had Adam/Zoe been kept in the hospital for 7 days?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#adam-versus-zoe",
    "href": "lectures/day2-scms/lec3-scms.html#adam-versus-zoe",
    "title": "Structural Causal Models",
    "section": "Adam versus Zoe",
    "text": "Adam versus Zoe\n\nAverage causal effects in subgroup with surgery=4:\n\n3-days LOS: 5.7\n7-days LOS: 9.7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhat do we expect for Adam and Zoe if they would have been kept in the hospital for 7 days?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#computing-counterfactuals-with-scms",
    "href": "lectures/day2-scms/lec3-scms.html#computing-counterfactuals-with-scms",
    "title": "Structural Causal Models",
    "section": "Computing counterfactuals with SCMs",
    "text": "Computing counterfactuals with SCMs\n\nGiven our information on the structural equation for survival: \\[\\text{survival} = \\text{los} - 2*\\text{surgery} + u_{\\text{survival}}\\]\nand observed values on Adam’s and Zoe’s surgery AND survival following los=3\nwe can compute their individual \\(u_{\\text{survival}}\\):\n\n\n\n\n\n\npatient\nsurgery\nlos\nsurvival\n\n\n\n\nAdam\n4\n3\n4\n\n\nZoe\n4\n3\n7.5\n\n\n\n\n\n\n\n\npatient\nsurgery\nlos\nsurvival\nu_survival\n\n\n\n\nAdam\n4\n3\n4\n9\n\n\nZoe\n4\n3\n7.5\n12.5\n\n\n\n\n\n\n\n\npatient\nsurgery\nlos\nsurvival\nu_survival\nsurvival7\n\n\n\n\nAdam\n4\n3\n4\n9\n8\n\n\nZoe\n4\n3\n7.5\n12.5\n11.5\n\n\n\n\n\n\nand (counterfactual) survival under 7 days LOS"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#computing-counterfactuals",
    "href": "lectures/day2-scms/lec3-scms.html#computing-counterfactuals",
    "title": "Structural Causal Models",
    "section": "Computing counterfactuals",
    "text": "Computing counterfactuals\n\nnotation: \\(P(Y_{t'}  = y' | T=t,Y=y)\\) where \\(Y_{t'}\\) means “set \\(T=t'\\) through intervention”\nsteps:\n\nAbduction (update \\(P(U)\\) from observed evidence)\nAction (modify the treatment)\nPrediction (calculate outcomes in submodel, putting in the updated \\(P(U)\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#pearls-causal-hierarchy-of-questions",
    "href": "lectures/day2-scms/lec3-scms.html#pearls-causal-hierarchy-of-questions",
    "title": "Structural Causal Models",
    "section": "Pearl’s Causal Hierarchy (of questions)",
    "text": "Pearl’s Causal Hierarchy (of questions)\nIf you have data to solve the upper, you can solve the lower ranks too (Bareinboim et al. 2022)\n\ncounterfactuals\ninterventions\nassociations"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#where-do-we-get-this-knowledge-from",
    "href": "lectures/day2-scms/lec3-scms.html#where-do-we-get-this-knowledge-from",
    "title": "Structural Causal Models",
    "section": "Where do we get this knowledge from?",
    "text": "Where do we get this knowledge from?\n\nnot from observational data\nnot from RCTs\nfrom assumptions\ncan get bounds from combinations of RCT data and observational data\ncaveat: some say the hierarchy is upside down because you go further away from data and closer to unverifiable assumptions the ‘higher’ you get"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#not-covered-but-also-possible",
    "href": "lectures/day2-scms/lec3-scms.html#not-covered-but-also-possible",
    "title": "Structural Causal Models",
    "section": "Not covered but also possible:",
    "text": "Not covered but also possible:\n\nDAGs:\n\nsoft intervention: don’t set treatment to fixed value but replace function with other function of variables\nexpress patterns for missing data by including missingness indicators\n\nSCMs:\n\nprobability of sufficiency\nprobability of necessity"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#references",
    "href": "lectures/day2-scms/lec3-scms.html#references",
    "title": "Structural Causal Models",
    "section": "References",
    "text": "References\n\n\n\n\nWouter van Amsterdam — WvanAmsterdam — vanamsterdam.github.io\n\n\n\n\nBareinboim, Elias, Juan Correa, Duligur Ibeling, and Thomas Icard. 2022. “On Pearl’s Hierarchy and the Foundations of Causal Inference (1st Edition).” In Probabilistic and Causal Inference: The Works of Judea Pearl, edited by Hector Geffner, Rita Dechter, and Joseph Halpern, 507–56. ACM Books.\n\n\nPearl, Judea, ed. 2009. “The Logic of Structure-Based Counterfactuals.” In Causality, 2nd ed., 201–58. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511803161.009."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "",
    "text": "Dates: Aug 5th - Aug 10th 2024\nplease have a look at the setup-document here before the first day and make sure you have a working R installation with the required packages"
  },
  {
    "objectID": "index.html#course-objectives",
    "href": "index.html#course-objectives",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Course objectives",
    "text": "Course objectives\nLearn causal inference and causal data science!"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Schedule",
    "text": "Schedule\n\nDay 1: Intro & Potential Outcomes\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n09:00 - 09:30\nLecture\nCausal Inference: What, Why, and How\npdf\n\n\n09:30 - 10:45\nLecture\nIntro to Potential Outcomes I\n\n\n\n11:00 - 12:15\nPractical\nCausal assumptions\nhtml\n\n\n12:30 - 13:15\nLUNCH\n\n\n\n\n13:15 - 14:00\nLecture\nIntro to Potential Outcomes II\n\n\n\n14:15 - 15:00\nLecture\nAdjustment Methods I: Stratification, Matching and Propensity Scores\n\n\n\n15:15 - 16:30\nPractical\nAdjustment Methods I\nhtml\n\n\n\n\n\nDay 2: DAGs and SCMs\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n09:00 - 09:45\nLecture\nIntro to DAGs I\nhtml pdf\n\n\n09:45 - 10:45\nLecture\nIntro to DAGs II\n\n\n\n11:00 - 12:00\npractical\nDrawing and Using DAGs I\nhtml\n\n\n12:00 - 13:00\nLUNCH\n\n\n\n\n13:00 - 14:00\nLecture\nStructural Causal Models\nhtml pdf\n\n\n14:15 - 15:00\nLecture\nAdjustment Methods II: Regression and Outcome Adjustment\nhtml pdf\n\n\n15:15 - 16:30\npractical\nSCMs and meta-learners\nhtml\n\n\n\n\n\nDay 3: Target Trial Emulation\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n09:00 - 09:45\nLecture\nIntro to Trials and Target Trials I\npdf\n\n\n10:00 - 10:45\nLecture\nTarget Trials Emulation I\n\n\n\n11:00 - 12:00\npractical\nTarget Trials in Practice I\n\n\n\n12:30 - 13:30\nLUNCH\n\n\n\n\n13:30 - 14:15\nLecture\nTarget Trials Emulation II\n\n\n\n14:30 - 15:00\nLecture\nTarget Trials Emulation III\n\n\n\n15:15 - 16:30\npractical\nTarget Trials in practice II\n\n\n\n\n\n\nDay 4: Causal Data Science\n\n\n\n\n\n\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n09:00 - 09:45\nLecture\nCausal Perspectives on Prediction Modeling I\n\n\n\n10:00 - 10:45\nLecture\nCausal Perspectives on Prediction Modeling II\n\n\n\n11:00 - 12:00\npractical\nCausal Perspectives on Prediction Modeling\n\n\n\n12:00 - 13:30\nLUNCH\n\n\n\n\n13:30 - 14:15\nLecture\nCausal Structure Learning I\n\n\n\n14:30 - 15:00\nLecture\nCausal Structure Learning II\n\n\n\n15:15 - 16:30\npractical\nCausal Structure Learning\n\n\n\n\n\n\nDay 5: Advanced Topics in Causal Inference\n\n\n\n\n\n\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n09:00 - 09:45\nLecture\nMediation, Instrumental Variables and DAGs in Longitudinal settings\n\n\n\n10:00 - 10:45\nLecture\nWhen traditional methods fail\n\n\n\n11:00 - 12:00\nLecture\nCausal Inference in Quasi-Experimental and Policy Evaluation settings\n\n\n\n12.00 - 12.45\n\nQ&A and borrel"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Instructors",
    "text": "Instructors\n\nOisín Ryan (coordinator)\nBas Penning-de Vries\nWouter van Amsterdam"
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Links",
    "text": "Links\n\nCourse home on utrechtsummerschool.nl"
  },
  {
    "objectID": "index.html#license-disclaimer",
    "href": "index.html#license-disclaimer",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "License & disclaimer",
    "text": "License & disclaimer\nAll course materials are licensed under CC-BY-4.0.\n \nThese course materials were developed with great care. If you find any inaccuracies please contact us."
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#todays-lectures",
    "href": "lectures/day2-scms/lec1.html#todays-lectures",
    "title": "Causal Directed Acylic Graphs",
    "section": "Today’s lectures",
    "text": "Today’s lectures\n\nintroduce 1.5 new framework based on\n\ncausal Directed Acyclic Graphs (DAGs)\nStructral Causal Models (SCMs)\n\ncounterfactuals and Pearl’s Causal Hierarchy of questions\nlectures will follow Pearl’s book Causality Pearl (2009), specifically chapters 3 (DAGs) and 7 (SCMs)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#causal-inference-frameworks",
    "href": "lectures/day2-scms/lec1.html#causal-inference-frameworks",
    "title": "Causal Directed Acylic Graphs",
    "section": "Causal inference frameworks",
    "text": "Causal inference frameworks\nWhat are they for?\nMathematical language to\n\ndefine causal quantities\nexpress assumptions\nderive how to estimate causal quantities"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#causal-inference-frameworks-1",
    "href": "lectures/day2-scms/lec1.html#causal-inference-frameworks-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Causal inference frameworks",
    "text": "Causal inference frameworks\nWhy learn more than one?\n\n\nOn day 1 we learned about the Potential Outcomes framework\n\nDefines causal effects in terms of (averages of) individual potential outcomes\nEstimation requires assumptions of (conditional) exchangeability and positivity / overlap and consistency\n\nThere isn’t only 1 way to think about causality, find one that ‘clicks’\nNow we will learn another framework: Structural Causal Models and causal graphs\n\ncausal relations and manipulations of variables\nDeveloped by different people initially - Judea Pearl, Peter Spirtes, Clark Glymour\nSCM approach is broader in that it can define more different types of causal questions\n\nEquivalence: given the same data and assumptions, get the same estimates"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#lecture-1-2-topics",
    "href": "lectures/day2-scms/lec1.html#lecture-1-2-topics",
    "title": "Causal Directed Acylic Graphs",
    "section": "Lecture 1 & 2 topics",
    "text": "Lecture 1 & 2 topics\n\nmotivating examples for DAGs\nwhat are DAGs\ncausal inference with DAGs\n\nwhat is an intervention\nDAG-structures: confounding, mediation, colliders\nd-separation\nback-door criterion"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#sec-example-delivery",
    "href": "lectures/day2-scms/lec1.html#sec-example-delivery",
    "title": "Causal Directed Acylic Graphs",
    "section": "Example task: are hospital deliveries good for babies?",
    "text": "Example task: are hospital deliveries good for babies?"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#example-task-are-hospital-deliveries-good-for-babies",
    "href": "lectures/day2-scms/lec1.html#example-task-are-hospital-deliveries-good-for-babies",
    "title": "Causal Directed Acylic Graphs",
    "section": "Example task: are hospital deliveries good for babies?",
    "text": "Example task: are hospital deliveries good for babies?\n\nYou’re a data scientist in a children’s hospital\nHave data on\n\ndelivery location (home or hospital)\nneonatal outcomes (good or bad)\npregnancy risk (high or low)\n\nQuestion: do hospital deliveries result in better outcomes for babies?"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#observed-data",
    "href": "lectures/day2-scms/lec1.html#observed-data",
    "title": "Causal Directed Acylic Graphs",
    "section": "Observed data",
    "text": "Observed data\n\npercentage of good neonatal outcomes\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nrisk\nlow\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nhigh\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\nbetter outcomes for babies delivered in the hospital for both risk groups"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#observed-data-1",
    "href": "lectures/day2-scms/lec1.html#observed-data-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Observed data",
    "text": "Observed data\n\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nrisk\nlow\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nhigh\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\n\n\n\n\n\nmarginal\n688 / 800 = 86%\n163 / 200 = 81.5%\n\n\n\n\nbetter outcomes for babies delivered in the hospital for both risk groups\nbut not better marginal (‘overall’)\nhow is this possible? (a.k.a. simpsons paradox)\nwhat is the correct way to estimate the effect of delivery location?"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#new-question-hernia",
    "href": "lectures/day2-scms/lec1.html#new-question-hernia",
    "title": "Causal Directed Acylic Graphs",
    "section": "New question: hernia",
    "text": "New question: hernia\n\nfor a patient with a hernia, will they be able to walk sooner when recovering at home or when recovering in a hospital?\nobserved data: location, recovery, bed-rest"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#observed-data-2",
    "href": "lectures/day2-scms/lec1.html#observed-data-2",
    "title": "Causal Directed Acylic Graphs",
    "section": "Observed data 2",
    "text": "Observed data 2\n\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nbedrest\nno\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nyes\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\n\n\n\n\n\nmarginal\n688 / 800 = 86%\n163 / 200 = 81.5%\n\n\n\n\nmore bed rest in hospital\nwhat is the correct way to estimate the effect of location?"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#how-to-unravel-this",
    "href": "lectures/day2-scms/lec1.html#how-to-unravel-this",
    "title": "Causal Directed Acylic Graphs",
    "section": "How to unravel this?",
    "text": "How to unravel this?\n\nwe got two questions with exactly the same data\nin one example, ‘stratified analysis’ seemed best\nin the other example, ‘marginal analysis’ seemed best\nwith Directed Acyclic Graphs we can make our decision"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#causal-directed-acyclic-graphs",
    "href": "lectures/day2-scms/lec1.html#causal-directed-acyclic-graphs",
    "title": "Causal Directed Acylic Graphs",
    "section": "Causal Directed Acyclic Graphs",
    "text": "Causal Directed Acyclic Graphs\ndiagram that represents our assumptions on causal relations\n\nnodes are variables\narrows (directed edges) point from cause to effect\n\n\n\n\n\n\n\nFigure 1: Directed Acyclic Graph\n\n\n\n\nwhen used to convey causal assumptions, DAGs are ‘causal’ DAGs\nthis is not the only use of DAGs (see day 4)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#making-dags-for-our-examples",
    "href": "lectures/day2-scms/lec1.html#making-dags-for-our-examples",
    "title": "Causal Directed Acylic Graphs",
    "section": "Making DAGs for our examples:",
    "text": "Making DAGs for our examples:\nThe pregnancy DAG\n\n\n\n\n\n\n\n\n\n\nassumptions:\n\nwomen with high risk of bad neonatal outcomes (pregnancy risk) are referred to the hospital for delivery\nhospital deliveries lead to better outcomes for babies as more emergency treatments possible\nboth pregnancy risk and hospital delivery cause neonatal outcome\n\nthe other variable pregnancy risk is a common cause of the treatment (hospital delivery) and the outcome (this is what’s called a confounder)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#making-dags-for-our-examples-1",
    "href": "lectures/day2-scms/lec1.html#making-dags-for-our-examples-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Making DAGs for our examples:",
    "text": "Making DAGs for our examples:\nThe hernia DAG\n\n\n\n\n\n\n\n\n\n\nassumptions:\n\npatients admitted to the hospital keep more bed rest than those who remain at home\nbed rest leads to lower recovery times thus less walking patients after 1 week\n\nthe other variable bed rest is a mediator between the treatment (hospitalized) and the outcome"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#causal-dags-to-the-rescue",
    "href": "lectures/day2-scms/lec1.html#causal-dags-to-the-rescue",
    "title": "Causal Directed Acylic Graphs",
    "section": "Causal DAGs to the rescue",
    "text": "Causal DAGs to the rescue\n\nthe other variable was:\n\na common cause of the treatment and outcome in the pregnancy example\na mediator between the treatment and the outcome in the hernia example\n\nusing our background knowledge we could see something is different about these examples\nnext: ground this in causal theory and see implications for analysis"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#why-math",
    "href": "lectures/day2-scms/lec1.html#why-math",
    "title": "Causal Directed Acylic Graphs",
    "section": "Why math???",
    "text": "Why math???\n\n\n\n\n\n\n\nneed probability for estimation\nneed conditional independence for causal inference\nneed to understand ‘strength’ of assumptions\n\n\n\n\n\n\noh no math"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#marginal-joint-and-conditional-probabilites",
    "href": "lectures/day2-scms/lec1.html#marginal-joint-and-conditional-probabilites",
    "title": "Causal Directed Acylic Graphs",
    "section": "Marginal, Joint and Conditional probabilites",
    "text": "Marginal, Joint and Conditional probabilites\nProbability statements about random events \\(A\\) and \\(B\\)\n\n\\(A\\): patient diest (\\(A=1\\))\n\\(B\\): patient has cancer (\\(B=1\\))\n\n\n\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A)\\)\nmarginal probability that event \\(A\\) occurs\n\n\n\\(P(B)\\)\nmarginal probability that event \\(B\\) occurs\n\n\n\n\n\n\n\njoint probability table\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n10\n80\n90\n\n\n\n\n15\n85\n100\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n\n\n\n\n\n\nhas no cancer\n\n\n\n\n\n\n\n15\n85\n100\n\n\n\n\\(P(A=1) = 15 / 100\\)\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n\n\n10\n\n\n\nhas no cancer\n\n\n90\n\n\n\n\n\n\n100\n\n\n\n\\(P(B=1) = 10 / 100\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#marginal-joint-and-conditional-probabilites-1",
    "href": "lectures/day2-scms/lec1.html#marginal-joint-and-conditional-probabilites-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Marginal, Joint and Conditional probabilites",
    "text": "Marginal, Joint and Conditional probabilites\nProbability statements about random events \\(A\\) and \\(B\\):\n\n\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A)\\)\nmarginal probability that event \\(A\\) occurs\n\n\n\\(P(A,B)\\)\njoint probability of \\(A\\) and \\(B\\)\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n10\n80\n90\n\n\n\n\n15\n85\n100\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n\n\n\n\n\nhas no cancer\n\n\n\n\n\n\n\n\n\n100\n\n\n\n\\(P(B=1,A=1) = 5 / 100\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#marginal-joint-and-conditional-probabilites-2",
    "href": "lectures/day2-scms/lec1.html#marginal-joint-and-conditional-probabilites-2",
    "title": "Causal Directed Acylic Graphs",
    "section": "Marginal, Joint and Conditional probabilites",
    "text": "Marginal, Joint and Conditional probabilites\nProbability statements about random events \\(A\\) and \\(B\\):\n\n\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A)\\)\nmarginal probability that event \\(A\\) occurs\n\n\n\\(P(A,B)\\)\njoint probability of \\(A\\) and \\(B\\)\n\n\n\\(P(A|B)\\)\nconditional probability of \\(A\\) given \\(B\\)\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n10\n80\n90\n\n\n\n\n15\n85\n100\n\n\n\n- marginal \\(P(A=1) = 15/100\\)\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n- marginal \\(P(A=1) = 15/100\\)\n- conditional \\(P(A=1|B=1) = 5 / 10\\)\n\n\n\n\n\n\n\n\n\n\n\nconditional probabilities require dividing by the denominator of the conditioning set\n\n\nThis is why we need positivity (as dividing by \\(0\\) is not defined)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#probability-rules-and-identities",
    "href": "lectures/day2-scms/lec1.html#probability-rules-and-identities",
    "title": "Causal Directed Acylic Graphs",
    "section": "Probability rules and identities",
    "text": "Probability rules and identities\n\n\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A) = \\sum_{b} P(A,B=b)\\)\nmarginal is sum over joint\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n\n\n\n\n\nhas no cancer\n10\n\n\n\n\n\n\n15\n\n100\n\n\n\n\\[\\begin{align}\n    P(A=1) &= P(A=1,B=0) + P(A=1,B=1) \\\\\n           &= 5/100 + 10/100 \\\\\n           & = 15/100\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#probability-rules-and-identities-1",
    "href": "lectures/day2-scms/lec1.html#probability-rules-and-identities-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Probability rules and identities",
    "text": "Probability rules and identities\n\n\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A) = \\sum_{b} P(A,B=b)\\)\nmarginal is sum over joint\n\n\n\\(P(A,B) = P(A|B)P(B)\\)\nproduct rule\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n\n10\n\n\n\nhas no cancer\n\n\n\n\n\n\n\n\n\n100\n\n\n\n\\[\\begin{align}\n    P(A=1,B=1) &= P(A=1|B=1)P(B=1) \\\\\n               &= 5/10 * 10/100 \\\\\n               & = 5/100\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#probability-rules-and-identities-2",
    "href": "lectures/day2-scms/lec1.html#probability-rules-and-identities-2",
    "title": "Causal Directed Acylic Graphs",
    "section": "Probability rules and identities",
    "text": "Probability rules and identities\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A) = \\sum_{b} P(A,B=b)\\)\nmarginal is sum over joint\n\n\n\\(P(A,B) = P(A|B)P(B)\\)\nproduct rule\n\n\n\\(P(A|B) = \\frac{P(A,B)}{P(B)}\\)\nconditional is joint over marginal (follows from product rule)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#probability-rules-and-identities-3",
    "href": "lectures/day2-scms/lec1.html#probability-rules-and-identities-3",
    "title": "Causal Directed Acylic Graphs",
    "section": "Probability rules and identities",
    "text": "Probability rules and identities\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A) = \\sum_{b} P(A,B=b)\\)\nmarginal is sum over joint\n\n\n\\(P(A,B) = P(A|B)P(B)\\)\nproduct rule\n\n\n\\(P(A|B) = \\frac{P(A,B)}{P(B)}\\)\nconditional is joint over marginal (follows from product rule)\n\n\n\\(P(A|C) = \\sum_{b} P(A|B=b,C)P(B=b|C)\\)\ntotal expectation (consequence of marginal vs joint and product rule)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#marginal-and-conditional-independence",
    "href": "lectures/day2-scms/lec1.html#marginal-and-conditional-independence",
    "title": "Causal Directed Acylic Graphs",
    "section": "Marginal and conditional independence:",
    "text": "Marginal and conditional independence:\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A,B) = P(A)P(B)\\)\n(marginal) independence of \\(A\\) and \\(B\\)\n\n\n\n\nknowing \\(A\\) has no information on what to expect of \\(B\\)\nIf I roll a die, the result of that die (\\(A\\)) has no information on the weather in the Netherlands (\\(B\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#marginal-and-conditional-independence-1",
    "href": "lectures/day2-scms/lec1.html#marginal-and-conditional-independence-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Marginal and conditional independence:",
    "text": "Marginal and conditional independence:\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A,B) = P(A)P(B)\\)\n(marginal) independence of \\(A\\) and \\(B\\)\n\n\n\\(P(A,B|C) = P(A|C)P(B|C)\\)\nconditional independence of \\(A\\) and \\(B\\) given \\(C\\)\n\n\n\\(P(A|B,C) = P(A|C)\\)\nconditional independence of \\(A\\) and \\(B\\) given \\(C\\)\n\n\n\n\n\\(C\\) has all the information that is shared between \\(A\\) and \\(B\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#conditional-independence-in-an-example",
    "href": "lectures/day2-scms/lec1.html#conditional-independence-in-an-example",
    "title": "Causal Directed Acylic Graphs",
    "section": "Conditional Independence in an example",
    "text": "Conditional Independence in an example\n\n\n\n\n\n\n\nCharlie calls Alice and reads her script \\(C\\), then she calls Bob and reads him the same\nA week later we ask Alice to repeat the story Charlie told her, she remembered \\(A\\), a noisy version of \\(C\\)\nWe ask Bob the same, he recounts \\(B\\), a different noisy version of \\(C\\)\nAre \\(A\\) and \\(B\\) independent? No! \\(P(A,B) \\neq P(A)P(B)\\)\n\nIf we learn \\(A\\) from Alice, we can get a good guess about \\(B\\) from Bob\n\nIf we knew \\(C\\), would hearing \\(A\\) give use more information about \\(B\\)?\n\nNo, because all the shared information between \\(A\\) and \\(B\\) is explained by \\(C\\), so:\n\\(P(A,B) \\neq P(A)P(B)\\)\n\\(P(A,B|C) = P(A|C)P(B|C)\\)\n\nVariables can be marginally dependent but conditionally independent (and vice-versa)\n\n\n\n\n\n\nABC"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#sec-assumptions",
    "href": "lectures/day2-scms/lec1.html#sec-assumptions",
    "title": "Causal Directed Acylic Graphs",
    "section": "Assumption parlance",
    "text": "Assumption parlance\n\nnecessary assumption:\n\nA must hold for B to be true\n\nsufficient assumption:\n\nB is always true when A holds\n\nstrong assumption:\n\nrequires strong evidence, we’d rather not make these\n\nweak assumption:\n\nrequires weak evidence\n\nstrong vs weak assumption are judged on relative terms\n\nif assumption A is sufficient for B, B cannot be a stronger assumption that A"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-convey-two-types-of-assumptions",
    "href": "lectures/day2-scms/lec1.html#dags-convey-two-types-of-assumptions",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs convey two types of assumptions:",
    "text": "DAGs convey two types of assumptions:\ncausal direction and conditional independence\n\ncausal direction: what causes what?\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: DAG 1\n\n\n\n\n\n\n\n\nDAG 2\n\n\n\n\n\n\nread Figure 4 as\n\nsprinkler on may (or may not) cause wet floor\nwet floor cannot cause sprinkler on"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-convey-two-types-of-assumptions-1",
    "href": "lectures/day2-scms/lec1.html#dags-convey-two-types-of-assumptions-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs convey two types of assumptions:",
    "text": "DAGs convey two types of assumptions:\ncausal direction and conditional independence\n\nconditional indepence (e.g. exclusion of influence / information)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: DAG 1\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: DAG 2\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: DAG 3\n\n\n\n\n\n\n\nFigure 5 says fire can only cause wet floor through sprinkler on\n\nthis implies fire is independent of wet floor given sprinkler on and can be tested!\n\nFigure 6 says there may be other ways through which fire causes wet floor\n\nFigure 6 is thus a weaker assumption than Figure 5\n\nFigure 7 is also compatible with Figure 6"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-are-non-parametric",
    "href": "lectures/day2-scms/lec1.html#dags-are-non-parametric",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs are ‘non-parametric’",
    "text": "DAGs are ‘non-parametric’\nThey relay what variable ‘listens’ to what, but not in what way\n\n\n\n\n\n\n\n\n\nDAG\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Three datasets with the same DAG\n\n\n\n\n\n\n\n\n\\(Y = T + 0.5 (X - \\pi) + \\epsilon\\) (linear)\n\\(Y = T + \\sin(X) + \\epsilon\\) (non-linear additive)\n\\(Y = T * \\sin(X) - (1-T) \\sin(x) + \\epsilon\\) (non-linear + interaction)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-are-non-parametric-1",
    "href": "lectures/day2-scms/lec1.html#dags-are-non-parametric-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs are ‘non-parametric’",
    "text": "DAGs are ‘non-parametric’\nThey relay what variable ‘listens’ to what, but not in what way\n\n\n\n\n\n\n\n\n\nDAG\n\n\n\n\n\nthis DAG says \\(Y\\) is a function of \\(X,T\\) and external noise \\(U_Y\\), or:\n\\(Y = f_Y(X,T,U_Y)\\)\nin the next lecture we’ll talk more about these ‘structural equations’"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-imply-a-causal-factorization-of-the-joint-distribution",
    "href": "lectures/day2-scms/lec1.html#dags-imply-a-causal-factorization-of-the-joint-distribution",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs imply a causal factorization of the joint distribution",
    "text": "DAGs imply a causal factorization of the joint distribution\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: observational data\n\n\n\n\n\n\\[\\begin{align}\n    P(Y,T,Z,W) &=^1 P(Y|T,Z,W)P(T,Z,W) \\\\\n               &=^2 P(Y|T,Z)P(T,Z,W) \\\\\n               &=^3 P(Y|T,Z)P(T|Z,W)P(Z,W) \\\\\n               &=^4 P(Y|T,Z)P(T|Z,W)P(Z)P(W)\n\\end{align}\\]\n\n\nproduct-rule\n\\(Y\\) independent of \\(W\\) given \\(T,Z\\) per DAG\nproduct-rule\n\\(Z,W\\) marginally independent per DAG\n\n\n\n\n\nIf this looks complicated: just follow the arrows"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#sec-def-intervention",
    "href": "lectures/day2-scms/lec1.html#sec-def-intervention",
    "title": "Causal Directed Acylic Graphs",
    "section": "The DAG definition of an intervention",
    "text": "The DAG definition of an intervention\nassume this is our DAG for a situation and we want to learn the effect \\(T\\) has on \\(Y\\)\n\nthis is denoted \\(P(Y|\\text{do}(T))\\): a hypothetical intervention in the system\nin the graph, intervening on variable \\(T\\) means removing all incoming arrows\nthis assumes such a modular intervention is possible: i.e. leave everything else unaltered\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: observational data\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: intervened DAG\n\n\n\n\n\n\n\nwhich means \\(T\\) does not listen to other variables anymore, but is set at a particular value, like in an experiment\nimagining this scenario requires a well-defined treatment variable (akin to consistency)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery---changed-distribution",
    "href": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery---changed-distribution",
    "title": "Causal Directed Acylic Graphs",
    "section": "Intervention as graph surgery - changed distribution",
    "text": "Intervention as graph surgery - changed distribution\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: observational data\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: intervened DAG\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n      P_{\\text{obs}}(Y,T,Z) &= P(Y|T,Z)\\color{red}{P(T|Z)}P(Z) \\\\\n        P_{\\text{obs}}(Y|T) &= \\sum_{z} P(Y|T,Z=z)P(Z=z|T)\n\\end{align}\\]\n\n\n\\[\\begin{align}\n      P_{\\text{int}}(Y,T,Z) &= P(Y|T,Z)\\color{green}{P(T)}P(Z) \\\\\n        P_{\\text{int}}(Y|T) &= \\sum_{z} P(Y|T,Z=z)P(Z=z|T) \\\\\n               &\\class{fragment}{= \\sum_{z} P(Y|T,Z=z)\\color{green}{P(Z)}} \\\\\n               &\\class{fragment}{= P(Y|\\text{do}(T))}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery---changed-distribution-1",
    "href": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery---changed-distribution-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Intervention as graph surgery - changed distribution",
    "text": "Intervention as graph surgery - changed distribution\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: observational data\n\n\n\n\\[P_{\\text{obs}}(Y|T) = \\sum_{z} P(Y|T,Z=z)\\color{red}{P(Z=z|T)}\\]\n\n\n\n\n\n\n\n\nFigure 15: intervened DAG\n\n\n\n\\[P_{\\text{int}}(Y|T) = \\sum_{z} P(Y|T,Z=z)\\color{green}{P(Z=z)} \\qquad(1)\\]\n\n\n\n\nin \\(P_{\\text{obs}}\\), \\(P(Z|T) \\color{red}{\\neq} P(Z)\\)\nin \\(P_{\\text{int}}\\), \\(P(Z|T) \\color{green}{=} P(Z)\\)\nthereby \\(P_{\\text{obs}}(Y|T) \\neq P_{\\text{int}}(P(Y|T)) = P(Y|\\text{do}(T))\\)\nseeing is not doing\nlooking at Equation 1, we can compute these from \\(P_{\\text{obs}}\\)! (this is what is called an estimand)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#back-to-example-1",
    "href": "lectures/day2-scms/lec1.html#back-to-example-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Back to example 1",
    "text": "Back to example 1\n\n\n\n\n\n\n\n\n\nDAG\n\n\n\n\n\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nrisk\nlow\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nhigh\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\n\n\n\n\n\nmarginal\n688 / 800 = 86%\n163 / 200 = 81.5%\n\n\n\n\n\n\n\nestimand: \\(P(\\text{outcome}|\\text{do}(\\text{location})) = \\sum_{\\text{risk}} P(\\text{outcome}|\\text{location},\\text{risk})P(\\text{risk})\\)\n\\(P(\\text{risk}=\\text{low})=74\\%\\)\n\n\n\\[\\begin{align}\nP(\\text{outcome}|\\text{do}(\\text{hospital})) &= 95 * 0.74 + 80 * 0.26 = 91.1\\% \\\\\n     P(\\text{outcome}|\\text{do}(\\text{home})) &= 90 * 0.74 + 50 * 0.26 = 79.6\\%\n\\end{align}\\]\n\n\nconclusion: sending all deliveries to the hospital leads to better neonatal outcomes"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#back-to-example-2",
    "href": "lectures/day2-scms/lec1.html#back-to-example-2",
    "title": "Causal Directed Acylic Graphs",
    "section": "Back to example 2",
    "text": "Back to example 2\n\n\n\n\n\n\n\n\n\nDAG\n\n\n\n\n\nremoving all arrows going in to \\(T\\) results in the same DAG\nso \\(P(Y|T) = P(Y|\\text{do}(T))\\)\ni.e. use the marginals"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#the-gist-of-observational-causal-inference",
    "href": "lectures/day2-scms/lec1.html#the-gist-of-observational-causal-inference",
    "title": "Causal Directed Acylic Graphs",
    "section": "The gist of observational causal inference",
    "text": "The gist of observational causal inference\nis to take data we have to make inferences about data from a different distribution (i.e. the intervened-on distribution)\n\n\n\n\n\n\n\n\nFigure 16: observational data: data we have\n\n\n\n\n\n\n\n\n\nFigure 17: intervened DAG: what we want to know\n\n\n\n\n\ncausal inference frameworks provide a language to express assumptions\nbased on these assumptions, the framework tells us whether such an inference is possible\n\nthis is often referred to as is the effect identified\n\nand provide formula(s) for how to do so based on the observed data distribution (estimand(s))\n(one could say this is essentially assumption-based extrapolation, some researchers think this entire enterprise is anti-scientific)\nnot yet said: how to do statistical inference to estimate the estimand (much can still go wrong here)\n\ncan also be part of identification, see the following lecture on SCMs"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#basic-dag-patterns-chain",
    "href": "lectures/day2-scms/lec1.html#basic-dag-patterns-chain",
    "title": "Causal Directed Acylic Graphs",
    "section": "Basic DAG patterns: chain",
    "text": "Basic DAG patterns: chain\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18: chain / mediation\n\n\n\n\n\n\n\\(M\\) mediates effect of \\(X\\) on \\(Y\\)\n\\(X \\perp Y | M\\)\ndo not want to adjust for \\(M\\) when estimating total effect of \\(X\\) on \\(Y\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#basic-dag-patterns-fork",
    "href": "lectures/day2-scms/lec1.html#basic-dag-patterns-fork",
    "title": "Causal Directed Acylic Graphs",
    "section": "Basic DAG patterns: fork",
    "text": "Basic DAG patterns: fork\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: fork / confounder\n\n\n\n\n\n\n\\(Z\\) causes both \\(X\\) and \\(Y\\) (common cause / confounder)\n\\(X \\perp Y | Z\\)\n\\(Z \\to X\\) is a back-door: a path between \\(X\\) and \\(Y\\) that starts with an arrow into \\(X\\)\ntypically want to adjust for \\(Z\\) (see later 5.9)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#basic-dag-patterns-collider",
    "href": "lectures/day2-scms/lec1.html#basic-dag-patterns-collider",
    "title": "Causal Directed Acylic Graphs",
    "section": "Basic DAG patterns: collider",
    "text": "Basic DAG patterns: collider\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20: collider\n\n\n\n\n\n\n\\(X\\) and \\(Y\\) both cause \\(Z\\)\n\\(X \\perp Y\\) (but NOT when conditioning on \\(Z\\))\noften do not want to condition on \\(Z\\) as this induces a correlation between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#collider-bias---tinder",
    "href": "lectures/day2-scms/lec1.html#collider-bias---tinder",
    "title": "Causal Directed Acylic Graphs",
    "section": "Collider bias - Tinder",
    "text": "Collider bias - Tinder\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) collider\n\n\n\n\n\nFigure 21: \\[\\begin{align}\n    \\text{intelligent} &\\sim U[0,1] \\\\\n    \\text{attractive}  &\\sim U[0,1] \\\\\n    \\text{on tinder}   &= I_{\\text{intelligent} + \\text{attractive} &lt; 1}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#conditioning-on-a-collider-creates-dependence-of-its-parents",
    "href": "lectures/day2-scms/lec1.html#conditioning-on-a-collider-creates-dependence-of-its-parents",
    "title": "Causal Directed Acylic Graphs",
    "section": "Conditioning on a collider creates dependence of its parents",
    "text": "Conditioning on a collider creates dependence of its parents\n\nmay not be too visible: doing an analysis in a selected subgroup is a form of (‘invisible’) conditioning)\ne.g. when selecting only patients in the hospital\n\nbeing admitted to the hospital is a collider (has many different causes, e.g. traffic accident or fever)\nusually only one of these is the reason for hospital admission\nthe causes for hospital admission now seem anti-correlated\n\ncollider conditioning might be an explanation for the obsesity paradox (i.e. obesity is correlated with better outcomes in diverse medical settings) (e.g. Banack and Stokes 2017)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#when-life-gets-complicated-real",
    "href": "lectures/day2-scms/lec1.html#when-life-gets-complicated-real",
    "title": "Causal Directed Acylic Graphs",
    "section": "When life gets complicated / real",
    "text": "When life gets complicated / real\n\nBogie, James; Fleming, Michael; Cullen, Breda; Mackay, Daniel; Pell, Jill P. (2021). Full directed acyclic graph.. PLOS ONE. Figure. https://doi.org/10.1371/journal.pone.0249258.s003"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#d-separation-directional-separation",
    "href": "lectures/day2-scms/lec1.html#d-separation-directional-separation",
    "title": "Causal Directed Acylic Graphs",
    "section": "d-separation (directional-separation)",
    "text": "d-separation (directional-separation)\n\npaths\na path is a set of nodes connected by edges (\\(x \\ldots y\\))\na directed-path is a path with a constant direction (\\(x \\dots t\\))\nan unblocked-path is a path without a collider (\\(t \\ldots y\\))\na blocked-path is a path with a collider (\\(s,t, u\\))\nd(irectional)-separation of \\(x,y\\) means there is no unblocked path between them"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#d-separation-when-conditioning",
    "href": "lectures/day2-scms/lec1.html#d-separation-when-conditioning",
    "title": "Causal Directed Acylic Graphs",
    "section": "d-separation when conditioning",
    "text": "d-separation when conditioning\n\npaths with conditioning variables \\(r\\), \\(t\\)\nconditioning on variable:\n\nwhen variable is a collider: opens a path (\\(t\\) opens \\(s,t,u\\) etc.)\notherwise: blocks a path (e.g. \\(r\\) blocks \\(x,r,s\\))\n\nconditioning set \\(Z=\\{r,t\\}\\): set of conditioning variables"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#sec-backdoor",
    "href": "lectures/day2-scms/lec1.html#sec-backdoor",
    "title": "Causal Directed Acylic Graphs",
    "section": "The back-door criterion and adjustment",
    "text": "The back-door criterion and adjustment\nDefinition 3.3.1 (Back-Door) (for pairs of variables)\nA set of variables \\(Z\\) satisfies the back-door criterion relative to an ordered pair of variables \\((X,Y)\\) in a DAG if:\n\nno node in \\(Z\\) is a descendant of \\(X\\) (e.g. mediators)\n\\(Z\\) blocks every path between \\(X\\) and \\(Y\\) that contains an arrow into \\(X\\)\n\n\nTheorem 3.2.2 (Back-Door Adjustment)\nIf a set of variables \\(Z\\) satisfies the back-door criterion relative to \\((X,Y)\\), then the causal effect of \\(X\\) on \\(Y\\) is identifiable and is given by the formula\n\\[P(y|\\text{do}(x)) = \\sum_z P(y|x,z)P(z) \\qquad(2)\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#did-we-see-this-equation-before",
    "href": "lectures/day2-scms/lec1.html#did-we-see-this-equation-before",
    "title": "Causal Directed Acylic Graphs",
    "section": "Did we see this equation before?",
    "text": "Did we see this equation before?\n\nYes! When computing the effect of hospital deliveries on neonatal outcomes Equation 1\nDAGs tell us what to adjust for\nautomatic algorithms tell use whether an estimand exists and what it is\nseveral point-and-click websites for making DAGs that implement these algorithms:\n\ndagitty.net\ncausalfusion.net"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#how-about-positivity",
    "href": "lectures/day2-scms/lec1.html#how-about-positivity",
    "title": "Causal Directed Acylic Graphs",
    "section": "How about positivity",
    "text": "How about positivity\n\nbackdoor adjustment with \\(z\\) requires computing \\(P(y|x,z)\\)\nby the product rule:\n\\[P(y|x,z) = \\frac{P(y,x,z)}{P(x,z)}\\]\nthis division is only defined when \\(P(x,z) &gt; 0\\)\nwhich is the same as the positivity assumption from Day 1 in Potential Outcomes"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#references",
    "href": "lectures/day2-scms/lec1.html#references",
    "title": "Causal Directed Acylic Graphs",
    "section": "References",
    "text": "References\n\n\n\n\nWouter van Amsterdam — WvanAmsterdam — vanamsterdam.github.io\n\n\n\n\nBanack, H. R., and A. Stokes. 2017. “The ‘Obesity Paradox’ May Not Be a Paradox at All.” International Journal of Obesity 41 (8): 1162–63. https://doi.org/10.1038/ijo.2017.99.\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press."
  },
  {
    "objectID": "practicals/00_setup/setup.html",
    "href": "practicals/00_setup/setup.html",
    "title": "Setup for Causal Inference and Causal Data Science Course",
    "section": "",
    "text": "We will work with R. You can use your preferred way of working in R to do the practicals. Our preferred way is this:\n\nCreate a new folder with a good name, e.g., practicals_causal_datascience\nOpen RStudio\nCreate a new project from RStudio, which you associate with the folder\nCreate a raw_data subfolder\nCreate an R script for the current practical, e.g., introduction.R\nCreate your well-documented and well-styled code in this R script\n\nWe try to make our practicals light in the number of required packages, but the packages below are needed. You can install them via:\n\nneeded_packages &lt;- c(\n  \"data.table\", \"broom\", \"purrr\", \"dagitty\", \"ggplot2\", \"dplyr\", \"marginaleffects\",\n  \"MatchIt\",\"survey\",\"tableone\"\n)\ncran_repo &lt;- \"https://mirror.lyrahosting.com/CRAN/\" # &lt;- a CRAN mirror in the Netherlands, can select another one from here https://cran.r-project.org/mirrors.html\n\nfor (pkg in needed_packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, repos=cran_repo)\n  }\n}"
  },
  {
    "objectID": "practicals/21_dags/dags.html",
    "href": "practicals/21_dags/dags.html",
    "title": "Practical on DAGs",
    "section": "",
    "text": "This lab will guide you through practical examples of drawing and analyzing DAGs using R. You will learn how to simulate data, create DAGs, and use them for causal inference.\nCode\n# Install necessary packages if not already installed\nrequired_pkgs &lt;- c(\"dagitty\", \"ggplot2\", \"broom\", \"purrr\", \"dplyr\", \"data.table\", \"marginaleffects\")\ncran_repo &lt;- \"https://mirror.lyrahosting.com/CRAN/\" # &lt;- a CRAN mirror in the Netherlands, can select another one from here https://cran.r-project.org/mirrors.html\n\nfor (pkg in required_pkgs) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, repos=cran_repo)\n  }\n}\n\nsuppressPackageStartupMessages({\n  # Load packages\n  library(purrr)\n  library(broom)\n  library(dagitty)\n  library(ggplot2)\n  library(dplyr)\n  library(marginaleffects)\n  library(data.table)\n})\n\nsource(here::here(\"practicals\", \"21_dags\", \"_makedatas.R\"))\ndatas &lt;- make_datas()"
  },
  {
    "objectID": "practicals/21_dags/dags.html#birthweight-data",
    "href": "practicals/21_dags/dags.html#birthweight-data",
    "title": "Practical on DAGs",
    "section": "1.1 Birthweight data:",
    "text": "1.1 Birthweight data:\nWe’ll use the (simulated) dataset birthw with data on birthweight and survival of babies.\nThe birthw dataset contains the following variables:\n\nageover35: Indicator mother’s age over 35 years (0 = age &lt;= 35, 1 = age &gt;35)\nsmoking: Smoking status during pregnancy (0 = no, 1 = yes)\nlbwt: Low birth weight (0 = &gt;=2500grams, 1 = &lt; 2500grams)\ndeath: Neonatal death within 3 months (0 = no, 1 = yes)\n\n\n\nCode\nbirthw &lt;- datas[[\"birthw\"]]\nhead(birthw)\n\n\n\n  \n\n\n\nThe data can alternatively be downloaded here: birthw.csv"
  },
  {
    "objectID": "practicals/21_dags/dags.html#create-a-dag",
    "href": "practicals/21_dags/dags.html#create-a-dag",
    "title": "Practical on DAGs",
    "section": "1.2 Create a DAG",
    "text": "1.2 Create a DAG\n\n1.2.1 Think of a DAG that may fit this data using the observed variables\nTake a few minutes to create a DAG (collaboratively) (using e.g. dagitty.net)\n\n\n1.2.2 Are there variables that may be missing in the data but are relevant?\nIf so, add them to the DAG, and indicate that they are unobserved\n\n\n1.2.3 With your DAG, can the causal effect be estimated?\nUse e.g. dagitty.net to create your DAG and see if there are ways to estimate the causal effect."
  },
  {
    "objectID": "practicals/21_dags/dags.html#analyse-the-data",
    "href": "practicals/21_dags/dags.html#analyse-the-data",
    "title": "Practical on DAGs",
    "section": "1.3 Analyse the data",
    "text": "1.3 Analyse the data\nLet’s try some analyses on the data. We’ll fit different logistic regression models with different covariates (independent variables). Specifically, fit a model with:\n\nall observed covariates (fit_allobs)\nonly the somking variable (fit_marginal)\n\nThese models give us estimates of (log) odds ratios for the independent variables. To translate a logistic regression model into differences in probabilities we use the avg_comparisons function from the marginaleffects package.\n\n\nCode\nrequire(marginaleffects)\n\nfit_allobs &lt;- glm(death~., data=birthw, family=\"binomial\")\nfit_marginal &lt;- glm(death~smoking, data=birthw, family=\"binomial\")\n\navg_comparisons(fit_allobs, variables=\"smoking\")\n\n\n\n    Term          Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %\n smoking mean(1) - mean(0)    -0.13     0.0179 -7.25   &lt;0.001 41.1 -0.165\n  97.5 %\n -0.0948\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nCode\navg_comparisons(fit_marginal, variables=\"smoking\")\n\n\n\n    Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n smoking mean(1) - mean(0)   0.0808     0.0288 2.81  0.00502 7.6 0.0244  0.137\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\nThe effect estimates of fit_allobs and fit_marginal are quite different, they have different signs. How could this be explained? Which effect estimate do you think is more credible?"
  },
  {
    "objectID": "practicals/21_dags/dags.html#assume-a-dag",
    "href": "practicals/21_dags/dags.html#assume-a-dag",
    "title": "Practical on DAGs",
    "section": "1.4 Assume a DAG",
    "text": "1.4 Assume a DAG\n\n\n\n\n\n\nAssume the following DAG\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: DAG for smoking and death\n\n\n\nIn this DAG, there is another variable gene that influences both lbwt and death.\n\n\n\n\n\n\n\n\n\nHow does this DAG change the analysis? (tip: enter it in dagitty.net)\n\n\n\n\n\nanswer: the smoking-death relationship has no confounders, the marginal estimate is correct. Adjusting for lbwt ‘washes-out’ part of smoking’s effect because lbwt is a mediator. Also, lbwt is a collider between gene and smoking, and gene has a direct arrow into death. Conditioning on lbwt opens a bidirected path between smoking and gene, creating a new backdoor path. So there are two reasons not to condition on lbwt: it is a mediator and a collider with an unmeasured variable\n\n\n\n\n\n\n\n\n\nSee this other DAG on the smoking question\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: birthweight DAG 2\n\n\n\n\n\n\nGiven the DAG in Figure 2, see the following regression model\n\nfit2 &lt;- glm(death~smoking+ht+ageover35, data=birthw, family=binomial)\n\n\n\n\n\n\n\nAssuming no parametric form bias, will this lead to an unbiased causal effect estimate?\n\n\n\n\n\nanswer: yes this is a correct analysis. lbwt is still a collider, but it does not open any new back-door paths because gene no longer has a direct effect on death and all variables other than smoking that do have such an arrow are in the conditioning set (ageover35,ht) so these paths are blocked"
  },
  {
    "objectID": "practicals/21_dags/dags.html#non-coding-questions",
    "href": "practicals/21_dags/dags.html#non-coding-questions",
    "title": "Practical on DAGs",
    "section": "2.1 Non-coding questions",
    "text": "2.1 Non-coding questions\n\n2.1.1 Strength of Assumptions\n\n\n\n\n\n\n\n\n\n\n\n(a) DAG\n\n\n\n\n\n\n\n\n\n\n\n(b) DAG\n\n\n\n\n\n\n\n\n\n\n\n(c) DAG\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n\nWhat is the correct ordering of the strength of assumptions in the above DAGs, starting with the strongest assumption\n\n\n\n\n\nanswer: Figure 4 (b) &gt; Figure 4 (c) &gt; Figure 4 (a)\nFigure 4 (b) is stronger than Figure 4 (c) as in the latter, it could be that the effects through \\(W\\) are all absent (remember that the presence of an arrow from A to B implies a possible effect of A on B)\nFigure 4 (c) is stronger than Figure 4 (a) as in the first, Z can only affect Y through T and W, whereas in Figure 4 (a) Z can effect Y through T and can effect Y through other paths (e.g. W)\nsee also the lecture on DAGs\n\n\n\n\n\n2.1.2 RCTs\nAccording to the DAG framework, why are RCTs especially fit for causal questions?\n\nthey are often infeasible and unethical\nthey sample data from the target distribution\nthey have better external validity than observational studies\nrandomization balances confounders\n\n\n\n\n\n\n\nWhich answers are true?\n\n\n\n\n\nanswer: 2.\nSee also the DAG lecture\nContext:\n\nThis is often said of RCTs but has no direct bearing on why they are special for causal inference\nRemember that the target distribution has no arrows going in to the treatment variable, this is what we get in a RCT\nRCTs are often critiqued as having poor external validity, because they may recruit non-random subpopulations from the target population\nThis is a subtle point, but RCTs have no confounders as there are no common causes of the treatment and the outcome. Variables that are confounders in observational studies are prognostic factors in RCTs, as they (by definition of being a confounder in an observational study) influence the outcome, but not the treatment in the RCT. Randomization balances the distribution of prognostic factors between treatment arms in expectation. In a particular RCT, observed (and unobserved) prognostic factors will always have some random variation between treatment arms. This does not reduce the validity of the RCT in terms of bias. This variation is reflected in the standard error of the estimate. In some cases, adjusting for known prognostic factors in RCTs may reduce the variance of the treatment estimate (i.e. narrowing the confidence interval), but this is an entire discussion on its own."
  },
  {
    "objectID": "practicals/21_dags/dags.html#creating-and-visualizing-a-dag",
    "href": "practicals/21_dags/dags.html#creating-and-visualizing-a-dag",
    "title": "Practical on DAGs",
    "section": "3.1 Creating and Visualizing a DAG",
    "text": "3.1 Creating and Visualizing a DAG\nLet’s create a DAG for the pregnancy example:\n\n\nCode\n# Define the DAG\ndag &lt;- dagitty(\"dag {\n  pregnancy_risk -&gt; hospital_delivery\n  pregnancy_risk -&gt; neonatal_outcome\n  hospital_delivery -&gt; neonatal_outcome\n}\")\n\n# Plot the DAG\nplot(dag)\n\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\n\nThis DAG assumes that pregnancy risk influences both the likelihood of hospital delivery and neonatal outcomes, and that hospital delivery affects neonatal outcomes.\n\n3.1.1 Simulating Data\nWe will simulate data based on the DAG structure:\n\n\nCode\nset.seed(123)\n\nn &lt;- 1000\n\n# Simulate variables\npregnancy_risk &lt;- rbinom(n, 1, 0.3)  # 30% high risk\nhospital_delivery &lt;- rbinom(n, 1, 0.5 + 0.3 * pregnancy_risk)  # 50% baseline + 30% if high risk\nneonatal_outcome &lt;- rbinom(n, 1, 0.8 - 0.3 * pregnancy_risk + 0.15 * hospital_delivery)  # outcome affected by both\n\n# Create a data frame\ndf &lt;- data.table(pregnancy_risk, hospital_delivery, neonatal_outcome)\n\n\n\n\n3.1.2 Analyzing the Data\nCheck the relationships in the data:\n\n\nCode\n# Summary statistics\nsummary(df)\n\n\n pregnancy_risk  hospital_delivery neonatal_outcome\n Min.   :0.000   Min.   :0.000     Min.   :0.000   \n 1st Qu.:0.000   1st Qu.:0.000     1st Qu.:1.000   \n Median :0.000   Median :1.000     Median :1.000   \n Mean   :0.295   Mean   :0.586     Mean   :0.806   \n 3rd Qu.:1.000   3rd Qu.:1.000     3rd Qu.:1.000   \n Max.   :1.000   Max.   :1.000     Max.   :1.000   \n\n\nCode\n# Plot the data\nggplot(df, aes(x = factor(hospital_delivery), fill = factor(neonatal_outcome))) +\n  geom_bar(position = \"fill\") +\n  facet_grid(~ pregnancy_risk) +\n  labs(x = \"Hospital Delivery\", y = \"Proportion\", fill = \"Neonatal Outcome\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Causal Inference Using DAGs\nLet’s use the DAG to determine what to condition on to estimate the causal effect of hospital delivery on neonatal outcomes:\n\n\nCode\n# Identify adjustment set using DAGitty\nadjustmentSets(dag, exposure = \"hospital_delivery\", outcome = \"neonatal_outcome\")\n\n\n{ pregnancy_risk }\n\n\nThe output will suggest which variables to condition on to estimate the causal effect correctly. In this case, we need to condition on pregnancy_risk.\n\n\n3.1.4 Estimating the Causal Effect\nEstimate the causal effect using a regression model:\n\n\nCode\n# Fit a regression model\nmodel &lt;- glm(neonatal_outcome ~ hospital_delivery + pregnancy_risk, family = binomial, data = df)\n\n# Summarize the model\nsummary(model)\n\n\n\nCall:\nglm(formula = neonatal_outcome ~ hospital_delivery + pregnancy_risk, \n    family = binomial, data = df)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.7830     0.1407  12.675  &lt; 2e-16 ***\nhospital_delivery   1.0073     0.1998   5.042 4.61e-07 ***\npregnancy_risk     -2.2336     0.1987 -11.239  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 983.94  on 999  degrees of freedom\nResidual deviance: 834.27  on 997  degrees of freedom\nAIC: 840.27\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n3.1.5 Drawing Conclusions\nInterpret the model’s output to understand the effect of hospital delivery on neonatal outcomes, controlling for pregnancy risk.\n\n\nCode\navg_comparisons(model, variables=\"hospital_delivery\")\n\n\n\n              Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)    S\n hospital_delivery mean(1) - mean(0)    0.133     0.0253 5.26   &lt;0.001 22.7\n  2.5 % 97.5 %\n 0.0837  0.183\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response \n\n\n\n\n\n\n\n\nIs this odds ratio a correct estimate of the causal effect?\n\n\n\n\n\nanswer: no\nhint: compare the structural equation used in generating the data with the statistical analysis\nThis linear probability structural equation is not well-approximated by a linear logistic model (i.e. without interaction terms). We can model the outcome without making parametric assumptions by including an interaction term, and then extract the risk difference using avg_comparisons from package marginaleffects.\nThe correct estimate is given by:\n\n\nCode\nfull_model &lt;- glm(neonatal_outcome~hospital_delivery*pregnancy_risk, family=binomial, data=df)\navg_comparisons(full_model, variables=\"hospital_delivery\")\n\n\n\n              Term          Contrast Estimate Std. Error    z Pr(&gt;|z|)    S\n hospital_delivery mean(1) - mean(0)    0.121     0.0261 4.63   &lt;0.001 18.0\n  2.5 % 97.5 %\n 0.0696  0.172\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted \nType:  response"
  }
]