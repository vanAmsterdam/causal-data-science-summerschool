[
  {
    "objectID": "lectures/prep/math.html#setup",
    "href": "lectures/prep/math.html#setup",
    "title": "Mathematics intro",
    "section": "Setup",
    "text": "Setup\nProbability statements about random events \\(A\\) and \\(B\\)\n\n\\(A\\): patient dies (\\(A=1\\))\n\\(B\\): patient has cancer (\\(B=1\\))\n\n\nSay we have 100 patients, we can tabulate them according to their cancer status and whether they died or not.\n\njoint probability table\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n10\n80\n90\n\n\n\n\n15\n85\n100"
  },
  {
    "objectID": "lectures/prep/math.html#marginal-probabilities",
    "href": "lectures/prep/math.html#marginal-probabilities",
    "title": "Mathematics intro",
    "section": "Marginal probabilities",
    "text": "Marginal probabilities\n\nMarginal probabilities concern probabilities of one random event, regardless of the other random event.\nWe read these probabilities from the margins of the joint probability table.\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A=1)\\)\nmarginal probability that event \\(A\\) occurs\n\n\n\\(P(B=1)\\)\nmarginal probability that event \\(B\\) occurs\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n10\n80\n90\n\n\n\n\n15\n85\n100\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n\n\n\n\n\n\nhas no cancer\n\n\n\n\n\n\n\n15\n85\n100\n\n\n\n\\(P(A=1) = 15 / 100\\)\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n\n\n10\n\n\n\nhas no cancer\n\n\n90\n\n\n\n\n\n\n100\n\n\n\n\\(P(B=1) = 10 / 100\\)"
  },
  {
    "objectID": "lectures/prep/math.html#joint-probabilities",
    "href": "lectures/prep/math.html#joint-probabilities",
    "title": "Mathematics intro",
    "section": "Joint Probabilities",
    "text": "Joint Probabilities\n\nA joint probability concerns the probability of two random events jointly occurring together.\nThese are a based on a single cell in the joint probability table\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A)\\)\nmarginal probability that event \\(A\\) occurs\n\n\n\\(P(A=1,B=1)\\)\njoint probability of \\(A\\) and \\(B\\)\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n10\n80\n90\n\n\n\n\n15\n85\n100\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n\n\n\n\n\nhas no cancer\n\n\n\n\n\n\n\n\n\n100\n\n\n\n\\(P(B=1,A=1) = 5 / 100\\)"
  },
  {
    "objectID": "lectures/prep/math.html#conditional-probabilities",
    "href": "lectures/prep/math.html#conditional-probabilities",
    "title": "Mathematics intro",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\n\nConditional probabilities concern the probability of one random event given that another random event has occurred.\ne.g. what is the probability that a patient dies (\\(A=1\\)) given that they have cancer (\\(B=1\\))?\nThese are read from the joint probability table by looking in the row or column of the conditioning event.\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A)\\)\nmarginal probability that event \\(A\\) occurs\n\n\n\\(P(A,B)\\)\njoint probability of \\(A\\) and \\(B\\)\n\n\n\\(P(A=1|B=1)\\)\nconditional probability of \\(A\\) given \\(B\\)\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n10\n80\n90\n\n\n\n\n15\n85\n100\n\n\n\n- marginal \\(P(A=1) = 15/100\\)\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n5\n10\n\n\n\nhas no cancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n- marginal \\(P(A=1) = 15/100\\)\n- conditional \\(P(A=1|B=1) = 5 / 10\\)"
  },
  {
    "objectID": "lectures/prep/math.html#sum-rule",
    "href": "lectures/prep/math.html#sum-rule",
    "title": "Mathematics intro",
    "section": "Sum rule",
    "text": "Sum rule\nA marginal probability can be computed by summing over the joint probabilities of all possible values of the other random event.\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A) = \\sum_{b} P(A,B=b)\\)\nmarginal is sum over joint\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n\n\n\n\n\nhas no cancer\n10\n\n\n\n\n\n\n15\n\n100\n\n\n\n\\[\\begin{align}\n    P(A=1) &= P(A=1,B=1) + P(A=1,B=0) \\\\\n           &= 5/100 + 10/100 \\\\\n           & = 15/100\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/prep/math.html#product-rule",
    "href": "lectures/prep/math.html#product-rule",
    "title": "Mathematics intro",
    "section": "Product rule",
    "text": "Product rule\nA joint probability can be computed by multiplying the conditional probability of one random event given the other random event with the marginal probability of the other random event.\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A) = \\sum_{b} P(A,B=b)\\)\nmarginal is sum over joint\n\n\n\\(P(A,B) = P(A|B)P(B)\\)\nproduct rule\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\n\n\n\n\n\ndies\nlives\n\n\n\nB\nhas cancer\n5\n\n10\n\n\n\nhas no cancer\n\n\n\n\n\n\n\n\n\n100\n\n\n\n\\[\\begin{align}\n    P(A=1,B=1) &= P(A=1|B=1)P(B=1) \\\\\n               &= 5/10 * 10/100 \\\\\n               & = 5/100\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/prep/math.html#product-rule---different-form",
    "href": "lectures/prep/math.html#product-rule---different-form",
    "title": "Mathematics intro",
    "section": "Product rule - different form",
    "text": "Product rule - different form\na conditional probability can be computed by dividing the joint probability of the two random events by the marginal probability of the other random event, since1\n\\[\nx = y * z \\implies y = \\frac{x}{z}\n\\]\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A) = \\sum_{b} P(A,B=b)\\)\nmarginal is sum over joint\n\n\n\\(P(A,B) = P(A|B)P(B)\\)\nproduct rule\n\n\n\\(P(A|B) = \\frac{P(A,B)}{P(B)}\\)\nconditional is joint over marginal (follows from product rule)\n\n\n\n\\(z \\neq 0\\)"
  },
  {
    "objectID": "lectures/prep/math.html#law-of-total-probability",
    "href": "lectures/prep/math.html#law-of-total-probability",
    "title": "Mathematics intro",
    "section": "Law of total probability",
    "text": "Law of total probability\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A) = \\sum_{b} P(A,B=b)\\)\nmarginal is sum over joint\n\n\n\\(P(A,B) = P(A|B)P(B)\\)\nproduct rule\n\n\n\\(P(A|B) = \\frac{P(A,B)}{P(B)}\\)\nconditional is joint over marginal (follows from product rule)\n\n\n\\(P(A|C) = \\sum_{b} P(A|B=b,C)P(B=b|C)\\)\ntotal probability (consequence of marginal vs joint and product rule)\n\n\n\n\nthis identity can be proven quite easily using the product rule and the sum rule Section 5.1"
  },
  {
    "objectID": "lectures/prep/math.html#marginal-independence",
    "href": "lectures/prep/math.html#marginal-independence",
    "title": "Mathematics intro",
    "section": "Marginal independence",
    "text": "Marginal independence\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A,B) = P(A)P(B)\\)\n(marginal) independence of \\(A\\) and \\(B\\)\n\n\n\n\nknowing \\(A\\) has no information on what to expect of \\(B\\)\nIf I roll a die, the result of that die (\\(A\\)) has no information on the weather in the Netherlands (\\(B\\))"
  },
  {
    "objectID": "lectures/prep/math.html#conditional-independence",
    "href": "lectures/prep/math.html#conditional-independence",
    "title": "Mathematics intro",
    "section": "Conditional independence",
    "text": "Conditional independence\n\nsome events may not be independent in general, but they may be independent given some other event \\(C\\).\nstatement:\n\\[P(A,B|C) = P(A|C)P(B|C)\\]"
  },
  {
    "objectID": "lectures/prep/math.html#conditional-independence-in-an-example",
    "href": "lectures/prep/math.html#conditional-independence-in-an-example",
    "title": "Mathematics intro",
    "section": "Conditional Independence in an example",
    "text": "Conditional Independence in an example\n\n\n\n\nCharlie calls Alice and reads her script \\(C\\), then she calls Bob and reads him the same\nA week later we ask Alice to repeat the story Charlie told her, she remembered \\(A\\), a noisy version of \\(C\\)\nWe ask Bob the same, he recounts \\(B\\), a different noisy version of \\(C\\)\nAre \\(A\\) and \\(B\\) independent? No! \\(P(A,B) \\neq P(A)P(B)\\)\n\nIf we learn \\(A\\) from Alice, we can get a good guess about \\(B\\) from Bob\n\nIf we knew \\(C\\), would hearing \\(A\\) give us more information about \\(B\\)?\n\nNo, because all the shared information between \\(A\\) and \\(B\\) is explained by \\(C\\), so:\n\\(P(A,B) \\neq P(A)P(B)\\)\n\\(P(A,B|C) = P(A|C)P(B|C)\\)\n\nVariables can be marginally dependent but conditionally independent (and vice-versa)\n\n\n\n\n\n\nABC"
  },
  {
    "objectID": "lectures/prep/math.html#conditional-independence-stated-differently",
    "href": "lectures/prep/math.html#conditional-independence-stated-differently",
    "title": "Mathematics intro",
    "section": "Conditional independence, stated differently",
    "text": "Conditional independence, stated differently\n\\[P(A|B,C) = P(A|C)\\]\n\n\n\n\n\n\n\nstatement\ninterpretation\n\n\n\n\n\\(P(A,B) = P(A)P(B)\\)\n(marginal) independence of \\(A\\) and \\(B\\)\n\n\n\\(P(A,B|C) = P(A|C)P(B|C)\\)\nconditional independence of \\(A\\) and \\(B\\) given \\(C\\)\n\n\n\\(P(A|B,C) = P(A|C)\\)\nconditional independence of \\(A\\) and \\(B\\) given \\(C\\)\n\n\n\n\nboth statements of conditional independence can be shown to be equivalent (when the involved conditional probabilities are well-defined) Section 5.2"
  },
  {
    "objectID": "lectures/prep/math.html#hierarchy-of-conditions-assumptions",
    "href": "lectures/prep/math.html#hierarchy-of-conditions-assumptions",
    "title": "Mathematics intro",
    "section": "Hierarchy of conditions / assumptions",
    "text": "Hierarchy of conditions / assumptions\n\nnecessary assumption:\n\nA must hold for B to be true\nhaving a heart is necessary for having a heart rate\n\nsufficient assumption:\n\nB is always true when A holds\nBeing a square is sufficient to be a rectangle\n\nstrong assumption:\n\nrequires strong evidence, we’d rather not make these\n\nweak assumption:\n\nrequires weak evidence\n\nstrong vs weak assumption are judged on relative terms\n\nif assumption A is sufficient for B, B cannot be a stronger assumption than A"
  },
  {
    "objectID": "lectures/prep/math.html#sec-proof-totalprob",
    "href": "lectures/prep/math.html#sec-proof-totalprob",
    "title": "Mathematics intro",
    "section": "Law of total (conditional) probability",
    "text": "Law of total (conditional) probability\nWe are asked to prove:\n\\[P(A \\mid C) = \\sum_b P(A \\mid B = b, C) \\, P(B = b \\mid C)\\]\n\\[\\begin{align}\nP(A \\mid C)\n&= \\sum_b P(A, B = b \\mid C) && \\text{(sum rule)} \\\\\n&= \\sum_b P(A \\mid B = b, C) \\, P(B = b \\mid C) && \\text{(product rule)}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/prep/math.html#sec-proof-condindep",
    "href": "lectures/prep/math.html#sec-proof-condindep",
    "title": "Mathematics intro",
    "section": "Conditional independence equivalent statements",
    "text": "Conditional independence equivalent statements\nWe will prove that the conditional independence statement\n\\[\nP(A \\mid B, C) = P(A \\mid C)\n\\]\nis equivalent to\n\\[\nP(A, B \\mid C) = P(A \\mid C) \\cdot P(B \\mid C)\n\\]\nusing basic rules of probability."
  },
  {
    "objectID": "lectures/prep/math.html#conditional-independence-equivalent-statements",
    "href": "lectures/prep/math.html#conditional-independence-equivalent-statements",
    "title": "Mathematics intro",
    "section": "Conditional independence equivalent statements",
    "text": "Conditional independence equivalent statements\n✅ Proof (⇐ direction):\nAssume\n\\[\nP(A, B \\mid C) = P(A \\mid C) \\cdot P(B \\mid C)\n\\]\nBy the product rule,\n\\[\nP(A, B \\mid C) = P(A \\mid B, C) \\cdot P(B \\mid C)\n\\]\nComparing both expressions:\n\\[\nP(A \\mid B, C) \\cdot P(B \\mid C) = P(A \\mid C) \\cdot P(B \\mid C)\n\\]\nDivide both sides by \\(P(B \\mid C) &gt; 0\\), we get:\n\\[\nP(A \\mid B, C) = P(A \\mid C)\n\\]"
  },
  {
    "objectID": "lectures/prep/math.html#conditional-independence-equivalent-statements-1",
    "href": "lectures/prep/math.html#conditional-independence-equivalent-statements-1",
    "title": "Mathematics intro",
    "section": "Conditional independence equivalent statements",
    "text": "Conditional independence equivalent statements\n✅ Proof (⇒ direction):\nAssume\n\\[\nP(A \\mid B, C) = P(A \\mid C)\n\\]\nAgain by the product rule:\n\\[\nP(A, B \\mid C) = P(A \\mid B, C) \\cdot P(B \\mid C)\n\\]\nSubstitute \\(P(A \\mid C)\\) for \\(P(A \\mid B, C)\\), we get:\n\\[\nP(A, B \\mid C) = P(A \\mid C) \\cdot P(B \\mid C)\n\\]"
  },
  {
    "objectID": "lectures/prep/math.html#conditional-independence-equivalent-statements-2",
    "href": "lectures/prep/math.html#conditional-independence-equivalent-statements-2",
    "title": "Mathematics intro",
    "section": "Conditional independence equivalent statements",
    "text": "Conditional independence equivalent statements\n✅ Conclusion:\n\\[\nP(A \\mid B, C) = P(A \\mid C) \\quad \\Longleftrightarrow \\quad P(A, B \\mid C) = P(A \\mid C) \\cdot P(B \\mid C)\n\\]\nas required."
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#todays-lectures",
    "href": "lectures/day2-scms/lec1.html#todays-lectures",
    "title": "Causal Directed Acylic Graphs",
    "section": "Today’s lectures",
    "text": "Today’s lectures\n\nintroduce 1.5 new framework based on\n\ncausal Directed Acyclic Graphs (DAGs)\nStructral Causal Models (SCMs)\n\ncounterfactuals and Pearl’s Causal Hierarchy of questions\nlectures will follow Pearl’s book Causality Pearl (2009), specifically chapters 3 (DAGs) and 7 (SCMs)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#causal-inference-frameworks",
    "href": "lectures/day2-scms/lec1.html#causal-inference-frameworks",
    "title": "Causal Directed Acylic Graphs",
    "section": "Causal inference frameworks",
    "text": "Causal inference frameworks\nWhat are they for?\nMathematical language to\n\ndefine causal quantities\nexpress assumptions\nderive how to estimate causal quantities"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#causal-inference-frameworks-1",
    "href": "lectures/day2-scms/lec1.html#causal-inference-frameworks-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Causal inference frameworks",
    "text": "Causal inference frameworks\nWhy learn more than one?\n\n\nOn day 1 we learned about the Potential Outcomes framework\n\nDefines causal effects in terms of (averages of) individual potential outcomes\nEstimation requires assumptions of (conditional) exchangeability and positivity / overlap and consistency\n\nThere isn’t only 1 way to think about causality, find one that ‘clicks’\nNow we will learn another framework: Structural Causal Models and causal graphs\n\ncausal relations and manipulations of variables\nDeveloped by different people initially - Judea Pearl, Peter Spirtes, Clark Glymour\nSCM approach is broader in that it can define more different types of causal questions\n\nEquivalence: given the same data and assumptions, get the same estimates"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#lecture-1-2-topics",
    "href": "lectures/day2-scms/lec1.html#lecture-1-2-topics",
    "title": "Causal Directed Acylic Graphs",
    "section": "Lecture 1 & 2 topics",
    "text": "Lecture 1 & 2 topics\n\nmotivating examples for DAGs\nwhat are DAGs\ncausal inference with DAGs\n\nwhat is an intervention\nDAG-structures: confounding, mediation, colliders\nd-separation\nback-door criterion"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#sec-example-delivery",
    "href": "lectures/day2-scms/lec1.html#sec-example-delivery",
    "title": "Causal Directed Acylic Graphs",
    "section": "Example task: are hospital deliveries good for babies?",
    "text": "Example task: are hospital deliveries good for babies?"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#example-task-are-hospital-deliveries-good-for-babies",
    "href": "lectures/day2-scms/lec1.html#example-task-are-hospital-deliveries-good-for-babies",
    "title": "Causal Directed Acylic Graphs",
    "section": "Example task: are hospital deliveries good for babies?",
    "text": "Example task: are hospital deliveries good for babies?\n\nYou’re a data scientist in a children’s hospital\nHave data on\n\ndelivery location (home or hospital)\nneonatal outcomes (good or bad)\npregnancy risk (high or low)\n\nQuestion: do hospital deliveries result in better outcomes for babies?"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#observed-data",
    "href": "lectures/day2-scms/lec1.html#observed-data",
    "title": "Causal Directed Acylic Graphs",
    "section": "Observed data",
    "text": "Observed data\n\npercentage of good neonatal outcomes\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nrisk\nlow\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nhigh\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\nbetter outcomes for babies delivered in the hospital for both risk groups"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#observed-data-1",
    "href": "lectures/day2-scms/lec1.html#observed-data-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Observed data",
    "text": "Observed data\n\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nrisk\nlow\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nhigh\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\n\n\n\n\n\nmarginal\n688 / 800 = 86%\n163 / 200 = 81.5%\n\n\n\n\nbetter outcomes for babies delivered in the hospital for both risk groups\nbut not better marginal (‘overall’)\nhow is this possible?\nwhat is the correct way to estimate the effect of delivery location?"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#new-question-hernia",
    "href": "lectures/day2-scms/lec1.html#new-question-hernia",
    "title": "Causal Directed Acylic Graphs",
    "section": "New question: hernia",
    "text": "New question: hernia\n\nfor a patient with a hernia, will they be able to walk sooner when recovering at home or when recovering in a hospital?\nobserved data: location, recovery, bed-rest"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#observed-data-2",
    "href": "lectures/day2-scms/lec1.html#observed-data-2",
    "title": "Causal Directed Acylic Graphs",
    "section": "Observed data 2",
    "text": "Observed data 2\n\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nbedrest\nno\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nyes\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\n\n\n\n\n\nmarginal\n688 / 800 = 86%\n163 / 200 = 81.5%\n\n\n\n\nmore bed rest in hospital\nwhat is the correct way to estimate the effect of location?"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#how-to-unravel-this",
    "href": "lectures/day2-scms/lec1.html#how-to-unravel-this",
    "title": "Causal Directed Acylic Graphs",
    "section": "How to unravel this?",
    "text": "How to unravel this?\n\nwe got two questions with exactly the same data\nin one example, ‘stratified analysis’ seemed best\nin the other example, ‘marginal analysis’ seemed best\nneed a language to formalize this differentness\nwith Directed Acyclic Graphs we can make our decision"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#causal-directed-acyclic-graphs",
    "href": "lectures/day2-scms/lec1.html#causal-directed-acyclic-graphs",
    "title": "Causal Directed Acylic Graphs",
    "section": "Causal Directed Acyclic Graphs",
    "text": "Causal Directed Acyclic Graphs\ndiagram that represents our assumptions on causal relations\n\nnodes are variables\narrows (directed edges) point from cause to effect\n\n\n\n\n\n\n\nFigure 1: Directed Acyclic Graph\n\n\n\n\nwhen used to convey causal assumptions, DAGs are ‘causal’ DAGs1\n\nthis is not the only use of DAGs (see day 4)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#making-dags-for-our-examples",
    "href": "lectures/day2-scms/lec1.html#making-dags-for-our-examples",
    "title": "Causal Directed Acylic Graphs",
    "section": "Making DAGs for our examples:",
    "text": "Making DAGs for our examples:\nThe pregnancy DAG\n\n\n\n\n\n\nFigure 2\n\n\n\n\nassumptions:\n\nwomen with high risk of bad neonatal outcomes (pregnancy risk) are referred to the hospital for delivery\nhospital deliveries lead to better outcomes for babies as more emergency treatments possible\nboth pregnancy risk and hospital delivery cause neonatal outcome\n\nthe other variable pregnancy risk is a common cause of the treatment (hospital delivery) and the outcome (this is called a confounder)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#making-dags-for-our-examples-1",
    "href": "lectures/day2-scms/lec1.html#making-dags-for-our-examples-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Making DAGs for our examples:",
    "text": "Making DAGs for our examples:\nThe hernia DAG\n\n\n\n\n\n\nFigure 3\n\n\n\n\nassumptions:\n\npatients admitted to the hospital keep more bed rest than those who remain at home\nbed rest leads to lower recovery times thus less walking patients after 1 week\n\nthe other variable bed rest is a mediator between the treatment (hospitalized) and the outcome"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#causal-dags-to-the-rescue",
    "href": "lectures/day2-scms/lec1.html#causal-dags-to-the-rescue",
    "title": "Causal Directed Acylic Graphs",
    "section": "Causal DAGs to the rescue",
    "text": "Causal DAGs to the rescue\n\nthe other variable was:\n\na common cause (confounder) of the treatment and outcome in the pregnancy example\na mediator between the treatment and the outcome in the hernia example\n\nusing our background knowledge we could see something is different about these examples\nthis insight prompted us to a different analysis\nnext: ground this in causal theory and see implications for analysis"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-convey-two-types-of-assumptions",
    "href": "lectures/day2-scms/lec1.html#dags-convey-two-types-of-assumptions",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs convey two types of assumptions:",
    "text": "DAGs convey two types of assumptions:\ncausal direction and conditional independence\n\ncausal direction: what causes what?\n\n\n\n\n\n\n\n\n\n\nFigure 4: DAG 1\n\n\n\n\n\n\n\n\nDAG 2\n\n\n\n\n\n\nread Figure 4 as\n\nsprinkler on may (or may not) cause wet floor\nwet floor cannot cause sprinkler on"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#basic-dag-patterns-fork",
    "href": "lectures/day2-scms/lec1.html#basic-dag-patterns-fork",
    "title": "Causal Directed Acylic Graphs",
    "section": "Basic DAG patterns: fork",
    "text": "Basic DAG patterns: fork\n\n\n\n\n\n\n\n\n\nFigure 5: fork / confounder\n\n\n\n\n\n\n\\(Z\\) causes both \\(X\\) and \\(Y\\) (common cause / confounder)\n\\(Z\\) = sun rises, \\(X\\) = rooster crows, \\(Y\\) = temperature rises\n\\(X \\mathrel{\\not\\!\\perp} Y\\) (i.e. \\(X\\) and \\(Y\\) are dependent)\n\\(X \\perp Y | Z\\) (conditioning on the sun rising, the rooster crowing has no information on the temperature)\n\\(Z \\to X\\) is a back-door: a path between \\(X\\) and \\(Y\\) that starts with an arrow into \\(X\\)\ntypically want to adjust for \\(Z\\) (see later 6.4)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#basic-dag-patterns-chain",
    "href": "lectures/day2-scms/lec1.html#basic-dag-patterns-chain",
    "title": "Causal Directed Acylic Graphs",
    "section": "Basic DAG patterns: chain",
    "text": "Basic DAG patterns: chain\n\n\n\n\n\n\n\n\n\nFigure 6: chain / mediation\n\n\n\n\n\n\n\\(M\\) mediates effect of \\(X\\) on \\(Y\\)\n\\(X\\): student signs up for causal inference course, \\(M\\): student studies causal inference, \\(Y\\): student understands causal inference\n\\(X \\mathrel{\\not\\!\\perp} Y\\) (i.e. \\(X\\) and \\(Y\\) are dependent)\n\\(X \\perp Y | M\\)\ntypically do not want to adjust for \\(M\\) when estimating total effect of \\(X\\) on \\(Y\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#basic-dag-patterns-collider",
    "href": "lectures/day2-scms/lec1.html#basic-dag-patterns-collider",
    "title": "Causal Directed Acylic Graphs",
    "section": "Basic DAG patterns: collider",
    "text": "Basic DAG patterns: collider\n\n\n\n\n\n\n\n\n\nFigure 7: collider\n\n\n\n\n\n\n\\(X\\) and \\(Y\\) both cause \\(Z\\)\n\\(X \\perp Y\\) (but NOT when conditioning on \\(Z\\))\noften do not want to condition on \\(Z\\) as this induces a correlation between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#collider-bias---tinder",
    "href": "lectures/day2-scms/lec1.html#collider-bias---tinder",
    "title": "Causal Directed Acylic Graphs",
    "section": "Collider bias - Tinder",
    "text": "Collider bias - Tinder\n\n\n\n\n\n\n\n\n\n\n\n\n(a) collider\n\n\n\n\n\nFigure 8: \\[\\begin{align}\n    \\text{intelligent} &\\sim U[0,1] \\\\\n    \\text{attractive}  &\\sim U[0,1] \\\\\n    \\text{on tinder}   &= I_{\\text{intelligent} + \\text{attractive} &lt; 1}\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#conditioning-on-a-collider-creates-dependence-of-its-parents",
    "href": "lectures/day2-scms/lec1.html#conditioning-on-a-collider-creates-dependence-of-its-parents",
    "title": "Causal Directed Acylic Graphs",
    "section": "Conditioning on a collider creates dependence of its parents",
    "text": "Conditioning on a collider creates dependence of its parents\n\nmay not be too visible: doing an analysis in a selected subgroup is a form of (‘invisible’) conditioning\ne.g. when selecting only patients in the hospital\n\nbeing admitted to the hospital is a collider (has many different causes, e.g. traffic accident or fever)\nusually only one of these is the reason for hospital admission\nthe causes for hospital admission now seem anti-correlated"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-convey-two-types-of-assumptions-1",
    "href": "lectures/day2-scms/lec1.html#dags-convey-two-types-of-assumptions-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs convey two types of assumptions:",
    "text": "DAGs convey two types of assumptions:\ncausal direction and conditional independence\n\nconditional indepence (e.g. exclusion of influence / information)\n\n\n\n\n\n\n\n\n\n\nFigure 10: DAG 1\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: DAG 2\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: DAG 3\n\n\n\n\n\n\n\nFigure 10 says fire can only cause wet floor through sprinkler on\n\nthis implies fire is independent of wet floor given sprinkler on and can be tested!\n\nFigure 11 says there may be other ways through which fire causes wet floor\n\nFigure 11 is thus a weaker assumption than Figure 10\n\nFigure 12 is also compatible with Figure 11"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-are-non-parametric",
    "href": "lectures/day2-scms/lec1.html#dags-are-non-parametric",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs are ‘non-parametric’",
    "text": "DAGs are ‘non-parametric’\nThey relay what variable ‘listens’ to what, but not in what way\n\n\n\n\n\n\nDAG\n\n\n\n\n\nthis DAG says \\(Y\\) is a function of \\(X,T\\) and external noise \\(U_Y\\), or:\n\\(Y = f_Y(X,T,U_Y)\\)\nin the next lecture we’ll talk more about these ‘structural equations’"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-are-non-parametric-1",
    "href": "lectures/day2-scms/lec1.html#dags-are-non-parametric-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs are ‘non-parametric’",
    "text": "DAGs are ‘non-parametric’\nThey relay what variable ‘listens’ to what, but not in what way\n\n\n\n\n\n\nDAG\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Three datasets with the same DAG\n\n\n\n\n\n\n\n\n\\(Y = T + 0.5 (X - \\pi) + \\epsilon\\) (linear)\n\\(Y = T + \\sin(X) + \\epsilon\\) (non-linear additive)\n\\(Y = T * \\sin(X) - (1-T) \\sin(x) + \\epsilon\\) (non-linear + interaction)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#sec-def-intervention",
    "href": "lectures/day2-scms/lec1.html#sec-def-intervention",
    "title": "Causal Directed Acylic Graphs",
    "section": "The DAG definition of an intervention",
    "text": "The DAG definition of an intervention\nassume this is our DAG for a situation and we want to learn the effect \\(T\\) has on \\(Y\\)\n\nin the graph, intervening on variable \\(T\\) means removing all incoming arrows\nthis assumes such a modular intervention is possible: i.e. leave everything else unaltered\n\n\n\n\n\n\n\n\n\n\nFigure 14: observational data\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: intervened DAG\n\n\n\n\n\n\n\nwhich means \\(T\\) does not listen to other variables anymore, but is set at a particular value, like in an experiment\nimagining this scenario requires a well-defined treatment variable (akin to consistency)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#intervention-example-hospital-deliveries",
    "href": "lectures/day2-scms/lec1.html#intervention-example-hospital-deliveries",
    "title": "Causal Directed Acylic Graphs",
    "section": "Intervention example: hospital deliveries",
    "text": "Intervention example: hospital deliveries\n\n\n\n\n\n\n\n\n\nFigure 16: observ(ational /ed) data: hospital delivery depends on pregnancy outcome risk\n\n\n\n\n\n\n\n\n\n\n\nFigure 17: hypothetical situation: send all pregnancies to hospital or home, regardless of risk\n\n\n\n\n\n\n\nthis is called graph surgery because we cut all the arrows going to the treatment (hospital delivery)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#from-graph-to-data",
    "href": "lectures/day2-scms/lec1.html#from-graph-to-data",
    "title": "Causal Directed Acylic Graphs",
    "section": "From graph to data",
    "text": "From graph to data\n\nwe now have a graphical definition of an intervention, how to map this onto data?\n\n\n\n\n\n\n\n\nAll we need is basic probability applied to the DAG\n\n\n\nproduct rule: \\(P(A,B) = P(A|B)P(B)\\)\nsum rule: \\(P(A) = \\sum_B P(A|B)P(B)\\)\ntotal probability: \\(P(A|C)  = \\sum_B P(A|B,C)P(B|C)\\)\n\nSee the preporatory math lecuture"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-imply-a-causal-factorization-of-the-joint-distribution",
    "href": "lectures/day2-scms/lec1.html#dags-imply-a-causal-factorization-of-the-joint-distribution",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs imply a causal factorization of the joint distribution",
    "text": "DAGs imply a causal factorization of the joint distribution\n\nassume these variables \\(T\\): treatment, \\(Y\\): outcome, \\(Z\\): ‘other’ variable\nthe product rule allows us to write this joint in many (9) different factorizations, \\(P(Y,T,Z)=\\)\n\n\\(P(Y|T,Z)P(T,Z)\\)\n\\(P(Z|T,Y)P(T,Y)\\)\n\\(P(Y|T,Z)P(T|Z)P(Z)\\)\n…\n\nwhereas all of these are correct, knowing the DAG, one of these is special: the causal factorization"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#dags-imply-a-causal-factorization-of-the-joint-distribution-1",
    "href": "lectures/day2-scms/lec1.html#dags-imply-a-causal-factorization-of-the-joint-distribution-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "DAGs imply a causal factorization of the joint distribution",
    "text": "DAGs imply a causal factorization of the joint distribution\n\n\n\n\n\n\n\n\n\nFigure 18: observational data\n\n\n\n\n\n\\[\\begin{align}\n    P(Y,T,Z) &= P(Y|T,Z)P(T,Z) \\\\\n             &= P(Y|T,Z)P(T|Z)P(Z)\n\\end{align}\\]\n\n\n2 times the product rule\n\n\n\n\n\nIf this looks complicated: just follow the arrows, starting with variables with no incoming arrows"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery",
    "href": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery",
    "title": "Causal Directed Acylic Graphs",
    "section": "Intervention as graph surgery",
    "text": "Intervention as graph surgery\nWhy is the causal factorization special?\n\n\n\n\n\n\n\n\n\nFigure 19: observational data\n\n\n\n\n\n\n\n\n\n\n\nFigure 20: intervened DAG\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n      P_{\\text{obs}}(Y,T,Z) &= P(Y|T,Z)\\color{red}{P(T|Z)}P(Z)\n\\end{align}\\]\n\n\n\\[\\begin{align}\n      P_{\\text{int}}(Y,T,Z) &= P(Y|T,Z)\\color{green}{P(T)}P(Z)\n\\end{align}\\]\n\n\n\n\nin the causal factorization, intervening on \\(T\\) means changing only one of the conditionals in the factorization, the others remain the same\nthis is what is meant with a modular intervention"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery-1",
    "href": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Intervention as graph surgery",
    "text": "Intervention as graph surgery\nConnection with probabilities\n\nthe conditional distribution of \\(Y\\) given \\(T\\) is denoted as \\(P(Y|T)\\) (‘seeing’)\nthe causal effect of \\(T\\) on \\(Y\\) is denoted \\(P(Y|\\text{do}(T))\\), which is \\(Y\\) given \\(T\\) in the graph where all arrows coming in to \\(T\\) are removed (‘doing’)\nwe compute this from the truncated factorization, which comes from the causal factorization by removing \\(P(T|Z)\\):\n\ncausal factorization: \\(P(Y|T,Z)P(T|Z)P(Z)\\)\ntruncated factorization: \\(P(Y|T,Z)P(Z)\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery-2",
    "href": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery-2",
    "title": "Causal Directed Acylic Graphs",
    "section": "Intervention as graph surgery",
    "text": "Intervention as graph surgery\nChanged distribution\n\n\n\n\n\n\n\n\n\nFigure 21: observational data\n\n\n\n\n\n\n\n\n\n\n\nFigure 22: intervened DAG\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n      P_{\\text{obs}}(Y,T,Z) &= P(Y|T,Z)\\color{red}{P(T|Z)}P(Z) \\\\\n        P_{\\text{obs}}(Y|T) &= \\sum_{z} P(Y|T,Z=z)P(Z=z|T)\n\\end{align}\\]\n\n\n\\[\\begin{align}\n      P_{\\text{int}}(Y,T,Z) &= P(Y|T,Z)\\color{green}{P(T)}P(Z) \\\\\n        P_{\\text{int}}(Y|T) &= \\sum_{z} P(Y|T,Z=z)P(Z=z|T) \\\\\n               &\\class{fragment}{= \\sum_{z} P(Y|T,Z=z)\\color{green}{P(Z)}} \\\\\n               &\\class{fragment}{= P(Y|\\text{do}(T))}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery---changed-distribution",
    "href": "lectures/day2-scms/lec1.html#intervention-as-graph-surgery---changed-distribution",
    "title": "Causal Directed Acylic Graphs",
    "section": "Intervention as graph surgery - changed distribution",
    "text": "Intervention as graph surgery - changed distribution\n\n\n\n\n\n\n\n\n\nFigure 23: observational data\n\n\n\n\\[P_{\\text{obs}}(Y|T) = \\sum_{z} P(Y|T,Z=z)\\color{red}{P(Z=z|T)}\\]\n\n\n\n\n\n\n\n\nFigure 24: intervened DAG\n\n\n\n\\[P_{\\text{int}}(Y|T) = \\sum_{z} P(Y|T,Z=z)\\color{green}{P(Z=z)} \\qquad(1)\\]\n\n\n\n\nin \\(P_{\\text{obs}}\\), \\(P(Z|T) \\color{red}{\\neq} P(Z)\\)\nin \\(P_{\\text{int}}\\), \\(P(Z|T) \\color{green}{=} P(Z)\\)\nthereby \\(P_{\\text{obs}}(Y|T) \\neq P_{\\text{int}}(P(Y|T)) = P(Y|\\text{do}(T))\\)\nseeing is not doing\nlooking at Equation 1, we can compute these from \\(P_{\\text{obs}}\\)! (this is what is called an estimand)"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#back-to-example-1",
    "href": "lectures/day2-scms/lec1.html#back-to-example-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Back to example 1",
    "text": "Back to example 1\nSeeing\n\n\n\n\n\n\nDAG\n\n\n\n\n\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nrisk\nlow\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nhigh\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\n\n\n\n\n\nmarginal\n688 / 800 = 86%\n163 / 200 = 81.5%\n\n\n\n\n\n\n\nseeing: \\(P(\\text{outcome}|\\text{location}) = \\sum_{\\text{risk}} P(\\text{outcome}|\\text{location},\\text{risk})P(\\text{risk}|\\text{location})\\)\n\\(P(\\text{risk}=\\text{low} | \\text{location} = \\text{hospital})=10\\%\\)\n\\(P(\\text{risk}=\\text{low} | \\text{location} = \\text{home})=90\\%\\)\n\n\n\\[\\begin{align}\nP(\\text{outcome}|\\text{location} = \\text{hospital}) &= 95 * 0.1 + 80 * 0.9 = 81.5\\% \\\\\n     P(\\text{outcome}|\\text{location} = \\text{home}) &= 90 * 0.9 + 50 * 0.1 = 86\\%\n\\end{align}\\]\n\n\nconclusion: deliveries in the hospital had worse neonatal outcomes"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#back-to-example-1-1",
    "href": "lectures/day2-scms/lec1.html#back-to-example-1-1",
    "title": "Causal Directed Acylic Graphs",
    "section": "Back to example 1",
    "text": "Back to example 1\n\n\n\n\n\n\nDAG\n\n\n\n\n\n\n\n\n\nlocation\n\n\n\n\n\n\n\nhome\nhospital\n\n\nrisk\nlow\n648 / 720 = 90%\n19 / 20 = 95%\n\n\n\nhigh\n40 / 80 = 50%\n144 / 180 = 80%\n\n\n\n\n\n\n\n\n\nmarginal\n688 / 800 = 86%\n163 / 200 = 81.5%\n\n\n\n\n\n\n\nestimand: \\(P(\\text{outcome}|\\text{do}(\\text{location})) = \\sum_{\\text{risk}} P(\\text{outcome}|\\text{location},\\text{risk})P(\\text{risk})\\)\n\\(P(\\text{risk}=\\text{low})=74\\%\\)\n\n\n\\[\\begin{align}\nP(\\text{outcome}|\\text{do}(\\text{hospital})) &= 95 * 0.74 + 80 * 0.26 = 91.1\\% \\\\\n     P(\\text{outcome}|\\text{do}(\\text{home})) &= 90 * 0.74 + 50 * 0.26 = 79.6\\%\n\\end{align}\\]\n\n\nconclusion: sending all deliveries to the hospital leads to better neonatal outcomes"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#back-to-example-2",
    "href": "lectures/day2-scms/lec1.html#back-to-example-2",
    "title": "Causal Directed Acylic Graphs",
    "section": "Back to example 2",
    "text": "Back to example 2\n\n\n\n\n\n\nDAG\n\n\n\n\n\nremoving all arrows going in to \\(T\\) results in the same DAG\nso \\(P(Y|T) = P(Y|\\text{do}(T))\\)\ni.e. use the marginals"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#the-gist-of-observational-causal-inference",
    "href": "lectures/day2-scms/lec1.html#the-gist-of-observational-causal-inference",
    "title": "Causal Directed Acylic Graphs",
    "section": "The gist of observational causal inference",
    "text": "The gist of observational causal inference\nis to take data we have to make inferences about data from a different distribution (i.e. the intervened-on distribution)\n\n\n\n\n\n\n\n\nFigure 25: observational data: data we have\n\n\n\n\n\n\n\n\n\nFigure 26: intervened DAG: what we want to know\n\n\n\n\n\ncausal inference frameworks provide a language to express assumptions\nbased on these assumptions, the framework tells us whether such an inference is possible\n\nthis is often referred to as is the effect identified\n\nand provide formula(s) for how to do so based on the observed data distribution (estimand(s))\n(one could say this is essentially assumption-based extrapolation, some researchers think this entire enterprise is anti-scientific)\nnot yet said: how to do statistical inference to estimate the estimand (much can still go wrong here)\n\ncan also be part of identification, see the following lecture on SCMs"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#when-life-gets-complicated-real",
    "href": "lectures/day2-scms/lec1.html#when-life-gets-complicated-real",
    "title": "Causal Directed Acylic Graphs",
    "section": "When life gets complicated / real",
    "text": "When life gets complicated / real\n\nBogie, James; Fleming, Michael; Cullen, Breda; Mackay, Daniel; Pell, Jill P. (2021). Full directed acyclic graph.. PLOS ONE. Figure. https://doi.org/10.1371/journal.pone.0249258.s003"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#d-separation-directional-separation",
    "href": "lectures/day2-scms/lec1.html#d-separation-directional-separation",
    "title": "Causal Directed Acylic Graphs",
    "section": "d-separation (directional-separation)",
    "text": "d-separation (directional-separation)\n\npaths\na path is a set of nodes connected by edges (\\(x \\ldots y\\))\na directed-path is a path with a constant direction (\\(x \\dots t\\))\nan unblocked-path is a path without a collider (\\(t \\ldots y\\))\na blocked-path is a path with a collider (\\(s,t, u\\))\nd(irectional)-separation of \\(x,y\\) means there is no unblocked path between them"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#d-separation-when-conditioning",
    "href": "lectures/day2-scms/lec1.html#d-separation-when-conditioning",
    "title": "Causal Directed Acylic Graphs",
    "section": "d-separation when conditioning",
    "text": "d-separation when conditioning\n\npaths with conditioning variables \\(r\\), \\(t\\)\nconditioning on variable:\n\nwhen variable is a collider: opens a path (\\(t\\) opens \\(s,t,u\\) etc.)\notherwise: blocks a path (e.g. \\(r\\) blocks \\(x,r,s\\))\n\nconditioning set \\(Z=\\{r,t\\}\\): set of conditioning variables"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#sec-backdoor",
    "href": "lectures/day2-scms/lec1.html#sec-backdoor",
    "title": "Causal Directed Acylic Graphs",
    "section": "The back-door criterion and adjustment",
    "text": "The back-door criterion and adjustment\nDefinition 3.3.1 (Back-Door) (for pairs of variables)\nA set of variables \\(Z\\) satisfies the back-door criterion relative to an ordered pair of variables \\((X,Y)\\) in a DAG if:\n\nno node in \\(Z\\) is a descendant of \\(X\\) (e.g. mediators)\n\\(Z\\) blocks every path between \\(X\\) and \\(Y\\) that contains an arrow into \\(X\\)\n\n\nTheorem 3.2.2 (Back-Door Adjustment)\nIf a set of variables \\(Z\\) satisfies the back-door criterion relative to \\((X,Y)\\), then the causal effect of \\(X\\) on \\(Y\\) is identifiable and is given by the formula\n\\[P(y|\\text{do}(x)) = \\sum_z P(y|x,z)P(z) \\qquad(2)\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#did-we-see-this-equation-before",
    "href": "lectures/day2-scms/lec1.html#did-we-see-this-equation-before",
    "title": "Causal Directed Acylic Graphs",
    "section": "Did we see this equation before?",
    "text": "Did we see this equation before?\n\nYes! When computing the effect of hospital deliveries on neonatal outcomes Equation 1\nDAGs tell us what to adjust for\nautomatic algorithms tell use whether an estimand exists and what it is\nseveral point-and-click websites for making DAGs that implement these algorithms:\n\ndagitty.net\ncausalfusion.net"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#how-about-positivity",
    "href": "lectures/day2-scms/lec1.html#how-about-positivity",
    "title": "Causal Directed Acylic Graphs",
    "section": "How about positivity",
    "text": "How about positivity\n\nbackdoor adjustment with \\(z\\) requires computing \\(P(y|x,z)\\)\nby the product rule:\n\\[P(y|x,z) = \\frac{P(y,x,z)}{P(x,z)}\\]\nthis division is only defined when \\(P(x,z) &gt; 0\\)\nwhich is the same as the positivity assumption from Day 1 in Potential Outcomes"
  },
  {
    "objectID": "lectures/day2-scms/lec1.html#references",
    "href": "lectures/day2-scms/lec1.html#references",
    "title": "Causal Directed Acylic Graphs",
    "section": "References",
    "text": "References\n\n\n\n\nPearl, Judea. 2009. Causality. Cambridge University Press."
  },
  {
    "objectID": "lectures/day2-scms/learning-objectives.html",
    "href": "lectures/day2-scms/learning-objectives.html",
    "title": "learning objectives",
    "section": "",
    "text": "learning objectives\n\nDAGs tell you what to control for when doing causal inference\nfork: induces correlations\nfork / chain / collider\nDAGs imply structural equations\nin DAG CI, an intervention is graph surgery\ncausal inference is taking data from one distribution and using it to make inferences about another"
  },
  {
    "objectID": "lectures/day2-scms/day2-outline.html",
    "href": "lectures/day2-scms/day2-outline.html",
    "title": "Lecture 1 and 2: Causal Inference with DAGs",
    "section": "",
    "text": "Causal inference frameworks\n\nWhat are they for?\nWhy learn more than one?\n\nLecture 1 & 2 topics\n\nwhat are DAGs\ncausal inference with DAGs\n\nwhat is an intervention\nDAG-structures: confounding, mediation, colliders\nd-separation\nback-door criterion\n\n\nMotivating examples (same data, different dags)\n\nExample task: are hospital deliveries good for babies?\nNew question: hernia\n\nCausal Directed Acyclic Graphs\n\ndiagram that represents our assumptions on causal relations\nCausal DAGs to the rescue\n\nBack to example 1 and 2: we got the right answer\n\n\n\n\n\nDAG definitions and properties\n\nDAGs convey two types of assumptions: causal direction and conditional independence\nDAGs are ‘non-parametric’: They relay what variable ‘listens’ to what, but not in what way\n\nDAG rules: chain, fork, collider\n\n\n\n\n\nThe DAG definition of an intervention\n\nDAGs imply a causal factorization of the joint distribution\nIntervention as graph surgery - changed distribution\n\nThe gist of observational causal inference is to take data we have to make inferences about data from a different distribution (i.e. the intervened-on distribution)\nWhen life gets complicated / real: many variable\n\nd-separation (directional-separation)\nThe back-door criterion and adjustment\nDid we see this equation before?\n\nHow about positivity"
  },
  {
    "objectID": "lectures/day2-scms/day2-outline.html#lecture-1-motivation",
    "href": "lectures/day2-scms/day2-outline.html#lecture-1-motivation",
    "title": "Lecture 1 and 2: Causal Inference with DAGs",
    "section": "",
    "text": "Causal inference frameworks\n\nWhat are they for?\nWhy learn more than one?\n\nLecture 1 & 2 topics\n\nwhat are DAGs\ncausal inference with DAGs\n\nwhat is an intervention\nDAG-structures: confounding, mediation, colliders\nd-separation\nback-door criterion\n\n\nMotivating examples (same data, different dags)\n\nExample task: are hospital deliveries good for babies?\nNew question: hernia\n\nCausal Directed Acyclic Graphs\n\ndiagram that represents our assumptions on causal relations\nCausal DAGs to the rescue\n\nBack to example 1 and 2: we got the right answer"
  },
  {
    "objectID": "lectures/day2-scms/day2-outline.html#what-are-dags",
    "href": "lectures/day2-scms/day2-outline.html#what-are-dags",
    "title": "Lecture 1 and 2: Causal Inference with DAGs",
    "section": "",
    "text": "DAG definitions and properties\n\nDAGs convey two types of assumptions: causal direction and conditional independence\nDAGs are ‘non-parametric’: They relay what variable ‘listens’ to what, but not in what way\n\nDAG rules: chain, fork, collider"
  },
  {
    "objectID": "lectures/day2-scms/day2-outline.html#from-dags-to-causal-inference",
    "href": "lectures/day2-scms/day2-outline.html#from-dags-to-causal-inference",
    "title": "Lecture 1 and 2: Causal Inference with DAGs",
    "section": "",
    "text": "The DAG definition of an intervention\n\nDAGs imply a causal factorization of the joint distribution\nIntervention as graph surgery - changed distribution\n\nThe gist of observational causal inference is to take data we have to make inferences about data from a different distribution (i.e. the intervened-on distribution)\nWhen life gets complicated / real: many variable\n\nd-separation (directional-separation)\nThe back-door criterion and adjustment\nDid we see this equation before?\n\nHow about positivity"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#in-past-lectures-on-dags",
    "href": "lectures/day2-scms/lec3-scms.html#in-past-lectures-on-dags",
    "title": "Structural Causal Models",
    "section": "In past lectures on DAGs",
    "text": "In past lectures on DAGs\n\ncausal directed acyclic graphs (DAGs) encode assumptions on what variables cause what\nan intervention is defined as a mutilation of this DAG where the treatment variable no longer ‘listens’ to its parents\na causal effect is the effect of an intervention\nDAG patterns:\n\nfork (confounding)\nchain (mediation)\ncollider\n\ntypically:\n\n\ncondition on confounders, don’t condition on mediators or colliders\n\n\nin more complex DAGs, use d-separation to check identifyability\nbackdoor criterion"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#in-this-lecture-structural-causal-models-scms",
    "href": "lectures/day2-scms/lec3-scms.html#in-this-lecture-structural-causal-models-scms",
    "title": "Structural Causal Models",
    "section": "In this lecture: structural causal models (SCMs)",
    "text": "In this lecture: structural causal models (SCMs)\n\n\n\n\\[\\begin{align}\n  U_Z, U_T, U_Y &\\sim p(U) \\\\\n  Z &= f_Z(U_Z) \\\\\n  T &= f_T(Z,U_T) \\\\\n  Y &= f_Y(T,Z,U_Y)\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#why-scms",
    "href": "lectures/day2-scms/lec3-scms.html#why-scms",
    "title": "Structural Causal Models",
    "section": "Why SCMs?",
    "text": "Why SCMs?\n\nWith DAGs we can:\n\nexpress (non-parametric) prior knowledge\nunderstand that seeing \\(\\neq\\) doing\nknow what variables to condition on for estimating treatment effect\n\nHowever,\n\nDAGs and RCTs do not cover all causal questions\nSCMs go a level deeper than DAGs\nDAGs naturally ‘arise’ from SCMs\nsome questions are not identified when only specifying a DAG, but we may have additional information that can lead to identification\nunderstand ‘identifyability’\nSCM thinking aligns [^according to me] with physical thinking about the world and is a natural way to think about causality"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#topics-of-today",
    "href": "lectures/day2-scms/lec3-scms.html#topics-of-today",
    "title": "Structural Causal Models",
    "section": "Topics of today",
    "text": "Topics of today\n\nSCMs: the world as computer programs\ninterventions are submodels\nbonus queries:\n\ncounterfactuals\n\nPearl Causal Hierarchy \nreflections on DAGs, limitations"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#think-of-the-world-as-a-computer-program-with-a-set-of",
    "href": "lectures/day2-scms/lec3-scms.html#think-of-the-world-as-a-computer-program-with-a-set-of",
    "title": "Structural Causal Models",
    "section": "Think of the world as a computer program with a set of",
    "text": "Think of the world as a computer program with a set of\n\n(endogenous) variables:\n\nsurgery = duration of surgery (hours)\nlos = length of stay in hospital post surgery (days)\nsurvival = survival time (years)\n\nbackground variables (exogenous):\n\nu_surgery, u_los, u_survival\n\nfunctions f_ for each variable which depend on its parents pa_ and its own background u_:\n\nsurgery = f_surgery(pa_surgery,u_surgery)\nlos = f_los(pa_los, u_los)\nsurvival = f_survival(pa_survival, u_survival)\n\n\n\n\nTogether these define a Structural Causal Model (see definition 7.1.1 in Pearl 2009, and further) (notation: \\(M=&lt;U,V,F&gt;\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#sec-scm1",
    "href": "lectures/day2-scms/lec3-scms.html#sec-scm1",
    "title": "Structural Causal Models",
    "section": "Structural Causal Model 1",
    "text": "Structural Causal Model 1\n\nf_surgery &lt;- function(u_surgery) { # pa_surgery = {}\n  u_surgery\n}\nf_los &lt;- function(surgery, u_los) { # pa_los = {surgery}\n  surgery + u_los\n}\nf_survival &lt;- function(surgery, los, u_survival) { # pa_survival = {sugery, los}\n  survival = los - 2 * surgery + u_survival\n}\n\nscm1 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery  = f_surgery(u_surgery)\n  los      = f_los(surgery, u_los)\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}\nscm1(2, 1, 5)\n\n\n\n surgery      los survival \n       2        3        4"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph",
    "href": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph",
    "title": "Structural Causal Models",
    "section": "Recursive Structural Causal Models imply a Directed Acyclic Graph",
    "text": "Recursive Structural Causal Models imply a Directed Acyclic Graph\nAn SCM is recursive, i.e. acyclic when following the chain of parents, you never end up at the same variable twice\n\n\n\nscm1 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery  = f_surgery(u_surgery)\n  los      = f_los(surgery, u_los)\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph-1",
    "href": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph-1",
    "title": "Structural Causal Models",
    "section": "Recursive Structural Causal Models imply a Directed Acyclic Graph",
    "text": "Recursive Structural Causal Models imply a Directed Acyclic Graph\nAn SCM is recursive, i.e. acyclic when following the chain of parents, you never end up at the same variable twice\n\n\n\nscm1 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery  = f_surgery(u_surgery)\n  los      = f_los(surgery, u_los)\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph-2",
    "href": "lectures/day2-scms/lec3-scms.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph-2",
    "title": "Structural Causal Models",
    "section": "Recursive Structural Causal Models imply a Directed Acyclic Graph",
    "text": "Recursive Structural Causal Models imply a Directed Acyclic Graph\nAn SCM is recursive, i.e. acyclic when following the chain of parents, you never end up at the same variable twice\n\n\n\nscm1 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery  = f_surgery(u_surgery)\n  los      = f_los(surgery, u_los)\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}\n\n\n\n\n\nscm1 (without specifying the f_s) and the DAG are equivalent (they describe the same knowledge of the world)\nfor the remainder, we assume recursiveness"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#submodel-and-effect-of-action",
    "href": "lectures/day2-scms/lec3-scms.html#submodel-and-effect-of-action",
    "title": "Structural Causal Models",
    "section": "Submodel and Effect of Action",
    "text": "Submodel and Effect of Action\n\nsubmodel: in scm1 replace f_los with a specific value, e.g. 7 days \n\n\nsubmodel7 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery = f_surgery(u_surgery)\n  los = 7\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}\n\nsubmodel7(2, 1, 5)\n\n\n\n surgery      los survival \n       2        7        8 \n\n\n\neffect of action: resulting SCM of submodel (notation: \\(M_x=&lt;U,V,F_x&gt;\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#submodel-and-effect-of-action-as-a-mutilated-dag",
    "href": "lectures/day2-scms/lec3-scms.html#submodel-and-effect-of-action-as-a-mutilated-dag",
    "title": "Structural Causal Models",
    "section": "Submodel and Effect of Action as a mutilated DAG",
    "text": "Submodel and Effect of Action as a mutilated DAG\nIn scm1 replace f_los with a specific value, e.g. 7 days (notation: \\(M_x\\))\n\n\n\nsubmodel7 &lt;- function(u_surgery, u_los, u_survival) {\n  surgery = f_surgery(u_surgery)\n  los = 7\n  survival = f_survival(surgery, los, u_survival)\n  c(surgery=surgery, los=los, survival=survival)\n}\n\nsubmodel7(2, 1, 5)\n\n surgery      los survival \n       2        7        8 \n\n\n\n\n\n\n\n\n\nThe DAG describes a submodel where \\(T\\) no longer ‘listens’ to any variables but is controlled to be equal to a specific value (e.g. 7)\nThe Effect of Action \\(do(X=x)\\) is defined as the submodel \\(M_x\\)."
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#specifying-a-distribution-for-exogenous-variables-u",
    "href": "lectures/day2-scms/lec3-scms.html#specifying-a-distribution-for-exogenous-variables-u",
    "title": "Structural Causal Models",
    "section": "Specifying a distribution for exogenous variables U",
    "text": "Specifying a distribution for exogenous variables U\n\nExogenous variables U represent random variation in the world.\nWe can specify a distribution for them (e.g. Gaussian, Uniform)\n\n\n\nsample_u &lt;- function() {\n    u_surgery  = runif(1,  2,  8)\n    u_los      = runif(1, -1,  7)\n    u_survival = runif(1,  8, 13)\n    c(u_surgery=u_surgery, u_los=u_los, u_survival=u_survival)\n}\nsample_u()\n\n\n\n u_surgery      u_los u_survival \n  5.299693   4.308564  12.199266 \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: 1000 random samples of U"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#a-probabilistic-causal-model-is-a-scm-with-a-distribution-over-u",
    "href": "lectures/day2-scms/lec3-scms.html#a-probabilistic-causal-model-is-a-scm-with-a-distribution-over-u",
    "title": "Structural Causal Models",
    "section": "A Probabilistic Causal Model is a SCM with a distribution over U",
    "text": "A Probabilistic Causal Model is a SCM with a distribution over U\n\nsample_pcm &lt;- function() {\n  U &lt;- sample_u()\n  V &lt;- scm1(U[['u_surgery']], U[['u_los']], U[['u_survival']])\n  c(U, V)\n}\n  \nsample_pcm()\n\n\n\n u_surgery      u_los u_survival    surgery        los   survival \n  3.019182   1.401587  10.914728   3.019182   4.420770   9.297133 \n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Realisations of endogenous variables V over random samples of U in Figure 1"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#calculating-a-treatment-effect-in-a-fully-specified-probabilistic-causal-model",
    "href": "lectures/day2-scms/lec3-scms.html#calculating-a-treatment-effect-in-a-fully-specified-probabilistic-causal-model",
    "title": "Structural Causal Models",
    "section": "Calculating a treatment effect in a fully specified probabilistic causal model",
    "text": "Calculating a treatment effect in a fully specified probabilistic causal model\n\ntake random samples from U, push forward through submodel7 and submodel3\n\n\n# N = 1e3\n# us &lt;- map(1:N, ~sample_u())\n\nv3s &lt;- map(us, ~do.call(submodel3, as.list(.x)))\nv7s &lt;- map(us, ~do.call(submodel7, as.list(.x)))\n\nv3df &lt;- v3s |&gt; map(~data.table(t(.x))) |&gt; rbindlist()\nv7df &lt;- v7s |&gt; map(~data.table(t(.x))) |&gt; rbindlist()\nv3df[, idx:=.I]\nv7df[, idx:=.I]\n\ndfa &lt;- rbindlist(list(\n  scm1=vdf,\n  submodel3=v3df,\n  submodel7=v7df\n), idcol='model')\n\ndfa[, list(mean_survival=mean(survival)), by=\"model\"]\n\n\n\n       model mean_survival\n      &lt;char&gt;         &lt;num&gt;\n1:      scm1      8.489856\n2: submodel3      3.637293\n3: submodel7      7.637293"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#recap-of-definitions",
    "href": "lectures/day2-scms/lec3-scms.html#recap-of-definitions",
    "title": "Structural Causal Models",
    "section": "Recap of definitions",
    "text": "Recap of definitions\n\nStructural Causal model:\n\nendogenous variables \\(V\\)\nexogenous (noise) variables \\(U\\)\ndeterministic functions f_i(pa_i,u_i)\n\nEffect of Action do\\((T=t)\\): submodel where f_T replaced with fixed value t\nProbabilistic Causal Model: SCM + distribution over U"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#in-the-real-world",
    "href": "lectures/day2-scms/lec3-scms.html#in-the-real-world",
    "title": "Structural Causal Models",
    "section": "In the real world",
    "text": "In the real world\n\nknowing the SCM is a super-power: you basically know everything revelant about the system, but in the real world:\nwe do not observe \\(U\\)\nwe typically do not know f_\n\nwe may be willing to place assumptions on f_ (e.g. generalized linear models)\n\nwe are presented with realizations \\(V_i\\) of this SCM over a random sample of U\n\nthis is another assumption on the sampling but this is largely orthogonal to causal inference\n\nwe may be interest in knowing:\n\nwhat is the expected survival time if we always admit patients for exactly 7 days?\n\n\n\nWhen and how might we learn the answer to such questions?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#sec-identification",
    "href": "lectures/day2-scms/lec3-scms.html#sec-identification",
    "title": "Structural Causal Models",
    "section": "Identification",
    "text": "Identification\nCausal effect identification:"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#definition-3.2.3-identifiability",
    "href": "lectures/day2-scms/lec3-scms.html#definition-3.2.3-identifiability",
    "title": "Structural Causal Models",
    "section": "Definition 3.2.3 (Identifiability)",
    "text": "Definition 3.2.3 (Identifiability)\nLet \\(Q(M)\\) be any computable quantity of a model \\(M\\).\n\nWe say that \\(Q\\) is identifiable in a class \\(\\mathbb{M}\\) of models if, for any pairs of models \\(M_1\\) and \\(M_2\\) from \\(\\mathbb{M}\\),\n\n\n\\(Q(M_1) = Q(M_2)\\) whenever \\(P_{M_1} (y) = P_{M_2} (y)\\).\n\n\nIf our observations are limited and permit only a partial set \\(F_M\\) of features (of \\(P_M(y)\\)) to be estimated,\n\n\nwe define \\(Q\\) to be identifiable from \\(F_M\\) if \\(Q(M_1) = Q(M_2)\\) whenever \\(F_{M_1} = F_{M_2}\\)."
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#idenfitication-in-pictures",
    "href": "lectures/day2-scms/lec3-scms.html#idenfitication-in-pictures",
    "title": "Structural Causal Models",
    "section": "Idenfitication in pictures",
    "text": "Idenfitication in pictures\n\nSomeone killed the priest (†), we want to know who-dunnit (\\(=Q\\))\n\nBased on prior knowledge we have 5 suspects (all the SCMs compatible with our DAG)\n\n\n\n\n\nIf we had full data, we would know it was \\(M_3\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#idenfitication-in-pictures-1",
    "href": "lectures/day2-scms/lec3-scms.html#idenfitication-in-pictures-1",
    "title": "Structural Causal Models",
    "section": "Idenfitication in pictures",
    "text": "Idenfitication in pictures\nSomeone killed the priest (†) , we want to know who-dunnit (\\(=Q\\))\nBased on prior knowledge on 5 suspects (all the SCMs compatible with our DAG)\n\nIf we had full data, we would have know it was \\(M_3\\)\nUnfortunately, it was dark an we only got a gray-scale image of the perpetrator\n\nAll our suspects (models) lead to the same partial observations\n\n\nBased on observed data and assumptions we cannot identify the answer to our question \\(Q\\),\n\n\ni.e. multiple models with different answers for \\(Q\\) fit the observed data equally well"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#not-identified-vs-estimand",
    "href": "lectures/day2-scms/lec3-scms.html#not-identified-vs-estimand",
    "title": "Structural Causal Models",
    "section": "Not identified vs estimand",
    "text": "Not identified vs estimand\n\nThe backdoor adjustment in this DAG means the correct estimand is:\n\\[\\begin{align}\n  P(Y|\\text{do}(T)) &= \\sum_{z} P(Y|T,z)P(Z=z)\n\\end{align}\\]\n\nIf we did not observe \\(Z\\), we could still come up with a latent-variable model for \\(Z\\) and a model for \\(Y|T,Z\\) and get a value.\nHowever, we can formulate multiple distinct latent variable models that each yield a different treatment effect (i.e. the output of the estimand)\nBut these latent variable models all fit the observed data equally well\nSo we cannot identify the treatment effect"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#seeing-is-not-doing",
    "href": "lectures/day2-scms/lec3-scms.html#seeing-is-not-doing",
    "title": "Structural Causal Models",
    "section": "Seeing is not doing",
    "text": "Seeing is not doing\n\n\n\n\n\n\n\n\n\nFigure 3: \\[\\begin{align}\n  P(Y|T) &= \\sum_{z} P(Y|T,z)P(Z=z|T)\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n  P(Y|T) &= \\sum_{z} P(Y|T,z)P(Z=z|T) \\\\\n         &=^2 \\sum_{z} P(Y|T,z)P(Z=z)\n\\end{align}\\]\n\n\nFigure 4: \\(^2\\) because in the intervened DAG, \\(Z\\) is independent of \\(T\\)\n\n\n\n\n\n\n\n\n\\(P(Y|\\text{do}(T)) \\neq P(Y|T)\\) is Pearl’s definition of confounding (def 6.2.1)\nthis shows why RCTs are special (i.e. no backdoor paths into \\(T\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#another-path-to-identification-parametric-assumptions",
    "href": "lectures/day2-scms/lec3-scms.html#another-path-to-identification-parametric-assumptions",
    "title": "Structural Causal Models",
    "section": "Another path to identification: parametric assumptions",
    "text": "Another path to identification: parametric assumptions\n\nfor example:\n\nassumption 1: \\(\\mathbb{M}_1\\), all SCMs with same DAG\nassumption 2: \\(\\mathbb{M}_2\\) SCMs with linear functions and Gaussian error terms\nassumption 1+2: \\(\\mathbb{M} = \\mathbb{M_1} \\cap \\mathbb{M_2}\\) (DAG + linear gaussian)\n\nmany more effects are identified in this setting\n‘works’ with unobserved confounding, positivity violations\ncaveats:\n\nmuch harder to determine identifyability (no analogue of backdoor-rule)\nprefer weaker assumptions over stronger assumption"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#counterfactuals",
    "href": "lectures/day2-scms/lec3-scms.html#counterfactuals",
    "title": "Structural Causal Models",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nall of the above can be achieved with DAGs, but we haven’t used SCMs unique power yet: counterfactuals\nRCT / DAG questions: What is the expected survival if we keep all patients in the hospital for 7 days?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#take-it-one-level-higher-counterfactuals",
    "href": "lectures/day2-scms/lec3-scms.html#take-it-one-level-higher-counterfactuals",
    "title": "Structural Causal Models",
    "section": "Take it one level higher: counterfactuals",
    "text": "Take it one level higher: counterfactuals\n\n\n\nFor patient Adam we had this data:\n\nsurgery duration: 4 hours\nlength of stay: 3 days\nsurvival: 4 years\n\n\nFor patient Zoe we had this data:\n\nsurgery duration: 4 hours\nlength of stay: 3 days\nsurvival: 7.5 years\n\n\n\n\nwe do not observe Adam’s/Zoe’s U\nWhat would the expected survival have been had Adam/Zoe been kept in the hospital for 7 days?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#adam-versus-zoe",
    "href": "lectures/day2-scms/lec3-scms.html#adam-versus-zoe",
    "title": "Structural Causal Models",
    "section": "Adam versus Zoe",
    "text": "Adam versus Zoe\n\nAverage causal effects in subgroup with surgery=4:\n\n3-days LOS: 5.5\n7-days LOS: 9.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhat do we expect for Adam and Zoe if they would have been kept in the hospital for 7 days?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#computing-counterfactuals-with-scms",
    "href": "lectures/day2-scms/lec3-scms.html#computing-counterfactuals-with-scms",
    "title": "Structural Causal Models",
    "section": "Computing counterfactuals with SCMs",
    "text": "Computing counterfactuals with SCMs\n\nGiven our information on the structural equation for survival (Section 2.2): \\[\\text{survival} = \\text{los} - 2*\\text{surgery} + u_{\\text{survival}}\\]\nand observed values on Adam’s and Zoe’s surgery AND survival following los=3\nwe can compute their individual \\(u_{\\text{survival}}\\):\n\n\n\n\n\n\npatient\nsurgery\nlos\nsurvival\n\n\n\n\nAdam\n4\n3\n4\n\n\nZoe\n4\n3\n7.5\n\n\n\n\n\n\n\n\npatient\nsurgery\nlos\nsurvival\nu_survival\n\n\n\n\nAdam\n4\n3\n4\n9\n\n\nZoe\n4\n3\n7.5\n12.5\n\n\n\n\n\n\n\n\npatient\nsurgery\nlos\nsurvival\nu_survival\nsurvival7\n\n\n\n\nAdam\n4\n3\n4\n9\n8\n\n\nZoe\n4\n3\n7.5\n12.5\n11.5\n\n\n\n\n\n\nand (counterfactual) survival under 7 days LOS"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#computing-counterfactuals",
    "href": "lectures/day2-scms/lec3-scms.html#computing-counterfactuals",
    "title": "Structural Causal Models",
    "section": "Computing counterfactuals",
    "text": "Computing counterfactuals\n\nnotation: \\(P(Y_{t'}  = y' | T=t,Y=y)\\) where \\(Y_{t'}\\) means “set \\(T=t'\\) through intervention”\nsteps:\n\nAbduction (update \\(P(U)\\) from observed evidence)\nAction (modify the treatment)\nPrediction (calculate outcomes in submodel, putting in the updated \\(P(U)\\))"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#pearls-causal-hierarchy-of-questions",
    "href": "lectures/day2-scms/lec3-scms.html#pearls-causal-hierarchy-of-questions",
    "title": "Structural Causal Models",
    "section": "Pearl’s Causal Hierarchy (of questions)",
    "text": "Pearl’s Causal Hierarchy (of questions)\nIf you have data to solve the upper, you can solve the lower ranks too (Bareinboim et al. 2022)\n\ncounterfactuals\ninterventions\nassociations"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#where-do-we-get-this-knowledge-from",
    "href": "lectures/day2-scms/lec3-scms.html#where-do-we-get-this-knowledge-from",
    "title": "Structural Causal Models",
    "section": "Where do we get this knowledge from?",
    "text": "Where do we get this knowledge from?\n\nnot from observational data\nnot from RCTs\nfrom assumptions\ncan get bounds from combinations of RCT data and observational data\ncaveat: some say the hierarchy is upside down because you go further away from data and closer to unverifiable assumptions the ‘higher’ you get"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#not-covered-but-also-possible",
    "href": "lectures/day2-scms/lec3-scms.html#not-covered-but-also-possible",
    "title": "Structural Causal Models",
    "section": "Not covered but also possible:",
    "text": "Not covered but also possible:\n\nDAGs:\n\nsoft intervention: don’t set treatment to fixed value but replace function with other function of variables\nexpress patterns for missing data by including missingness indicators\n\nSCMs:\n\nprobability of sufficiency\nprobability of necessity"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms.html#references",
    "href": "lectures/day2-scms/lec3-scms.html#references",
    "title": "Structural Causal Models",
    "section": "References",
    "text": "References\n\n\n\n\nBareinboim, Elias, Juan Correa, Duligur Ibeling, and Thomas Icard. 2022. “On Pearl’s Hierarchy and the Foundations of Causal Inference (1st Edition).” In Probabilistic and Causal Inference: The Works of Judea Pearl, edited by Hector Geffner, Rita Dechter, and Joseph Halpern, 507–56. ACM Books.\n\n\nPearl, Judea, ed. 2009. “The Logic of Structure-Based Counterfactuals.” In Causality, 2nd ed., 201–58. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511803161.009."
  },
  {
    "objectID": "notes_2026.html",
    "href": "notes_2026.html",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "",
    "text": "math prep: linearity of expected value"
  },
  {
    "objectID": "practicals/30_testyourknowledge/dagscm.html",
    "href": "practicals/30_testyourknowledge/dagscm.html",
    "title": "Test your knowledge",
    "section": "",
    "text": "Question 1: Say \\(Y\\) depends on treatment \\(T\\) and covariate \\(X\\), how to encode a statistical interaction between \\(X\\) and \\(T\\) in a DAG?\n\n\n\n\n\nanswer: the DAG is not different from ‘non-interaction’ DAGs, just encode \\(X \\to Y\\) and \\(T \\to Y\\) (remember, DAGs are non-parametric)"
  },
  {
    "objectID": "practicals/30_testyourknowledge/dagscm.html#my-goals",
    "href": "practicals/30_testyourknowledge/dagscm.html#my-goals",
    "title": "Test your knowledge",
    "section": "my goals",
    "text": "my goals"
  },
  {
    "objectID": "practicals/30_testyourknowledge/dagscm.html#conclusion",
    "href": "practicals/30_testyourknowledge/dagscm.html#conclusion",
    "title": "Test your knowledge",
    "section": "Conclusion",
    "text": "Conclusion\nYou learned ABC"
  },
  {
    "objectID": "practicals/30_testyourknowledge/dagscm.html#further-reading",
    "href": "practicals/30_testyourknowledge/dagscm.html#further-reading",
    "title": "Test your knowledge",
    "section": "Further reading",
    "text": "Further reading\nRead Chapter 9 of Pearls Causality [@pearlCausalDiagramsIdentification2009]"
  },
  {
    "objectID": "practicals/41_prediction/index.html",
    "href": "practicals/41_prediction/index.html",
    "title": "Practical on Causal Perspectives on Prediction",
    "section": "",
    "text": "Researchers built a prediction model \\(f\\) that aims to predict the risk of a heart attack (\\(=Y\\)) conditional on features \\(X=\\{\\)age,bmi\\(\\}\\) when intervening on treatment \\(T=\\)statin (assumed to be a binary variable). Assume that the model was fit on a sufficiently large training set without parametric form bias. In addition, assume this DAG:\n\n\n\ndag-bmi\n\n\nThese numbers are produced by the model \\(f\\):\n\n\n\nstatin\nage\nbmi\n\\(f\\)\n\n\n\n\n1\n50\n20\n10%\n\n\n0\n50\n20\n15%\n\n\n1\n50\n25\n20%\n\n\n0\n50\n25\n18%\n\n\n1\n55\n25\n23%\n\n\n0\n55\n25\n21%\n\n\n\nRead the following statements:\n\nfor a patient of age=50 and bmi=25 who is not using a statin, the causal effect of reducing bmi by 5 points is a risk reduction of 18-15=3%\nfor a patient of age=50 and bmi=20 who is not using a statin, the causal effect of starting a statin is a risk reduction of 15-10=5%\nfor a patient of age=50, reducing bmi from 25 to 20 causes the effect of statins to become smaller on an absolute risk scale.\nfor a patient with a bmi of 25 who is taking a statin, the causal effect of aging by 5 years is an increase in risk of 3%\n\n\n\n\n\n\n\nWhich statements are correct?\n\n\n\n\n\nanswer: 2 is correct.\n\nis incorrect as this is not the causal effect due to the unobserved confounder \\(u\\) (age,statin) is not a sufficient adjustment set for the effect of bmi on heart attack.\nFrom the DAG we see that causal effect of statin is identified, as adjustment set (age,bmi) satisfies the backdoor rule (all backdoor paths are closed).\nis incorrect because the change in effect of statin for different levels of bmi does not in itself have a causal interpretation\nThe total causal effect for age could be estimated from the data using adjustment set (bmi), but statin is a mediator from age to heart attack so (bmi,statin), which is used by \\(f\\), is not a valid adjustment set, so 4 is incorrect."
  },
  {
    "objectID": "practicals/41_prediction/index.html#pre-and-post-deployment-validation-of-prediction-models",
    "href": "practicals/41_prediction/index.html#pre-and-post-deployment-validation-of-prediction-models",
    "title": "Practical on Causal Perspectives on Prediction",
    "section": "2.1 Pre and post-deployment validation of prediction models",
    "text": "2.1 Pre and post-deployment validation of prediction models\nResearchers built a prediction model to identify patients with a high risk of developing sepsis in the hospital, a life-threatening disease. The prediction model uses the patient’s age, temperature and and blood pressure, and had good discriminative performance in the training data. The model is deployed, doctors are alerted of high risk patients and are able to prevent 90% of sepsis cases in this high risk group compared to before deployment of the model, so it is a glaring success. Post-deployment, a follow-up study is conducted to test if the model is still predicting accurately.\n\n\n\n\n\n\nGiven the above information, what will this follow-up study find in terms of model discrimination, will it go up, down, or remain the same, and why?\n\n\n\n\n\nAnswer: discrimination has gone down.\nReason: pre-deployment, the model had good discrimation which means that the risk of sepsis is markedly higher in the ‘high risk’ group compared to the low risk group. Deploying the model lead to better treatment of the high risk group, substantively reducing the risk of sepsis. Now, the ‘high risk’ and ‘low risk’ groups are much closer together in terms of rates of sepsis. As a consequence, the discriminative performance of the model has gone down.\n\n\n\n\n\n\n\n\n\nGiven the previous answer, should the researchers be worried? Is retraining the model needed?\n\n\n\n\n\nAnswer: Decreased discrimination is a sign of a succesful deployment, as the goal was to reduce the risk in the high-risk patients.\n\n\n\n\n\n\n\n\n\nWhat is the best study design to test whether deploying such a model leads to better patient outcomes?\n\n\n\n\n\nAnswer: a cluster randomized trial, where some (groups of) doctors are randomized to have access to the model and others are not\nsee also the lecture"
  },
  {
    "objectID": "practicals/41_prediction/index.html#selecting-models-for-decision-support",
    "href": "practicals/41_prediction/index.html#selecting-models-for-decision-support",
    "title": "Practical on Causal Perspectives on Prediction",
    "section": "2.2 Selecting models for decision support",
    "text": "2.2 Selecting models for decision support\nResearchers from the Netherlands developed two models for 10-year cardiovascular disease: Zrisk and Brisk. The intended use of the models is to better prescribe an expensive cholesterol lowering drug: go-lesterol. Both models use go-lesterol as an input variable, but each has a different set of other co-variates. Both models were trained on the same large observational dataset, and were tested in two external studies:\n\nA Nationwide registry in Sweden on the entire population (10 million people, 100.000 cases of heart attack during 5-year follow-up)\nAn RCT with 2000 participants where go-lesterol was assigned randomly, 10 year follow-up and 50 heart attack cases\n\nThe results on AUC (a measure of discrimination between 0.5 and 1, where higher means better) are a bit puzzling:\n\n\n\nstudy\nZrisk\nBrisk\np-value\n\n\n\n\nSweden\n0.7\n0.85\n&lt;0.00001\n\n\nRCT\n0.72\n0.65\n0.032\n\n\n\nAssume that Sweden and the Netherlands are comparable in terms of health care and heart attack rates.\n\n\n\n\n\n\nBased on this information, what model had the best AUC in the training data?\n\n\n\n\n\nAnswer: Brisk.\nThe training data and the Sweden study come from the same distribution as by assumption the countries are comparable (note also that in both studies the treatment is not assigned by randomization). The training dataset was large, so on sufficiently large sample from the same distribution, both models will demonstrate the same performance\n\n\n\n\n\n\n\n\n\nWhat model would you recommend for decision support?\n\n\n\n\n\nAnswer: Zrisk. The goal of decision support models that include the targeted treatment (go-lesterol in this example) is to predict accurately under the hypothetical intervention of assigning that treatment. This means that the RCT samples from the target distribution, whereas the Swedish registry study does not.\n\n\n\n\n\n\n\n\n\nWhat other studies can you perform to decide on the previous question?\n\n\n\n\n\nAnswer: a (cluster) RCT where either Brisk or Zrisk is used. Hypothetical study design: require doctors to fill in information on both covariate sets so that the Zrisk and Brisk scores can both be computed. On a per-patient basis, randomly report the result from Zrisk or Brisk, without reporting what model provided the risk-estimate (this design assumes that the Zrisk and Brisk models are sufficiently complex that the doctors will not be able to guess which model is providing the prediction). After sufficient follow-up time, measure whether Zrisk or Brisk has lower rates of heart attacks.\nAlso possible: use Zrisk and Brisk as potential policies in the RCT, subset the patients to either those whose randomly allocated treatment is concordant with Zrisk or Brisk, and calculate which subgroup has the lowest rate of heart attacks.\nsee also the lecture"
  },
  {
    "objectID": "practicals/21_dags/dags.html",
    "href": "practicals/21_dags/dags.html",
    "title": "Practical on DAGs",
    "section": "",
    "text": "Code\n# Install necessary packages if not already installed\nrequired_pkgs &lt;- c(\"dagitty\", \"ggplot2\", \"broom\", \"purrr\", \"dplyr\", \"data.table\", \"marginaleffects\")\ncran_repo &lt;- \"https://mirror.lyrahosting.com/CRAN/\" # &lt;- a CRAN mirror in the Netherlands, can select another one from here https://cran.r-project.org/mirrors.html\n\nfor (pkg in required_pkgs) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, repos=cran_repo)\n  }\n}\n\nsuppressPackageStartupMessages({\n  # Load packages\n  library(purrr)\n  library(broom)\n  library(dagitty)\n  library(ggplot2)\n  library(dplyr)\n  library(marginaleffects)\n  library(data.table)\n})\n\nsource(here::here(\"practicals\", \"21_dags\", \"_makedatas.R\"))\ndatas &lt;- make_datas()\nbirthw &lt;- datas[['birthw']]"
  },
  {
    "objectID": "practicals/21_dags/dags.html#birthweight-data",
    "href": "practicals/21_dags/dags.html#birthweight-data",
    "title": "Practical on DAGs",
    "section": "1.1 Birthweight data:",
    "text": "1.1 Birthweight data:\nWe’ll use the (simulated) dataset birthw with data on birthweight and survival of babies.\nThe birthw dataset contains the following variables:\n\nageover35: Indicator mother’s age over 35 years (0 = age &lt;= 35, 1 = age &gt;35)\nsmoking: Smoking status during pregnancy (0 = no, 1 = yes)\nlbwt: Low birth weight (0 = &gt;=2500grams, 1 = &lt; 2500grams)\ndeath: Neonatal death within 3 months (0 = no, 1 = yes)\n\nThe data can be downloaded here: birthw.csv"
  },
  {
    "objectID": "practicals/21_dags/dags.html#create-a-dag",
    "href": "practicals/21_dags/dags.html#create-a-dag",
    "title": "Practical on DAGs",
    "section": "1.2 Create a DAG",
    "text": "1.2 Create a DAG\n\n1.2.1 Think of a DAG that may fit this data using the observed variables\nTake a few minutes to create a DAG (collaboratively) (using e.g. dagitty.net)\n\n\n1.2.2 Are there variables that may be missing in the data but are relevant?\nIf so, add them to the DAG, and indicate that they are unobserved\n\n\n1.2.3 With your DAG, can the causal effect be estimated?\nUse e.g. dagitty.net to create your DAG and see if there are ways to estimate the causal effect."
  },
  {
    "objectID": "practicals/21_dags/dags.html#analyse-the-data",
    "href": "practicals/21_dags/dags.html#analyse-the-data",
    "title": "Practical on DAGs",
    "section": "1.3 Analyse the data",
    "text": "1.3 Analyse the data\nLet’s try some analyses on the data. We’ll fit different logistic regression models with different covariates (independent variables). Specifically, fit a model with:\n\nall observed covariates (fit_allobs)\nonly the smoking variable (fit_marginal)\n\nThese models give us estimates of (log) odds ratios for the independent variables. To translate a logistic regression model into differences in probabilities we use the avg_comparisons function from the marginaleffects package.\n\n\nCode\nrequire(marginaleffects)\n\nfit_allobs &lt;- glm(death~., data=birthw, family=\"binomial\")\nfit_marginal &lt;- glm(death~smoking, data=birthw, family=\"binomial\")\n\navg_comparisons(fit_allobs, variables=\"smoking\")\n\n\n\n Estimate Std. Error     z Pr(&gt;|z|)    S  2.5 %  97.5 %\n    -0.13     0.0179 -7.25   &lt;0.001 41.1 -0.165 -0.0948\n\nTerm: smoking\nType: response\nComparison: 1 - 0\n\n\nCode\navg_comparisons(fit_marginal, variables=\"smoking\")\n\n\n\n Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n   0.0808     0.0288 2.81  0.00502 7.6 0.0244  0.137\n\nTerm: smoking\nType: response\nComparison: 1 - 0\n\n\nThe effect estimates of fit_allobs and fit_marginal are quite different, they have different signs. How could this be explained? Which effect estimate do you think is more credible?"
  },
  {
    "objectID": "practicals/21_dags/dags.html#assume-a-dag",
    "href": "practicals/21_dags/dags.html#assume-a-dag",
    "title": "Practical on DAGs",
    "section": "1.4 Assume a DAG",
    "text": "1.4 Assume a DAG\n\n\n\n\n\n\nAssume the following DAG\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: DAG for smoking and death\n\n\n\nIn this DAG, there is another variable gene that influences both lbwt and death.\n\n\n\n\n\n\n\n\n\nHow does this DAG change the analysis? (tip: enter it in dagitty.net)\n\n\n\n\n\nanswer: the smoking-death relationship has no confounders, the marginal estimate is correct. Adjusting for lbwt ‘washes-out’ part of smoking’s effect because lbwt is a mediator. Also, lbwt is a collider between gene and smoking, and gene has a direct arrow into death. Conditioning on lbwt opens a bidirected path between smoking and gene, creating a new backdoor path. So there are two reasons not to condition on lbwt: it is a mediator and a collider with an unmeasured variable\n\n\n\n\n\n\n\n\n\nSee this other DAG on the smoking question\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: birthweight DAG 2\n\n\n\n\n\n\nGiven the DAG in Figure 2, see the following regression model\n\nfit2 &lt;- glm(death~smoking+ht+ageover35, data=birthw, family=binomial)\n\n\n\n\n\n\n\nAssuming no parametric form bias, will this lead to an unbiased causal effect estimate?\n\n\n\n\n\nanswer: yes this is a correct analysis. lbwt is still a collider, but it does not open any new back-door paths because gene no longer has a direct effect on death and all variables other than smoking that do have such an arrow are in the conditioning set (ageover35,ht) so these paths are blocked"
  },
  {
    "objectID": "practicals/21_dags/dags.html#non-coding-questions",
    "href": "practicals/21_dags/dags.html#non-coding-questions",
    "title": "Practical on DAGs",
    "section": "2.1 Non-coding questions",
    "text": "2.1 Non-coding questions\n\n2.1.1 Strength of Assumptions\n\n\n\n\n\n\n\n\n\n\n\n(a) DAG\n\n\n\n\n\n\n\n\n\n\n\n(b) DAG\n\n\n\n\n\n\n\n\n\n\n\n(c) DAG\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n\nWhat is the correct ordering of the strength of assumptions in the above DAGs, starting with the strongest assumption\n\n\n\n\n\nanswer: Figure 4 (b) &gt; Figure 4 (c) &gt; Figure 4 (a)\nFigure 4 (b) is stronger than Figure 4 (c) as in the latter, it could be that the effects through \\(W\\) are all absent (remember that the presence of an arrow from A to B implies a possible effect of A on B)\nFigure 4 (c) is stronger than Figure 4 (a) as in the first, Z can only affect Y through T and W, whereas in Figure 4 (a) Z can effect Y through T and can effect Y through other paths (e.g. W)\nsee also the lecture on DAGs\n\n\n\n\n\n2.1.2 RCTs\nAccording to the DAG framework, why are RCTs especially fit for causal questions?\n\nthey are often infeasible and unethical\nthey sample data from the target distribution\nthey have better external validity than observational studies\nrandomization balances confounders\n\n\n\n\n\n\n\nWhich answers are true?\n\n\n\n\n\nanswer: 2.\nSee also the DAG lecture\nContext:\n\nThis is often said of RCTs but has no direct bearing on why they are special for causal inference\nRemember that the target distribution has no arrows going in to the treatment variable, this is what we get in a RCT\nRCTs are often critiqued as having poor external validity, because they may recruit non-random subpopulations from the target population\nThis is a subtle point, but RCTs have no confounders as there are no common causes of the treatment and the outcome. Variables that are confounders in observational studies are prognostic factors in RCTs, as they (by definition of being a confounder in an observational study) influence the outcome, but not the treatment in the RCT. Randomization balances the distribution of prognostic factors between treatment arms in expectation. In a particular RCT, observed (and unobserved) prognostic factors will always have some random variation between treatment arms. This does not reduce the validity of the RCT in terms of bias. This variation is reflected in the standard error of the estimate. In some cases, adjusting for known prognostic factors in RCTs may reduce the variance of the treatment estimate (i.e. narrowing the confidence interval), but this is an entire discussion on its own."
  },
  {
    "objectID": "practicals/21_dags/dags.html#creating-and-visualizing-a-dag",
    "href": "practicals/21_dags/dags.html#creating-and-visualizing-a-dag",
    "title": "Practical on DAGs",
    "section": "3.1 Creating and Visualizing a DAG",
    "text": "3.1 Creating and Visualizing a DAG\nLet’s create a DAG for the pregnancy example:\n\n\nCode\n# Define the DAG\ndag &lt;- dagitty(\"dag {\n  pregnancy_risk -&gt; hospital_delivery\n  pregnancy_risk -&gt; neonatal_outcome\n  hospital_delivery -&gt; neonatal_outcome\n}\")\n\n# Plot the DAG\nplot(dag)\n\n\nPlot coordinates for graph not supplied! Generating coordinates, see ?coordinates for how to set your own.\n\n\n\n\n\n\n\n\n\nThis DAG assumes that pregnancy risk influences both the likelihood of hospital delivery and neonatal outcomes, and that hospital delivery affects neonatal outcomes.\n\n3.1.1 Simulating Data\nWe will simulate data based on the DAG structure:\n\n\nCode\nset.seed(123)\n\nn &lt;- 10000\n\n# Simulate variables\npregnancy_risk &lt;- rbinom(n, 1, 0.3)  # 30% high risk\nhospital_delivery &lt;- rbinom(n, 1, 0.5 + 0.3 * pregnancy_risk)  # 50% baseline + 30% if high risk\nneonatal_outcome &lt;- rbinom(n, 1, 0.8 - 0.3 * pregnancy_risk + 0.15 * hospital_delivery)  # outcome affected by both\n\n# Create a data frame\ndf &lt;- data.table(pregnancy_risk, hospital_delivery, neonatal_outcome)\n\n\n\n\n3.1.2 Analyzing the Data\nCheck the relationships in the data:\n\n\nCode\n# Summary statistics\nsummary(df)\n\n\n pregnancy_risk   hospital_delivery neonatal_outcome\n Min.   :0.0000   Min.   :0.0000    Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000    1st Qu.:1.0000  \n Median :0.0000   Median :1.0000    Median :1.0000  \n Mean   :0.2952   Mean   :0.5859    Mean   :0.7982  \n 3rd Qu.:1.0000   3rd Qu.:1.0000    3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000    Max.   :1.0000  \n\n\nCode\n# Plot the data\nggplot(df, aes(x = factor(hospital_delivery), fill = factor(neonatal_outcome))) +\n  geom_bar(position = \"fill\") +\n  facet_grid(~ pregnancy_risk) +\n  labs(x = \"Hospital Delivery\", y = \"Proportion\", fill = \"Neonatal Outcome\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Causal Inference Using DAGs\nLet’s use the DAG to determine what to condition on to estimate the causal effect of hospital delivery on neonatal outcomes:\n\n\nCode\n# Identify adjustment set using DAGitty\nadjustmentSets(dag, exposure = \"hospital_delivery\", outcome = \"neonatal_outcome\")\n\n\n{ pregnancy_risk }\n\n\nThe output will suggest which variables to condition on to estimate the causal effect correctly. In this case, we need to condition on pregnancy_risk.\n\n\n3.1.4 Estimating the Causal Effect\nEstimate the causal effect using a regression model:\n\n\nCode\n# Fit a regression model\nmodel &lt;- glm(neonatal_outcome ~ hospital_delivery + pregnancy_risk, family = binomial, data = df)\n\n# Summarize the model\nsummary(model)\n\n\n\nCall:\nglm(formula = neonatal_outcome ~ hospital_delivery + pregnancy_risk, \n    family = binomial, data = df)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.48278    0.04048   36.63   &lt;2e-16 ***\nhospital_delivery  1.15677    0.06169   18.75   &lt;2e-16 ***\npregnancy_risk    -1.91810    0.06177  -31.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 10057.8  on 9999  degrees of freedom\nResidual deviance:  8896.9  on 9997  degrees of freedom\nAIC: 8902.9\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n3.1.5 Drawing Conclusions\nInterpret the model’s output to understand the effect of hospital delivery on neonatal outcomes, controlling for pregnancy risk.\n\n\nCode\navg_comparisons(model, variables=\"hospital_delivery\")\n\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.166     0.0084 19.8   &lt;0.001 286.7  0.15  0.183\n\nTerm: hospital_delivery\nType: response\nComparison: 1 - 0\n\n\n\n\n\n\n\n\nIs this odds ratio a correct estimate of the causal effect?\n\n\n\n\n\nanswer: no\nhint: compare the structural equation used in generating the data with the statistical analysis\nThis linear probability structural equation is not well-approximated by a linear logistic model (i.e. without interaction terms). We can model the outcome without making parametric assumptions by including an interaction term, and then extract the risk difference using avg_comparisons from package marginaleffects.\nThe correct estimate is given by:\n\n\nCode\nfull_model &lt;- glm(neonatal_outcome~hospital_delivery*pregnancy_risk, family=binomial, data=df)\navg_comparisons(full_model, variables=\"hospital_delivery\")\n\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.152    0.00869 17.5   &lt;0.001 225.2 0.135  0.169\n\nTerm: hospital_delivery\nType: response\nComparison: 1 - 0"
  },
  {
    "objectID": "practicals/21_dags/dags_bonus.html",
    "href": "practicals/21_dags/dags_bonus.html",
    "title": "Practical on DAGs, bonus exercises",
    "section": "",
    "text": "1 Causal Factorization\nTake this DAG\n\n\n\n\n\n\nFigure 1: DAG\n\n\n\n\n\n\n\n\n\nWrite down the causal factorization of the joint distribution over \\(Y,T,Z,W\\) for this DAG\n\n\n\n\n\nanswer:\n\n\\(P(Y,T,Z,W) = P(Y|T,Z)P(T|Z,W)P(Z)P(W)\\)\n\n\n\n\n\n\n\n\n\n\nDraw the mutilated DAG after intervening on \\(T\\)\n\n\n\n\n\nanswer:\n\n\n\n\n\n\nFigure 2: mutilated DAG\n\n\n\n\n\n\n\n\n\n\n\n\nWrite down the factorization for the joint in the mutilated DAG after intervening on \\(T\\)\n\n\n\n\n\nanswer:\n\n\n\n\n\n\nFigure 3: mutilated DAG\n\n\n\n\n\\(P(Y,T,Z,W) = P(Y|T,Z)P(T)P(Z)P(W)\\)"
  },
  {
    "objectID": "practicals/drafts/draft.html",
    "href": "practicals/drafts/draft.html",
    "title": "Draft practical:",
    "section": "",
    "text": "In this practical, …\nFirst we load a package\nlibrary(survival)\nIn this practical, we will also use the following two packages:"
  },
  {
    "objectID": "practicals/drafts/draft.html#non-coding-questions",
    "href": "practicals/drafts/draft.html#non-coding-questions",
    "title": "Draft practical:",
    "section": "Non-coding questions",
    "text": "Non-coding questions\n\n\n\n\n\n\nQuestion Foo\n\n\n\n\n\nanswer: bar"
  },
  {
    "objectID": "practicals/drafts/draft.html#my-goals",
    "href": "practicals/drafts/draft.html#my-goals",
    "title": "Draft practical:",
    "section": "my goals",
    "text": "my goals"
  },
  {
    "objectID": "practicals/drafts/draft.html#conclusion",
    "href": "practicals/drafts/draft.html#conclusion",
    "title": "Draft practical:",
    "section": "Conclusion",
    "text": "Conclusion\nYou learned ABC"
  },
  {
    "objectID": "practicals/drafts/draft.html#further-reading",
    "href": "practicals/drafts/draft.html#further-reading",
    "title": "Draft practical:",
    "section": "Further reading",
    "text": "Further reading\nRead Chapter 9 of Pearls Causality (Pearl 2009)"
  },
  {
    "objectID": "practicals/00_setup/setup.html",
    "href": "practicals/00_setup/setup.html",
    "title": "Setup for Causal Inference and Causal Data Science Course",
    "section": "",
    "text": "We will work with R. You can use your preferred way of working in R to do the practicals. Our preferred way is this:\n\nCreate a new folder with a good name, e.g., practicals_causal_datascience\nOpen RStudio\nCreate a new project from RStudio, which you associate with the folder\nCreate a raw_data subfolder\nCreate an R script for the current practical, e.g., introduction.R\nCreate your well-documented and well-styled code in this R script\n\nWe try to make our practicals light in the number of required packages, but the packages below are needed. You can install them via:\n\nneeded_packages &lt;- c(\n  \"data.table\", \"broom\", \"purrr\", \"dagitty\", \"ggplot2\", \"dplyr\", \"marginaleffects\",\n  \"MatchIt\",\"survey\",\"tableone\"\n)\ncran_repo &lt;- \"https://mirror.lyrahosting.com/CRAN/\" # &lt;- a CRAN mirror in the Netherlands, can select another one from here https://cran.r-project.org/mirrors.html\n\nfor (pkg in needed_packages) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, repos=cran_repo)\n  }\n}"
  },
  {
    "objectID": "practicals/22_scms/index.html",
    "href": "practicals/22_scms/index.html",
    "title": "Practical: Structural Causal Models and Meta-learners",
    "section": "",
    "text": "In this practical you’ll learn more about identification and counterfactuals using the Structural Causal Model approach, and meta-learners\n\nIdentification\n\n\n\n\n\n\nRemember the definition of identification in the lecture on SCMs:\n\n\n\n\n\nLet \\(Q(M)\\) be any computable quantity of a model \\(M\\). We say that \\(Q\\) is identifiable in a class \\(\\mathbb{M}\\) of models if, for any pairs of models \\(M_1\\) and \\(M_2\\) from \\(\\mathbb{M}\\), \\(Q(M_1) = Q(M_2)\\) whenever \\(P_{M_1} (y) = P_{M_2} (y)\\). If our observations are limited and permit only a partial set \\(F_M\\) of features (of \\(P_M(y)\\)) to be estimated, we define \\(Q\\) to be identifiable from \\(F_M\\) if \\(Q(M_1) = Q(M_2)\\) whenever \\(F_{M_1} = F_{M_2}\\).\n\n\n\nWe have two different datasets, for which we know they came from the following DAG:\n\n\n\n\n\n\nFigure 1: DAG U\n\n\n\n\n\nCode\nrequired_pkgs &lt;- c(\"marginaleffects\", \"ggplot2\", \"data.table\")\ncran_repo &lt;- \"https://mirror.lyrahosting.com/CRAN/\" # &lt;- a CRAN mirror in the Netherlands, can select another one from here https://cran.r-project.org/mirrors.html\n\nfor (pkg in required_pkgs) {\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, repos=cran_repo)\n  }\n}\n\nsuppressPackageStartupMessages({\n  # Load packages\n  library(marginaleffects)\n  library(ggplot2)\n  library(data.table)\n})\n\nsource(here::here(\"practicals\", \"22_scms\", \"_makedatas.R\"))\ndatas &lt;- make_datas()\n\ndata1 &lt;- datas[[\"data1\"]]\ndata2 &lt;- datas[[\"data2\"]]\n\n\nThe datasets can be downloaded here:\ndata1.csv\ndata2.csv\n\n\n\n\n\n\nstate the ATE in terms of expected values of ‘do’ expressions\n\n\n\n\n\nanswer: \\[\\text{ATE} = E[Y|\\text{do}(X=1)] - E[Y|\\text{do}(X=0)] \\tag{1}\\]\n\n\n\n\n\n\n\n\n\nwe did not measure \\(U\\), can we estimate this target query based on the DAG, using the observed data\n\n\n\n\n\nanswer: no, there is an open back-door path through \\(U\\) which we cannot block as we did not observe that variable\n\n\n\ndata1 and data2 come from the same DAG but from different SCMs\n\n\n\n\n\n\nHow can this be? What does this mean?\n\n\n\n\n\nanswer: the endogenous variables have the same parents, so the DAG is the same. The structural equations are different\n\n\n\nWe can estimate four features of the observed distribution: \\(P(Y=1|X=0),P(Y=1|X=1),P(Y=1),P(X=1)\\). Observe that for data1 and data2, these are approximately the same (up to sampling variation)\n\n\n\n\n\n\nEstimate them from the observed data\n\n\n\n\n\n\n\nCode\nwriteLines((paste0(\"data1\\nP(Y=1|X=0) = \", mean(data1[data1$x==0,\"y\"]), \"\\nP(Y=1|X=1) = \", mean(data1[data1$x==1,\"y\"]), \"\\nP(Y=1) = \", mean(data1[,\"y\"]), \"\\nP(X=1) = \", mean(data1[,\"x\"]))))\n\n\ndata1\nP(Y=1|X=0) = 0.338174273858921\nP(Y=1|X=1) = 0.694980694980695\nP(Y=1) = 0.523\nP(X=1) = 0.518\n\n\nCode\nwriteLines((paste0(\"data2\\nP(Y=1|X=0) = \", mean(data2[data2$x==0,\"y\"]), \"\\nP(Y=1|X=1) = \", mean(data2[data2$x==1,\"y\"]), \"\\nP(Y=1) = \", mean(data2[,\"y\"]), \"\\nP(X=1) = \", mean(data2[,\"x\"]))))\n\n\ndata2\nP(Y=1|X=0) = 0.294466403162055\nP(Y=1|X=1) = 0.686234817813765\nP(Y=1) = 0.488\nP(X=1) = 0.494\n\n\nCode\n# mean(data1[data1$x==0,\"y\"])\n# mean(data1[data1$x==1,\"y\"])\n# mean(data1[,\"y\"])\n# mean(data1[,\"x\"])\n#\n# mean(data2[data2$x==0,\"y\"])\n# mean(data2[data2$x==1,\"y\"])\n# mean(data2[,\"y\"])\n# mean(data2[,\"x\"])\n\n\n\n\n\n\n\n\n\n\n\nUse the fact that u is in the data to calculate the actual effect in both datasets, what are the answers?\n\n\n\n\n\n\n\nCode\nfit1 &lt;- glm(y~x*u, data=data1, family=binomial)\nfit2 &lt;- glm(y~x*u, data=data2, family=binomial)\navg_comparisons(fit1, variables=\"x\")\n\n\n\n Estimate Std. Error      z Pr(&gt;|z|)   S  2.5 % 97.5 %\n -0.00897     0.0675 -0.133    0.894 0.2 -0.141  0.123\n\nTerm: x\nType: response\nComparison: 1 - 0\n\n\nCode\navg_comparisons(fit2, variables=\"x\")\n\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.393      0.029 13.5   &lt;0.001 136.3 0.336   0.45\n\nTerm: x\nType: response\nComparison: 1 - 0\n\n\n\n\n\n\n\n\n\n\n\nExplain how this proves (to statistical error) that our target query was not identified\n\n\n\n\n\nanswer: there were two datasets with two different underlying models. Both yielded the same distribution in terms of observed variables \\(X,Y\\), but when using the unobserved variable \\(U\\), we could see both models had different answers to our query.\n\n\n\n\n\nCounterfactual computations\nUse the following information on patient John:\n\nage: 60\nhypertension: true\ndiabetes: true\nintervention: weight-loss program\nsurvival-time: 10\n\nIn addition to the following structural equation, where u denotes an (unobserved) exogenous noise variable, such that \\(E[u] = 0\\) (i.e. the mean is 0):\n\\[\\text{survival-time} = 120 - \\text{age} - 10*\\text{hypertension} - 15*\\text{diabetes} + 5*\\text{weight-loss-program} + u\\]\n\n\n\n\n\n\ncalculate the expected survival time for patients with the same covariate values as John when intervening to give or not give the weight-loss-program\n\n\n\n\n\nanswer:\n\\[\\begin{align}\n  E[\\text{survival-time}|\\text{do}(\\text{program}=0),...] &= E[120 - 60 - 10 - 15 + u] \\\\\n                                                   &= 120 - 60 - 10 - 15 + E[u] \\\\\n                                                   &= 35 + E[u] \\\\\n                                                   &= 35 + 0 \\\\\n                                                   &= 35 \\\\\n\\end{align}\\]\n\\[\\begin{align}\n  E[\\text{survival-time}|\\text{do}(\\text{program}=1),...] &= E[120 - 60 - 10 - 15 + 5 + u] \\\\\n                                                   &= 120 - 60 - 10 - 15 + 5 + E[u] \\\\\n                                                   &= 40 + E[u] \\\\\n                                                   &= 40 + 0 \\\\\n                                                   &= 40 \\\\\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nCalculate the survival time for John, given that he took the weight-loss-program and survived 10 year, if he would not have taken the weigth-loss-program\n\n\n\n\n\nanswer:\n\nstep 1. abduction: infer John’s u\nJohn’s expected survival time with the program (which he had) was 40 years. He lived for 10 years. We can infer that his \\(u=-30\\)\n\n\nstep 2. action: modify the treatment\nWe update his treatment status to ‘no weight-loss-program’, the formula is now the second answer to the previous question\n\n\nstep 3. predict:\nGiven John’s \\(u=-30\\) and his other observed values, we can now calculate that his expected survival time was \\(5\\) years if he would not have taken the weight-loss program.\n\n\nnote:\nin this simple linear case, the counterfactual could have been calculated directly, but in general this is not the case\n\n\n\n\n\n\nMeta-learners\n\n\n\n\n\n\nRemember the definition of the conditional average treatment effect (CATE) from lecture 4\n\n\n\n\n\n\\(\\text{CATE}(w) = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]\\)\n\n\n\n\n\n\n\n\n\nRembember the definition of the T-learner and the S-learner from lecture 4:\n\n\n\n\n\n\ndenote \\(\\tau(w) = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]\\)\nT-learner: model \\(T=0\\) and \\(T=1\\) separately (e.g. regression separetely for treated and untreated): \\[\\begin{align}\n  \\mu_0(w) &= E[Y|\\text{do}(T=0),W=w] \\\\\n  \\mu_1(w) &= E[Y|\\text{do}(T=1),W=w] \\\\\n  \\tau(w)  &= \\mu_1(w) - \\mu_0(w)\n\\end{align}\\]\nS-learner: use \\(T\\) as just another feature \\[\\begin{align}\n  \\mu(t,w) &= E[Y|T=t,W=w] \\\\\n  \\tau(w)  &= \\mu(1,w) - \\mu(0,w)\n\\end{align}\\]\n\n\n\n\nWith the following datasets:\n\n\n\n\n\n\n\n\nFigure 2: Three datasets with the same DAG\n\n\n\n\n\n\n\n\n\n\n\nwhat learning-approach would you recommend for estimating the CATE?\n\n\n\n\n\n\nS-learner with simple basemodel and no interaction (e.g. linear regression)\nS-learner with non-linear base model and no interaction term (e.g. splines / boosting / …)\nT-learner\n\nNOTE: we typically have data with multi-dimensional features and/or confounders. Having the above plot to decide on the right meta-learning approach is almost never possible."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "",
    "text": "Dates: July 7 - 11 2025"
  },
  {
    "objectID": "index.html#pre-course-preparation",
    "href": "index.html#pre-course-preparation",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Pre-course preparation",
    "text": "Pre-course preparation\n\nplease have a look at the setup-document here before the first day and make sure you have a working R installation with the required packages\nrequired math background: a minimal understanding of probability will be assumed; see the slides here: html, pdf and video-lecture (13 minutes total)"
  },
  {
    "objectID": "index.html#course-objectives",
    "href": "index.html#course-objectives",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Course objectives",
    "text": "Course objectives\nLearn causal inference and causal data science!\nPlease fill in the participant evaluation form here"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Schedule",
    "text": "Schedule\n\nDay 1: Intro & Potential Outcomes\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n09:00 - 09:30\nLecture\nCausal Inference: What, Why, and How\npdf\n\n\n09:30 - 10:45\nLecture\nIntro to Potential Outcomes I\n\n\n\n11:00 - 12:15\nPractical\nCausal assumptions\nhtml\n\n\n12:30 - 13:15\nLUNCH\n\n\n\n\n13:15 - 14:00\nLecture\nIntro to Potential Outcomes II\n\n\n\n14:15 - 15:00\nLecture\nAdjustment Methods I: Stratification, Matching and Propensity Scores\n\n\n\n15:15 - 16:30\nPractical\nAdjustment Methods I\nhtml\n\n\n\nPractical\nBonus Exercises\nhtml\n\n\n\n\n\nDay 2: DAGs and SCMs\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n\nTest-your-knowledge\nDay 1 material\ngoogle form\n\n\n09:00 - 09:45\nLecture\nIntro to DAGs I\nhtml pdf\n\n\n09:45 - 10:45\nLecture\nIntro to DAGs II\n\n\n\n11:00 - 12:30\npractical\nDrawing and Using DAGs I\nhtml\n\n\n12:30 - 13:15\nLUNCH\n\n\n\n\n13:15 - 14:00\nLecture\nStructural Causal Models\nhtml pdf\n\n\n14:15 - 15:00\nLecture\nAdjustment Methods II: Regression and Outcome Adjustment\nhtml pdf\n\n\n15:15 - 16:30\npractical\nSCMs and meta-learners\nhtml\n\n\n\nPractical\nBonus Exercises\nDAGs\n\n\n\n\n\nDay 3: Target Trial Emulation\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n\nTest-your-knowledge\nDay 2 material\ngoogle form\n\n\n09:00 - 09:45\nLecture\nIntro to Trials and Target Trials I\npdf\n\n\n10:00 - 10:45\nLecture\nTarget Trials Emulation I\n\n\n\n11:00 - 12:00\npractical\nTarget Trials in Practice I\nhtml\n\n\n12:30 - 13:15\nLUNCH\n\n\n\n\n13:15 - 14:15\nLecture\nTarget Trials Emulation II\n\n\n\n14:30 - 15:00\nLecture\nTarget Trials Emulation III\n\n\n\n15:15 - 16:30\npractical\nTarget Trials in practice II\n\n\n\n\n\n\nDay 4: Causal Data Science\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n09:00 - 09:45\nLecture\nCausal Structure Learning I\n\n\n\n10:00 - 10:45\nLecture\nCausal Structure Learning II\n\n\n\n11:00 - 12:30\npractical\nCausal Structure Learning\n\n\n\n12:30 - 13:15\nLUNCH\n\n\n\n\n13:15 - 14:15\nLecture\nCausal Perspectives on Prediction Modeling I\nhtml pdf\n\n\n14:30 - 15:00\nLecture\nCausal Perspectives on Prediction Modeling II\n\n\n\n15:15 - 16:30\npractical\nCausal Perspectives on Prediction Modeling\nhtml\n\n\n\n\n\nDay 5: Advanced Topics in Causal Inference\n\n\n\ntime\nactivity\ncontent\nlink\n\n\n\n\n09:00 - 10:45\nLecture\nMediation, Instrumental Variables and DAGs in Longitudinal settings\npdf\n\n\n11:00 - 12:00\nGroup Work\nDesign your own causal research project\n\n\n\n12:00 - 12:30\nLUNCH\n\n\n\n\n12:30 - 13:30\nGroup Work\nDiscuss causal research projects\n\n\n\n13:30 - 14:00\nLecture\nDiscussion, Q&A"
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Instructors",
    "text": "Instructors\n\nOisín Ryan (coordinator)\nBas Penning de Vries\nWouter van Amsterdam"
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "Links",
    "text": "Links\n\nCourse home on utrechtsummerschool.nl\nGithub repository for the materials of this course https://github.com/vanAmsterdam/causal-data-science-summerschool"
  },
  {
    "objectID": "index.html#license-disclaimer",
    "href": "index.html#license-disclaimer",
    "title": "Causal Inference and Causal Data Science Summerschool",
    "section": "License & disclaimer",
    "text": "License & disclaimer\nAll course materials are licensed under CC-BY-4.0.\n \nThese course materials were developed with great care. If you find any inaccuracies please contact us."
  },
  {
    "objectID": "lectures/day41-prediction/index.html#recap-causal-questions",
    "href": "lectures/day41-prediction/index.html#recap-causal-questions",
    "title": "Causal perspectives on prediction modeling",
    "section": "Recap: causal questions",
    "text": "Recap: causal questions\n\nquestions of association are of the kind:\n\nwhat is the probability of \\(Y\\) (potentially: after observing \\(X\\))?, e.g.:\n\nwhat is the chance of rain tomorrow given that is was dry today?\nwhat is the chance a patient with lung cancer lives more than 10% after diagnosis?\n\nthese hands behind your back and passively observe the world-questions\n\ncausal questions are of the kind:\n\nhow would \\(Y\\) change when we intervene on \\(T\\)?, e.g.:\n\nif we would send all pregant women to the hospital for delivery, what would happen with neonatal outcomes?\nif we start a marketing campain, by how much would our revenue increase?\n\nthese tell us what would happen if we changed something"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#examples-of-prediction-tasks",
    "href": "lectures/day41-prediction/index.html#examples-of-prediction-tasks",
    "title": "Causal perspectives on prediction modeling",
    "section": "Examples of prediction tasks",
    "text": "Examples of prediction tasks\nobserve an \\(X\\), want to know what to expect for \\(Y\\)\n1. X = patient caughs, Y = patient has lung cancer\n2. X = ECG, Y = patient has heart attack\n3. X = CT-scan, Y = patient dies within 2 years\n\n\n\n\n\nRenziehausen (2024). ECG normalized-2.jpgCoronary spasm - a cause of acute coronary syndrome with ST-segment elevation and refractory cardiogenic shock - a case report. figshare. Figure. https://doi.org/10.6084/m9.figshare.26045203.v1\n\n\n\n\n\n\n\nM Sherigar, Jagannath; Finnegan, Joseen; McManus, Damien; F Lioe, Tong; AJ Spence, Roy (2011). CT Scan of chest showing one of the lung nodules. figshare. Figure. https://doi.org/10.6084/m9.figshare.16069.v1"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#prediction-typical-approach",
    "href": "lectures/day41-prediction/index.html#prediction-typical-approach",
    "title": "Causal perspectives on prediction modeling",
    "section": "Prediction: typical approach",
    "text": "Prediction: typical approach\n\ndefine population, find a cohort\nmeasure \\(X\\) at prediction baseline\nmeasure \\(Y\\)\n\ncross-sectional (e.g. diagnosis)\nlongitudinal follow-up (e.g. survival)\n\nuse a statistical learning technique (e.g. regression, machine learning)\n\n\nfit model \\(f\\) to observed \\(\\{x_i,y_i\\}\\) with a criterion / loss function\n\n\nevaluate prediction performance with e.g. discrimination, calibration, \\(R^2\\)"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#prediction-typical-estimand",
    "href": "lectures/day41-prediction/index.html#prediction-typical-estimand",
    "title": "Causal perspectives on prediction modeling",
    "section": "Prediction: typical estimand",
    "text": "Prediction: typical estimand\nLet \\(f\\) depend on parameter \\(\\theta\\), prediction typically aims for:\n\\[f_{\\theta}(x) \\to E[Y|X=x]\\]\n\nwhen \\(Y\\) is binary:\n\nprobability of a heart attack in 10 years, given age and cholesterol\nprobability of lung cancer, given symptoms and CT-scan\ntypical evaluation metrics:\n\ndiscrimination: sensitivity, specificity, AUC\ncalibration"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#causal-inference-typical-approach",
    "href": "lectures/day41-prediction/index.html#causal-inference-typical-approach",
    "title": "Causal perspectives on prediction modeling",
    "section": "Causal inference: typical approach",
    "text": "Causal inference: typical approach\n\ndefine target population and targeted treatment comparison\nrun randomized controlled trial, randomizing treatment allocation (when possible)\nmeasure patient outcomes\nestimate parameter that summarizes average treatment effect (ATE)\n\n\ntypical estimand:\n\\[E[Y|\\text{do}(T=1)] - E[Y|\\text{do}(T=0)]\\]"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#causal-inference-versus-prediction",
    "href": "lectures/day41-prediction/index.html#causal-inference-versus-prediction",
    "title": "Causal perspectives on prediction modeling",
    "section": "Causal inference versus prediction",
    "text": "Causal inference versus prediction\n\n\nprediction\n\ntypical estimand \\(E[Y|X]\\)\ntypical study: longitudinal cohort\ntypical interpretation: \\(X\\) predicts \\(Y\\)\nprimary use: know what \\(Y\\) to expect when observing a new \\(X\\) assuming no change in joint distribution\n\n\ncausal inference\n\ntypical estimand \\(E[Y|\\text{do}(T=1)] - E[Y|\\text{do}(T=0)]\\)\ntypical study: RCT (or observational causal inference study)\ntypical interpretation: causal effect of \\(T\\) on \\(Y\\)\nprimary use: know what change in \\(Y\\) to expect when changing the treatment policy"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#what-do-we-mean-with-treatment-policy",
    "href": "lectures/day41-prediction/index.html#what-do-we-mean-with-treatment-policy",
    "title": "Causal perspectives on prediction modeling",
    "section": "What do we mean with treatment policy?",
    "text": "What do we mean with treatment policy?\nA treatment policy \\(\\pi\\) is a procedure for determining the treatment\nAssuming \\(T\\) is binary, \\(\\pi\\) can be:\n\n\\(\\pi = 0.5\\) (a 1/1 RCT)\ngive blood pressure pill to patients with hypertension:\n\n\\[\\pi(blood pressure) = \\begin{cases}\n  1, &blood pressure &gt; 140mmHg\\\\\n  0, &\\text{otherwise}\n  \\end{cases}\\]\n\ngive statins to patients with more than 10% predicted risk of heart attack:\n\n\\[\\pi(X) = \\begin{cases}\n  1, &f(X) &gt; 0.1\\\\\n  0, &\\text{otherwise}\n  \\end{cases}\\]\n\nthe propensity score can be seen as a (non-deterministic) treatment policy"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#where-can-prediction-and-causality-meet",
    "href": "lectures/day41-prediction/index.html#where-can-prediction-and-causality-meet",
    "title": "Causal perspectives on prediction modeling",
    "section": "Where can prediction and causality meet?",
    "text": "Where can prediction and causality meet?\n\nprediction has a causal interpretation\nprediction does not have a causal interpretation:\n\nbut is used for a causal task (e.g. treatment decision making)\nbut predictions can be improved with causal thinking in terms of e.g.:\n\n\ninterpretability, robustness, ‘spurious correlations’, generalization, fairness, selection bias"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#using-prediction-models-for-decision-making-is-often-thought-of-as-a-good-idea",
    "href": "lectures/day41-prediction/index.html#using-prediction-models-for-decision-making-is-often-thought-of-as-a-good-idea",
    "title": "Causal perspectives on prediction modeling",
    "section": "Using prediction models for decision making is often thought of as a good idea",
    "text": "Using prediction models for decision making is often thought of as a good idea\nFor example:\n\ngive chemotherapy to cancer patients with high predicted risk of recurrence\ngive statins to patients with a high risk of a heart attack\n\n\n\n\n\nTRIPOD+AI on prediction models (Collins et al. 2024)\n\n\n“Their primary use is to support clinical decision making, such as … initiate treatment or lifestyle changes.”"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#building-models-for-decision-support-without-regards-for-the-historic-treatment-policy-is-a-bad-idea",
    "href": "lectures/day41-prediction/index.html#building-models-for-decision-support-without-regards-for-the-historic-treatment-policy-is-a-bad-idea",
    "title": "Causal perspectives on prediction modeling",
    "section": "Building models for decision support without regards for the historic treatment policy is a bad idea",
    "text": "Building models for decision support without regards for the historic treatment policy is a bad idea"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#treatment-naive-prediction-models",
    "href": "lectures/day41-prediction/index.html#treatment-naive-prediction-models",
    "title": "Causal perspectives on prediction modeling",
    "section": "Treatment-naive prediction models",
    "text": "Treatment-naive prediction models\n\n\n\n\n\\[\\begin{align}\n    E[Y|X] \\class{fragment}{= E[E_{t~\\sim \\pi_0(X)}[Y|X,t]]}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#prediction-modeling-is-very-popular-in-medical-research",
    "href": "lectures/day41-prediction/index.html#prediction-modeling-is-very-popular-in-medical-research",
    "title": "Causal perspectives on prediction modeling",
    "section": "Prediction modeling is very popular in medical research",
    "text": "Prediction modeling is very popular in medical research"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#recommended-validation-practices-and-reporting-guidelines-do-not-protect-against-harm",
    "href": "lectures/day41-prediction/index.html#recommended-validation-practices-and-reporting-guidelines-do-not-protect-against-harm",
    "title": "Causal perspectives on prediction modeling",
    "section": "Recommended validation practices and reporting guidelines do not protect against harm",
    "text": "Recommended validation practices and reporting guidelines do not protect against harm\nbecause they do not evaluate the policy change"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#bigger-data-does-not-protect-against-harmful-prediction-models",
    "href": "lectures/day41-prediction/index.html#bigger-data-does-not-protect-against-harmful-prediction-models",
    "title": "Causal perspectives on prediction modeling",
    "section": "Bigger data does not protect against harmful prediction models",
    "text": "Bigger data does not protect against harmful prediction models"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#more-flexible-models-do-not-protect-against-harmful-prediction-models",
    "href": "lectures/day41-prediction/index.html#more-flexible-models-do-not-protect-against-harmful-prediction-models",
    "title": "Causal perspectives on prediction modeling",
    "section": "More flexible models do not protect against harmful prediction models",
    "text": "More flexible models do not protect against harmful prediction models"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#section",
    "href": "lectures/day41-prediction/index.html#section",
    "title": "Causal perspectives on prediction modeling",
    "section": "",
    "text": "What to do?"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#section-1",
    "href": "lectures/day41-prediction/index.html#section-1",
    "title": "Causal perspectives on prediction modeling",
    "section": "",
    "text": "What to do?\n\n\nEvaluate policy change (cluster randomized controlled trial)\nBuild models that are likely to have value for decision making"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#deploying-a-model-is-an-intervention-that-changes-the-way-treatment-decisions-are-made",
    "href": "lectures/day41-prediction/index.html#deploying-a-model-is-an-intervention-that-changes-the-way-treatment-decisions-are-made",
    "title": "Causal perspectives on prediction modeling",
    "section": "Deploying a model is an intervention that changes the way treatment decisions are made",
    "text": "Deploying a model is an intervention that changes the way treatment decisions are made"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#how-do-we-learn-about-the-effect-of-an-intervention",
    "href": "lectures/day41-prediction/index.html#how-do-we-learn-about-the-effect-of-an-intervention",
    "title": "Causal perspectives on prediction modeling",
    "section": "How do we learn about the effect of an intervention?",
    "text": "How do we learn about the effect of an intervention?\nWith causal inference!\n\nfor using a decision support model, the unit of intervention is usually the doctor\nrandomly assign doctors to have access to the model or not\nmeasure differences in treatment decisions and patient outcomes\nthis called a cluster RCT\nif using model improves outcomes, use that one\n\n\n\n\n\nUsing cluster RCTs to evaluated models for decision making is not a new idea (Cooper et al. 1997)\n\n\n“As one possibility, suppose that a trial is performed in which clinicians are randomized either to have or not to have access to such a decision aid in making decisions about where to treat patients who present with pneumonia.”\n\n\n\n\n\n\n\n\n\n\n\nWhat we don’t learn\n\n\nwas the model predicting anything sensible?"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#sec-policyeval",
    "href": "lectures/day41-prediction/index.html#sec-policyeval",
    "title": "Causal perspectives on prediction modeling",
    "section": "What if we cannot do this (cluster randomized) trial?",
    "text": "What if we cannot do this (cluster randomized) trial?\nOff-policy evaluation\n\nhave historic RCT data, want to evaluate new policy \\(\\pi_1\\)\n\ntarget distribution \\(p(t|x)=\\pi_1(x)\\)\nobserved distribution \\(q(t|x) = 0.5\\)\nnote: when \\(\\pi_1(x)\\) is deterministic (e.g. give the treatment when \\(f(x) &gt; 0.1\\)), we get the following:\n\nwhen randomized treatment is concordant with \\(\\pi_1\\), keep the patient (weight = 1), otherwise, remove from the data (weight = 0)\ncalculate average outcomes in the kept patients\n\nthis way, multiple alternative policies may be evaluated\n\nhave historic observational data, want to evaluate new policy \\(\\pi_1\\):\n\ntarget distribution \\(p(t|x)=\\pi_1(x)\\)\nobserved distribution \\(q(t|x) = \\pi_0(x)\\)\nwe need to estimate \\(q\\) (i.e. the propensity score), this procedure relies on the standard causal inference assumptions (no confounding, positivity)\nuse importance sampling to estimate the expected value of \\(Y\\) under \\(\\pi_1\\) from the observed data"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#what-can-we-mean-with-predictions-having-a-causal-interpretation",
    "href": "lectures/day41-prediction/index.html#what-can-we-mean-with-predictions-having-a-causal-interpretation",
    "title": "Causal perspectives on prediction modeling",
    "section": "What can we mean with predictions having a causal interpretation?",
    "text": "What can we mean with predictions having a causal interpretation?\nLet \\(f: \\mathbb{X} \\to \\mathbb{Y}\\) be a prediction model for outcome \\(Y\\) using features \\(X\\)\n\n\n\n\n\\(X\\) is an ancestor of \\(Y\\) (\\(X=\\{z_1,z_2,z_3\\}\\))\n\\(X\\) is a direct cause of \\(Y\\) (\\(X=\\{z_1,z_2\\}\\))\n\\(f: \\mathbb{X} \\to \\mathbb{Y}\\) describes the causal effect of \\(X\\) on \\(Y\\) (\\(X=\\{z_1\\}\\)), i.e.:\n\n\n\\[f(x) = E[Y|\\text{do}(X=x)]\\]\n\n\n\\(f: \\mathbb{T} \\times \\mathbb{X} \\to \\mathbb{Y}\\) describes the causal effect of \\(T\\) on \\(Y\\) conditional on \\(X\\) (\\(T=\\{z_1\\},X=\\{z_2,z_3,w\\}\\):\n\n\n\\[f(t,x) = E[Y|\\text{do}(T=t),X=x]\\]"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#interpretation-3.-all-covariates-are-causal",
    "href": "lectures/day41-prediction/index.html#interpretation-3.-all-covariates-are-causal",
    "title": "Causal perspectives on prediction modeling",
    "section": "interpretation 3. all covariates are causal",
    "text": "interpretation 3. all covariates are causal\nLet \\(f: \\mathbb{X} \\to \\mathbb{Y}\\) be a prediction model for outcome \\(Y\\) using features \\(X\\)\n\\[f(x) = E[Y|\\text{do}(X=x)]\\]\n\nthis is almost never true (i.e. back-door rule holds for all variables)\ntoo often this is assumed / interpreted this way (table 2 fallacy in health care literature)"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#example-of-table-2-fallacy-when-mis-using-qrisk",
    "href": "lectures/day41-prediction/index.html#example-of-table-2-fallacy-when-mis-using-qrisk",
    "title": "Causal perspectives on prediction modeling",
    "section": "Example of table 2 fallacy when mis-using Qrisk",
    "text": "Example of table 2 fallacy when mis-using Qrisk\nQrisk3: a risk prediction model for cardiovascular events in the coming 10-years. Widely used in the United Kingdom for deciding which patients should get statins"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#qrisk3---risks",
    "href": "lectures/day41-prediction/index.html#qrisk3---risks",
    "title": "Causal perspectives on prediction modeling",
    "section": "Qrisk3 - risks:",
    "text": "Qrisk3 - risks:\ncan go wrong when:\n\ne.g. fill in current length and weight\n\nreduce weight by 5 kgs\ninterpret difference as ‘effect of weight loss’\n\ncheck or un-check blood pressure medication\n\nobserve that with blood pressure medication, risk is higher"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#what-else-could-go-wrong",
    "href": "lectures/day41-prediction/index.html#what-else-could-go-wrong",
    "title": "Causal perspectives on prediction modeling",
    "section": "What else could go wrong?",
    "text": "What else could go wrong?\n\nQrisk3 states it is validated, but validated for what?\nQrisk3 is validated for non-use!"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#interpretation-4.-some-covariates-are-causal",
    "href": "lectures/day41-prediction/index.html#interpretation-4.-some-covariates-are-causal",
    "title": "Causal perspectives on prediction modeling",
    "section": "interpretation 4. some covariates are causal",
    "text": "interpretation 4. some covariates are causal\nor: prediction-under-intervention\n\\[f(t,x) = E[Y|\\text{do}(T=t),X=x]\\]\n\n\ninterpretation: what is the expected value of \\(Y\\) if we were to assign treatment \\(t\\) by intervention, given that we know \\(X=x\\) in this patient"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#estimand-for-prediction-under-intervention-models",
    "href": "lectures/day41-prediction/index.html#estimand-for-prediction-under-intervention-models",
    "title": "Causal perspectives on prediction modeling",
    "section": "Estimand for prediction-under-intervention models",
    "text": "Estimand for prediction-under-intervention models\nWhat is the estimand?\n\nprediction: \\(E[Y|X]\\)\naverage treatment effect: \\(E[Y|\\text{do}(T=1)] - E[Y|\\text{do}(T=0)]\\)\nconditional average treatment effect: \\(E[Y|\\text{do}(T=1),X] - E[Y|\\text{do}(T=0),X]\\)\nprediction-under-intervention: \\(E[Y|\\text{do}(T=t),X]\\)\n\nnote:\n\nfrom prediction-under-intervention models, the CATE can be derived\nin these models and the CATE: \\(T\\) has a causal interpretation, \\(X\\) does not!\n\ni.e. \\(X\\) does not cause the effect of treatment to be different"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#developing-prediction-under-intervention-models",
    "href": "lectures/day41-prediction/index.html#developing-prediction-under-intervention-models",
    "title": "Causal perspectives on prediction modeling",
    "section": "Developing prediction-under-intervention models",
    "text": "Developing prediction-under-intervention models\n\nrequires causal inference assumptions or RCTs\nsingle RCTs often not big enough, or did not measure the right \\(X\\)s\nwhen \\(X\\) is not a sufficient adjustment set, but \\(X+L\\) is, can use e.g. propensity score methods\nassumption of no unobserved confounding often hard to justify in observational data\nbut there’s more between heaven (RCT) and earth (confounder adjustment)\n\nproxy-variable methods (e.g. Miao, Geng, and Tchetgen Tchetgen 2018; van Amsterdam et al. 2022)\nconstant relative treatment effect assumption (e.g. Alaa et al. 2021; van Amsterdam and Ranganath 2023; Candido dos Reis et al. 2017)\ndiff-in-diff\ninstrumental variable analysis (Wald 1940; Puli and Ranganath 2021; Hartford et al. 2017)\nfront-door analysis\n\nnot covered now: formulating correct estimands (and getting the right data) becomes much more complicated when considering dynamic treatment decision processes (e.g. blood pressure control with multiple follow-up visits)"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#evaluation-of-prediction-under-intervention-models",
    "href": "lectures/day41-prediction/index.html#evaluation-of-prediction-under-intervention-models",
    "title": "Causal perspectives on prediction modeling",
    "section": "Evaluation of prediction-under-intervention models",
    "text": "Evaluation of prediction-under-intervention models\n\nprediction accuracy can be tested in RCTs, or in observational data with specialized methods accounting for confounding (e.g. Keogh and van Geloven 2024)\nin confounded observational data, typical metrics (e.g. AUC or calibration) are not sufficient as we want to predict well in data from other distribution than observed data (i.e. other treatment decisions)\na new policy can be evaluated in historic RCTs (e.g. Karmali et al. 2018)\nultimate test is cluster RCT\nif not perfect, likely a better recipe than treatment-naive models"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#interpretability",
    "href": "lectures/day41-prediction/index.html#interpretability",
    "title": "Causal perspectives on prediction modeling",
    "section": "Interpretability",
    "text": "Interpretability\n\nend-users (e.g. doctors) often want to understand why a prediction model returns a certain prediction\nthis has two possible interpretations:\n\nexplain the model (i.e. the computations)\nexplain the world (i.e. why is this patient at high risk of a certain outcome)\n\n\noften has a causal connotation, though achieving this is may be unfeasible as you need causal assumptions on all covariates (rember table 2 fallacy)"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#robustness-spurious-correlations-generalization",
    "href": "lectures/day41-prediction/index.html#robustness-spurious-correlations-generalization",
    "title": "Causal perspectives on prediction modeling",
    "section": "Robustness / spurious correlations / generalization",
    "text": "Robustness / spurious correlations / generalization\n\nprediction models are developed in some data, but are intended to be used elsewhere (in location, time, other)\nin causal language, shifts in distributions can be denoted as interventions on specific nodes\nprediction models that include (direct) causes may be more robust to changes as the chain between \\(X\\) and \\(Y\\) is shorter\nsome machine learning algorithms like deep learning are very good at detecting ‘background’ signals, e.g.:\n\ndetect the scanner type from a CT-scanner\n\nif hospital A has scanner type 1 and hospital B has scanner type 2\nand the outcome rates differ between the hospitals, models may (mis)use the scanner type to predict the outcome\nwhat will the model predict in hospital C? or when A or B buy a scanner of different type?\n\nmay be preventable with causality"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#fairness",
    "href": "lectures/day41-prediction/index.html#fairness",
    "title": "Causal perspectives on prediction modeling",
    "section": "Fairness",
    "text": "Fairness\n\nin the historic distribution, outcomes may be affected by unequal treatment of certain demographic groups\ninstead of perpetuating inequities, we may want to design models that diminish them\nthis means intervening in the distribution (= a causal task)\ncausality has a strong vocabulary for formalizing fairness\nactually achieving fairness is highly non-trivial, not in the least part due to unclear definitions\nchosing to not include sensitive attributes in a prediction model is often not gauranteed to improve fairness"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#selection-bias",
    "href": "lectures/day41-prediction/index.html#selection-bias",
    "title": "Causal perspectives on prediction modeling",
    "section": "Selection bias",
    "text": "Selection bias\n\nhave samples from some selected subpopulation\n\nuniversity hospital\nolder men\n\nwant to generalize to another subpopulation\n\ngeneral practitioner\nyounger women\n\nuse DAGs to express the difference between source and target population\ncalculate e.g. expected performance on target population with techniques like importance sampling"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#wrap-up",
    "href": "lectures/day41-prediction/index.html#wrap-up",
    "title": "Causal perspectives on prediction modeling",
    "section": "Wrap-up",
    "text": "Wrap-up\n\npredictions can have causal interpretations\nprediction-under-intervention: causal with respect to treatment (not covariates)\nmis-use of non-causal models for causal tasks (e.g. prediction model for treatment decisions) is perilous\n\nalways think about the policy change and its effect on outcomes\n\nevaluate policy changes with cluster RCTs, or historic RCTs and importance sampling\ncausal thinking may improve other aspects of non-causal prediction models such as robustness, fairness, generalization"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#sec-is",
    "href": "lectures/day41-prediction/index.html#sec-is",
    "title": "Causal perspectives on prediction modeling",
    "section": "Proof of importance sampling unbiasedness",
    "text": "Proof of importance sampling unbiasedness\nassuming \\(x\\) is discrete, otherwise replace sums with integrals for continuous \\(x\\)\nwant to compute the expected value of \\(g(x)\\) over distribution \\(p\\), but we have samples from another distribution \\(x \\sim q\\)\n\\[E_{x \\sim q} \\left[ \\frac{p(x)}{q(x)} g(x) \\right] = \\sum_x q(x) \\left( \\frac{p(x)}{q(x)} g(x) \\right) = \\sum_x p(x) g(x) = E_{x \\sim p} \\left[g(x) \\right]\\]\nthis assumes \\(q(x)&gt;0\\) whenever \\(p(x)&gt;0\\) for the ratio \\(p/q\\) to be defined"
  },
  {
    "objectID": "lectures/day41-prediction/index.html#references",
    "href": "lectures/day41-prediction/index.html#references",
    "title": "Causal perspectives on prediction modeling",
    "section": "References",
    "text": "References\n\n\n\n\nAlaa, Ahmed M., Deepti Gurdasani, Adrian L. Harris, Jem Rashbass, and Mihaela van der Schaar. 2021. “Machine Learning to Guide the Use of Adjuvant Therapies for Breast Cancer.” Nature Machine Intelligence, June, 1–11. https://doi.org/10/gk6bh7.\n\n\nAmsterdam, Wouter A. C. van, and Rajesh Ranganath. 2023. “Conditional Average Treatment Effect Estimation with Marginally Constrained Models.” Journal of Causal Inference 11 (1): 20220027. https://doi.org/10.1515/jci-2022-0027.\n\n\nAmsterdam, Wouter A. C. van, Joost J. C. Verhoeff, Netanja I. Harlianto, Gijs A. Bartholomeus, Aahlad Manas Puli, Pim A. de Jong, Tim Leiner, Anne S. R. van Lindert, Marinus J. C. Eijkemans, and Rajesh Ranganath. 2022. “Individual Treatment Effect Estimation in the Presence of Unobserved Confounding Using Proxies: A Cohort Study in Stage III Non-Small Cell Lung Cancer.” Scientific Reports 12 (1, 1): 5848. https://doi.org/10.1038/s41598-022-09775-9.\n\n\nCandido dos Reis, Francisco J., Gordon C. Wishart, Ed M. Dicks, David Greenberg, Jem Rashbass, Marjanka K. Schmidt, Alexandra J. van den Broek, et al. 2017. “An Updated PREDICT Breast Cancer Prognostication and Treatment Benefit Prediction Model with Independent Validation.” Breast Cancer Research 19 (1): 58. https://doi.org/10/gbhgpq.\n\n\nCollins, Gary S., Karel G. M. Moons, Paula Dhiman, Richard D. Riley, Andrew L. Beam, Ben Van Calster, Marzyeh Ghassemi, et al. 2024. “TRIPOD+AI Statement: Updated Guidance for Reporting Clinical Prediction Models That Use Regression or Machine Learning Methods.” BMJ 385 (April): e078378. https://doi.org/10.1136/bmj-2023-078378.\n\n\nCooper, Gregory F., Constantin F. Aliferis, Richard Ambrosino, John Aronis, Bruce G. Buchanan, Richard Caruana, Michael J. Fine, et al. 1997. “An Evaluation of Machine-Learning Methods for Predicting Pneumonia Mortality.” Artificial Intelligence in Medicine 9 (2): 107–38. https://doi.org/10.1016/S0933-3657(96)00367-3.\n\n\nHartford, Jason, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. 2017. “Deep IV: A Flexible Approach for Counterfactual Prediction.” In International Conference on Machine Learning, 1414–23. PMLR. https://proceedings.mlr.press/v70/hartford17a.html.\n\n\nKarmali, Kunal N., Donald M. Lloyd-Jones, Joep van der Leeuw, David C. Goff Jr, Salim Yusuf, Alberto Zanchetti, Paul Glasziou, et al. 2018. “Blood Pressure-Lowering Treatment Strategies Based on Cardiovascular Risk Versus Blood Pressure: A Meta-Analysis of Individual Participant Data.” PLOS Medicine 15 (3): e1002538. https://doi.org/10.1371/journal.pmed.1002538.\n\n\nKeogh, Ruth H., and Nan van Geloven. 2024. “Prediction Under Interventions: Evaluation of Counterfactual Performance Using Longitudinal Observational Data.” January 10, 2024. https://doi.org/10.48550/arXiv.2304.10005.\n\n\nMiao, Wang, Zhi Geng, and Eric J Tchetgen Tchetgen. 2018. “Identifying Causal Effects with Proxy Variables of an Unmeasured Confounder.” Biometrika 105 (4): 987–93. https://doi.org/10.1093/biomet/asy038.\n\n\nPuli, Aahlad Manas, and Rajesh Ranganath. 2021. “General Control Functions for Causal Effect Estimation from Instrumental Variables.” http://arxiv.org/abs/1907.03451.\n\n\nWald, Abraham. 1940. “The Fitting of Straight Lines If Both Variables Are Subject to Error.” The Annals of Mathematical Statistics 11 (3): 284–300. https://doi.org/10.1214/aoms/1177731868."
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#how-to-find-adjustment-sets",
    "href": "lectures/day2-scms/lec4.html#how-to-find-adjustment-sets",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "How to find adjustment sets?",
    "text": "How to find adjustment sets?\n\nadjustment sets:\n\nthe back-door criterion states that any set \\(Z\\) that blocks all backdoor paths from \\(X\\) to \\(Y\\) is a sufficient adjustment set for causal effect estimation of \\(P(Y|\\text{do}(X))\\) using the backdoor formula.\nhow do we find these sufficient sets?\nwhat if there are multiple?\n\nadjustment: how to do this?\n\nstratification\nwhat is regression adjustment?\nT-learner vs S-learner"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#valid-adjustment-sets",
    "href": "lectures/day2-scms/lec4.html#valid-adjustment-sets",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Valid adjustment sets",
    "text": "Valid adjustment sets\n\n\n\n\n\n\ndag\n\n\n\n\n\nin general:\n\n\\(PA_T\\) (the direct parents of treatment \\(T\\): \\(Z_1\\)) are a valid adjustment set\n\\(PA_Y\\) (the direct parents of outcome \\(Y\\): \\(Z_2\\)) are a valid adjustment set\n\nin this case:\n\n\\(W\\) is also a valid adjustment set"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#valid-adjustment-sets-picking-one",
    "href": "lectures/day2-scms/lec4.html#valid-adjustment-sets-picking-one",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Valid adjustment sets: picking one",
    "text": "Valid adjustment sets: picking one\n\nwebsites like dagitty.net and causalfusion.net provide user-friendly interfaces for creating and exporting DAGs, in addition:\n\nvalid adjustment sets (if they exist)\ntestable conditional independencies"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#what-not-to-do",
    "href": "lectures/day2-scms/lec4.html#what-not-to-do",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "What not to do",
    "text": "What not to do\n\ndo univariable pre-screening against outcome (and / or treatment)\n\n\nthis should maybe never be done\nespecially not in the context of causal inference"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#adjustment-formula",
    "href": "lectures/day2-scms/lec4.html#adjustment-formula",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Adjustment formula",
    "text": "Adjustment formula\n\\[P(y|\\text{do}(x)) = \\sum_z P(y|x,z)P(z)\\]\n\nentails summing over all possible values of \\(Z\\)\nsay \\(Z\\) is 5 categorical variables with each 3 categories, this means \\(4^5=1024\\) estimates of:\n\n\\(P(y|x,z)\\) for each value of \\(x\\)\n\nwhat if \\(Z\\) is continuous?\nin practice, researchers rely on smoothness assumptions (e.g. regression) to estimate \\(P(Y|x,z)\\) with a parametric model\nthis assumption can be based on substantive causal knowledge, but often seems inspired rather pragmatism or necessity\nmisspecification of this estimator leads to biased results (even if you know all the confounders)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#target-queries",
    "href": "lectures/day2-scms/lec4.html#target-queries",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Target queries",
    "text": "Target queries\n\nup to now we’ve worked exclusively with \\(P(y|\\text{do}(t))\\): the probability of observing outcome \\(y\\) when setting treatment \\(T\\) to \\(t\\)\nthis is not typically what is of most interest, say there are two treatment options \\(T \\in \\{0,1\\}\\) (control and ‘treatment’)\n\naverage treatment effect \\[\\text{ATE} = E[y|\\text{do}(t=1)] - E[y|\\text{do}(t=0)]\\]\nconditional average treatment effect \\[\\text{CATE} = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]\\]\nprediction-under-intervention \\(P(y|\\text{do}(t),w)\\) (more on this on day 4)\n\nthese can be computed from \\(P(y|\\text{do}(t),w)\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#the-simplest-case-linear-regression",
    "href": "lectures/day2-scms/lec4.html#the-simplest-case-linear-regression",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "The simplest case: linear regression",
    "text": "The simplest case: linear regression\n\nassume the following structural causal model (\\(z\\) is confounder, \\(u\\) is exogenous noise): \\[f_y(t,z,u) = \\beta_t t + \\beta_z z + \\beta_u u\\]\nthen: \\[\\begin{align}\n  \\text{ATE} &= E[Y|\\text{do}(t=1)] - E[Y|\\text{do}(t=0)] \\\\\n             &\\class{fragment}{= E_{z,u}[\\beta_t * 1+ \\beta_z z + \\beta_u u] - E_{z,u}[\\beta_t * 0 + \\beta_z z + \\beta_u u]} \\\\\n             &\\class{fragment}{= \\beta_t + E_{z,u}[\\beta_z z + \\beta_u u] - E_{z,u}[\\beta_z z + \\beta_u u]} \\\\\n             &\\class{fragment}{= \\beta_t}\n\\end{align}\\]\ni.e. the ATE collapses to the the regression parameter \\(\\beta_t\\) in a linear regression model of \\(y\\) on \\(t,z\\)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#sec-metalearners",
    "href": "lectures/day2-scms/lec4.html#sec-metalearners",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "General estimators for the ATE and the CATE (meta-learners)",
    "text": "General estimators for the ATE and the CATE (meta-learners)\n\ndenote \\(\\tau(w) = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]\\)\n\n(assuming \\(W\\) is a sufficient set)\n\nT-learner: model \\(T=0\\) and \\(T=1\\) separately (e.g. regression separetely for treated and untreated): \\[\\begin{align}\n  \\mu_0(w) &= E[Y|\\text{do}(T=0),W=w] \\\\\n  \\mu_1(w) &= E[Y|\\text{do}(T=1),W=w] \\\\\n  \\tau(w)  &= \\mu_1(w) - \\mu_0(w)\n\\end{align}\\]\nS-learner: use \\(T\\) as just another feature \\[\\begin{align}\n  \\mu(t,w) &= E[Y|T=t,W=w] \\\\\n  \\tau(w)  &= \\mu(1,w) - \\mu(0,w)\n\\end{align}\\]\n(many other variants combinations: this is a whole literature)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#intuitive-way-pointers",
    "href": "lectures/day2-scms/lec4.html#intuitive-way-pointers",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Intuitive way-pointers:",
    "text": "Intuitive way-pointers:\n\nwhere does the complexity come from?\n\nvariance in outcome under control: \\(E[y|\\text{do}(T=0),w]\\)\nvariance CATE: \\(\\tau(w)\\) (in statistics: interaction between treatment and covariate)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#where-does-the-variance-come-from",
    "href": "lectures/day2-scms/lec4.html#where-does-the-variance-come-from",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Where does the variance come from?",
    "text": "Where does the variance come from?\n\n\n\n\n\n\nDAG\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Three datasets with the same DAG\n\n\n\n\n\n\n\n\n\\(Y = T + 0.5 (X - \\pi) + \\epsilon\\) (linear)\n\\(Y = T + \\sin(X) + \\epsilon\\) (non-linear additive)\n\\(Y = T * \\sin(X) - (1-T) \\sin(x) + \\epsilon\\) (non-linear + interaction)"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#making-dags",
    "href": "lectures/day2-scms/lec4.html#making-dags",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Making DAGs",
    "text": "Making DAGs\n\nhow do you get a DAG? up to now we assumed we had one\nbased on prior evidence, expert knowledge\n“no causes in, no causes out”"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#a003024-the-death-of-dags",
    "href": "lectures/day2-scms/lec4.html#a003024-the-death-of-dags",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "A003024: The death of DAGs?",
    "text": "A003024: The death of DAGs?\nThe number of possible DAGs grows super-exponentially in the number of nodes\n\n\n\nn_nodes\nn_dags\ntime at 1 sec / DAG\n\n\n\n\n1\n1\n\n\n\n2\n3\n\n\n\n3\n25\n\n\n\n4\n543\n\n\n\n5\n29281\n&gt; an hour\n\n\n6\n3781503\n&gt; a day\n\n\n7\n1138779265\n&gt; a year\n\n\n8\n783702329343\n\n\n\n9\n1213442454842881\n&gt; human species\n\n\n10\n4175098976430598143\n&gt; age of universe"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#do-we-need-to-consider-all-dags",
    "href": "lectures/day2-scms/lec4.html#do-we-need-to-consider-all-dags",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "Do we need to consider all DAGs?",
    "text": "Do we need to consider all DAGs?\n\na single sufficient set suffices\nadjusting for all direct causes of the treatment or all direct causes of the outcome are always sufficent sets\ncan we judge these without specifying all covariate-covariate relationships?\npotential approach:\n\nput all potential confounders in a cluster (e.g Anand et al. 2023)\nignore covariate-covariate relationships in that cluster\nwhat happens when (partial) missing data?"
  },
  {
    "objectID": "lectures/day2-scms/lec4.html#references",
    "href": "lectures/day2-scms/lec4.html#references",
    "title": "Adjustment Sets and Approaches - and limitations / critiques",
    "section": "References",
    "text": "References\n\n\n\n\nAnand, Tara V., Adele H. Ribeiro, Jin Tian, and Elias Bareinboim. 2023. “Causal Effect Identification in Cluster DAGs.” Proceedings of the AAAI Conference on Artificial Intelligence 37 (10): 12172–79. https://doi.org/10.1609/aaai.v37i10.26435."
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#causal-inference-frameworks",
    "href": "lectures/day2-scms/lec1-outline.html#causal-inference-frameworks",
    "title": "Motivating examples",
    "section": "Causal inference frameworks",
    "text": "Causal inference frameworks\n\nWhat are they for?"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#causal-inference-frameworks-1",
    "href": "lectures/day2-scms/lec1-outline.html#causal-inference-frameworks-1",
    "title": "Motivating examples",
    "section": "Causal inference frameworks",
    "text": "Causal inference frameworks\n\nWhy learn more than one?"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#lecture-1-2-topics",
    "href": "lectures/day2-scms/lec1-outline.html#lecture-1-2-topics",
    "title": "Motivating examples",
    "section": "Lecture 1 & 2 topics",
    "text": "Lecture 1 & 2 topics"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#example-task-are-hospital-deliveries-good-for-babies",
    "href": "lectures/day2-scms/lec1-outline.html#example-task-are-hospital-deliveries-good-for-babies",
    "title": "Motivating examples",
    "section": "Example task: are hospital deliveries good for babies?",
    "text": "Example task: are hospital deliveries good for babies?"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#observed-data",
    "href": "lectures/day2-scms/lec1-outline.html#observed-data",
    "title": "Motivating examples",
    "section": "Observed data",
    "text": "Observed data"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#new-question-hernia",
    "href": "lectures/day2-scms/lec1-outline.html#new-question-hernia",
    "title": "Motivating examples",
    "section": "New question: hernia",
    "text": "New question: hernia"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#observed-data-2",
    "href": "lectures/day2-scms/lec1-outline.html#observed-data-2",
    "title": "Motivating examples",
    "section": "Observed data 2",
    "text": "Observed data 2"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#how-to-unravel-this",
    "href": "lectures/day2-scms/lec1-outline.html#how-to-unravel-this",
    "title": "Motivating examples",
    "section": "How to unravel this?",
    "text": "How to unravel this?"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#causal-directed-acyclic-graphs",
    "href": "lectures/day2-scms/lec1-outline.html#causal-directed-acyclic-graphs",
    "title": "Motivating examples",
    "section": "Causal Directed Acyclic Graphs",
    "text": "Causal Directed Acyclic Graphs\n\ndiagram that represents our assumptions on causal relations"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#making-dags-for-our-examples",
    "href": "lectures/day2-scms/lec1-outline.html#making-dags-for-our-examples",
    "title": "Motivating examples",
    "section": "Making DAGs for our examples:",
    "text": "Making DAGs for our examples:\n\nThe pregnancy DAG"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#making-dags-for-our-examples-1",
    "href": "lectures/day2-scms/lec1-outline.html#making-dags-for-our-examples-1",
    "title": "Motivating examples",
    "section": "Making DAGs for our examples:",
    "text": "Making DAGs for our examples:\n\nThe hernia DAG"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#causal-dags-to-the-rescue",
    "href": "lectures/day2-scms/lec1-outline.html#causal-dags-to-the-rescue",
    "title": "Motivating examples",
    "section": "Causal DAGs to the rescue",
    "text": "Causal DAGs to the rescue"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#why-math",
    "href": "lectures/day2-scms/lec1-outline.html#why-math",
    "title": "Motivating examples",
    "section": "Why math???",
    "text": "Why math???"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#marginal-joint-and-conditional-probabilites",
    "href": "lectures/day2-scms/lec1-outline.html#marginal-joint-and-conditional-probabilites",
    "title": "Motivating examples",
    "section": "Marginal, Joint and Conditional probabilites",
    "text": "Marginal, Joint and Conditional probabilites"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#conditional-probabilities-require-dividing-by-the-denominator-of-the-conditioning-set",
    "href": "lectures/day2-scms/lec1-outline.html#conditional-probabilities-require-dividing-by-the-denominator-of-the-conditioning-set",
    "title": "Motivating examples",
    "section": "conditional probabilities require dividing by the denominator of the conditioning set",
    "text": "conditional probabilities require dividing by the denominator of the conditioning set"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#probability-rules-and-identities",
    "href": "lectures/day2-scms/lec1-outline.html#probability-rules-and-identities",
    "title": "Motivating examples",
    "section": "Probability rules and identities",
    "text": "Probability rules and identities"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#marginal-and-conditional-independence",
    "href": "lectures/day2-scms/lec1-outline.html#marginal-and-conditional-independence",
    "title": "Motivating examples",
    "section": "Marginal and conditional independence:",
    "text": "Marginal and conditional independence:"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#conditional-independence-in-an-example",
    "href": "lectures/day2-scms/lec1-outline.html#conditional-independence-in-an-example",
    "title": "Motivating examples",
    "section": "Conditional Independence in an example",
    "text": "Conditional Independence in an example"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#assumption-parlance-strong-vs-weak-assumption-necessary-sufficient",
    "href": "lectures/day2-scms/lec1-outline.html#assumption-parlance-strong-vs-weak-assumption-necessary-sufficient",
    "title": "Motivating examples",
    "section": "Assumption parlance (strong vs weak assumption, necessary, sufficient)",
    "text": "Assumption parlance (strong vs weak assumption, necessary, sufficient)"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#dags-convey-two-types-of-assumptions",
    "href": "lectures/day2-scms/lec1-outline.html#dags-convey-two-types-of-assumptions",
    "title": "Motivating examples",
    "section": "DAGs convey two types of assumptions:",
    "text": "DAGs convey two types of assumptions:\n\ncausal direction and conditional independence"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#dags-are-non-parametric",
    "href": "lectures/day2-scms/lec1-outline.html#dags-are-non-parametric",
    "title": "Motivating examples",
    "section": "DAGs are ‘non-parametric’",
    "text": "DAGs are ‘non-parametric’\n\nThey relay what variable ‘listens’ to what, but not in what way"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#dags-imply-a-causal-factorization-of-the-joint-distribution",
    "href": "lectures/day2-scms/lec1-outline.html#dags-imply-a-causal-factorization-of-the-joint-distribution",
    "title": "Motivating examples",
    "section": "DAGs imply a causal factorization of the joint distribution",
    "text": "DAGs imply a causal factorization of the joint distribution"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#sec-def-intervention",
    "href": "lectures/day2-scms/lec1-outline.html#sec-def-intervention",
    "title": "Motivating examples",
    "section": "The DAG definition of an intervention",
    "text": "The DAG definition of an intervention"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#intervention-as-graph-surgery---changed-distribution",
    "href": "lectures/day2-scms/lec1-outline.html#intervention-as-graph-surgery---changed-distribution",
    "title": "Motivating examples",
    "section": "Intervention as graph surgery - changed distribution",
    "text": "Intervention as graph surgery - changed distribution"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#back-to-example-1",
    "href": "lectures/day2-scms/lec1-outline.html#back-to-example-1",
    "title": "Motivating examples",
    "section": "Back to example 1",
    "text": "Back to example 1"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#back-to-example-2",
    "href": "lectures/day2-scms/lec1-outline.html#back-to-example-2",
    "title": "Motivating examples",
    "section": "Back to example 2",
    "text": "Back to example 2"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#the-gist-of-observational-causal-inference",
    "href": "lectures/day2-scms/lec1-outline.html#the-gist-of-observational-causal-inference",
    "title": "Motivating examples",
    "section": "The gist of observational causal inference",
    "text": "The gist of observational causal inference"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#basic-dag-patterns-chain",
    "href": "lectures/day2-scms/lec1-outline.html#basic-dag-patterns-chain",
    "title": "Motivating examples",
    "section": "Basic DAG patterns: chain",
    "text": "Basic DAG patterns: chain"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#basic-dag-patterns-fork",
    "href": "lectures/day2-scms/lec1-outline.html#basic-dag-patterns-fork",
    "title": "Motivating examples",
    "section": "Basic DAG patterns: fork",
    "text": "Basic DAG patterns: fork"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#basic-dag-patterns-collider",
    "href": "lectures/day2-scms/lec1-outline.html#basic-dag-patterns-collider",
    "title": "Motivating examples",
    "section": "Basic DAG patterns: collider",
    "text": "Basic DAG patterns: collider"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#collider-bias---tinder",
    "href": "lectures/day2-scms/lec1-outline.html#collider-bias---tinder",
    "title": "Motivating examples",
    "section": "Collider bias - Tinder",
    "text": "Collider bias - Tinder"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#conditioning-on-a-collider-creates-dependence-of-its-parents",
    "href": "lectures/day2-scms/lec1-outline.html#conditioning-on-a-collider-creates-dependence-of-its-parents",
    "title": "Motivating examples",
    "section": "Conditioning on a collider creates dependence of its parents",
    "text": "Conditioning on a collider creates dependence of its parents"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#when-life-gets-complicated-real-many-variable",
    "href": "lectures/day2-scms/lec1-outline.html#when-life-gets-complicated-real-many-variable",
    "title": "Motivating examples",
    "section": "When life gets complicated / real: many variable",
    "text": "When life gets complicated / real: many variable"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#d-separation-directional-separation",
    "href": "lectures/day2-scms/lec1-outline.html#d-separation-directional-separation",
    "title": "Motivating examples",
    "section": "d-separation (directional-separation)",
    "text": "d-separation (directional-separation)"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#d-separation-when-conditioning",
    "href": "lectures/day2-scms/lec1-outline.html#d-separation-when-conditioning",
    "title": "Motivating examples",
    "section": "d-separation when conditioning",
    "text": "d-separation when conditioning"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#the-back-door-criterion-and-adjustment",
    "href": "lectures/day2-scms/lec1-outline.html#the-back-door-criterion-and-adjustment",
    "title": "Motivating examples",
    "section": "The back-door criterion and adjustment",
    "text": "The back-door criterion and adjustment"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#did-we-see-this-equation-before",
    "href": "lectures/day2-scms/lec1-outline.html#did-we-see-this-equation-before",
    "title": "Motivating examples",
    "section": "Did we see this equation before?",
    "text": "Did we see this equation before?"
  },
  {
    "objectID": "lectures/day2-scms/lec1-outline.html#how-about-positivity",
    "href": "lectures/day2-scms/lec1-outline.html#how-about-positivity",
    "title": "Motivating examples",
    "section": "How about positivity",
    "text": "How about positivity"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#valid-adjustment-sets",
    "href": "lectures/day2-scms/lec4-outline.html#valid-adjustment-sets",
    "title": "How to do adjustment",
    "section": "Valid adjustment sets",
    "text": "Valid adjustment sets"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#valid-adjustment-sets-picking-one",
    "href": "lectures/day2-scms/lec4-outline.html#valid-adjustment-sets-picking-one",
    "title": "How to do adjustment",
    "section": "Valid adjustment sets: picking one",
    "text": "Valid adjustment sets: picking one"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#what-not-to-do",
    "href": "lectures/day2-scms/lec4-outline.html#what-not-to-do",
    "title": "How to do adjustment",
    "section": "What not to do",
    "text": "What not to do"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#adjustment-formula",
    "href": "lectures/day2-scms/lec4-outline.html#adjustment-formula",
    "title": "How to do adjustment",
    "section": "Adjustment formula",
    "text": "Adjustment formula"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#target-queries",
    "href": "lectures/day2-scms/lec4-outline.html#target-queries",
    "title": "How to do adjustment",
    "section": "Target queries",
    "text": "Target queries"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#the-simplest-case-linear-regression",
    "href": "lectures/day2-scms/lec4-outline.html#the-simplest-case-linear-regression",
    "title": "How to do adjustment",
    "section": "The simplest case: linear regression",
    "text": "The simplest case: linear regression"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#sec-metalearners",
    "href": "lectures/day2-scms/lec4-outline.html#sec-metalearners",
    "title": "How to do adjustment",
    "section": "General estimators for the ATE and the CATE (meta-learners)",
    "text": "General estimators for the ATE and the CATE (meta-learners)"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#intuitive-way-pointers",
    "href": "lectures/day2-scms/lec4-outline.html#intuitive-way-pointers",
    "title": "How to do adjustment",
    "section": "Intuitive way-pointers:",
    "text": "Intuitive way-pointers:"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#where-does-the-variance-come-from",
    "href": "lectures/day2-scms/lec4-outline.html#where-does-the-variance-come-from",
    "title": "How to do adjustment",
    "section": "Where does the variance come from?",
    "text": "Where does the variance come from?"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#making-dags",
    "href": "lectures/day2-scms/lec4-outline.html#making-dags",
    "title": "How to do adjustment",
    "section": "Making DAGs",
    "text": "Making DAGs"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#a003024-the-death-of-dags",
    "href": "lectures/day2-scms/lec4-outline.html#a003024-the-death-of-dags",
    "title": "How to do adjustment",
    "section": "A003024: The death of DAGs?",
    "text": "A003024: The death of DAGs?"
  },
  {
    "objectID": "lectures/day2-scms/lec4-outline.html#do-we-need-to-consider-all-dags",
    "href": "lectures/day2-scms/lec4-outline.html#do-we-need-to-consider-all-dags",
    "title": "How to do adjustment",
    "section": "Do we need to consider all DAGs?",
    "text": "Do we need to consider all DAGs?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#in-this-lecture-structural-causal-models-scms",
    "href": "lectures/day2-scms/lec3-scms-outline.html#in-this-lecture-structural-causal-models-scms",
    "title": "Structural Causal Models: definitions",
    "section": "In this lecture: structural causal models (SCMs)",
    "text": "In this lecture: structural causal models (SCMs)"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#think-of-the-world-as-a-computer-program-with-a-set-of",
    "href": "lectures/day2-scms/lec3-scms-outline.html#think-of-the-world-as-a-computer-program-with-a-set-of",
    "title": "Structural Causal Models: definitions",
    "section": "Think of the world as a computer program with a set of",
    "text": "Think of the world as a computer program with a set of"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#structural-causal-model-1",
    "href": "lectures/day2-scms/lec3-scms-outline.html#structural-causal-model-1",
    "title": "Structural Causal Models: definitions",
    "section": "Structural Causal Model 1",
    "text": "Structural Causal Model 1"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph",
    "href": "lectures/day2-scms/lec3-scms-outline.html#recursive-structural-causal-models-imply-a-directed-acyclic-graph",
    "title": "Structural Causal Models: definitions",
    "section": "Recursive Structural Causal Models imply a Directed Acyclic Graph",
    "text": "Recursive Structural Causal Models imply a Directed Acyclic Graph"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#submodel-and-effect-of-action",
    "href": "lectures/day2-scms/lec3-scms-outline.html#submodel-and-effect-of-action",
    "title": "Structural Causal Models: definitions",
    "section": "Submodel and Effect of Action",
    "text": "Submodel and Effect of Action"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#submodel-and-effect-of-action-as-a-mutilated-dag",
    "href": "lectures/day2-scms/lec3-scms-outline.html#submodel-and-effect-of-action-as-a-mutilated-dag",
    "title": "Structural Causal Models: definitions",
    "section": "Submodel and Effect of Action as a mutilated DAG",
    "text": "Submodel and Effect of Action as a mutilated DAG"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#specifying-a-distribution-for-exogenous-variables-u",
    "href": "lectures/day2-scms/lec3-scms-outline.html#specifying-a-distribution-for-exogenous-variables-u",
    "title": "Structural Causal Models: definitions",
    "section": "Specifying a distribution for exogenous variables U",
    "text": "Specifying a distribution for exogenous variables U"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#a-probabilistic-causal-model-is-a-scm-with-a-distribution-over-u",
    "href": "lectures/day2-scms/lec3-scms-outline.html#a-probabilistic-causal-model-is-a-scm-with-a-distribution-over-u",
    "title": "Structural Causal Models: definitions",
    "section": "A Probabilistic Causal Model is a SCM with a distribution over U",
    "text": "A Probabilistic Causal Model is a SCM with a distribution over U"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#calculating-a-treatment-effect-in-a-fully-specified-probabilistic-causal-model",
    "href": "lectures/day2-scms/lec3-scms-outline.html#calculating-a-treatment-effect-in-a-fully-specified-probabilistic-causal-model",
    "title": "Structural Causal Models: definitions",
    "section": "Calculating a treatment effect in a fully specified probabilistic causal model",
    "text": "Calculating a treatment effect in a fully specified probabilistic causal model"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#recap-of-definitions",
    "href": "lectures/day2-scms/lec3-scms-outline.html#recap-of-definitions",
    "title": "Structural Causal Models: definitions",
    "section": "Recap of definitions",
    "text": "Recap of definitions"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#in-the-real-world",
    "href": "lectures/day2-scms/lec3-scms-outline.html#in-the-real-world",
    "title": "Structural Causal Models: definitions",
    "section": "In the real world",
    "text": "In the real world"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#sec-identification",
    "href": "lectures/day2-scms/lec3-scms-outline.html#sec-identification",
    "title": "Structural Causal Models: definitions",
    "section": "Identification",
    "text": "Identification"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#idenfitication-in-pictures",
    "href": "lectures/day2-scms/lec3-scms-outline.html#idenfitication-in-pictures",
    "title": "Structural Causal Models: definitions",
    "section": "Idenfitication in pictures",
    "text": "Idenfitication in pictures"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#not-identified-vs-estimand",
    "href": "lectures/day2-scms/lec3-scms-outline.html#not-identified-vs-estimand",
    "title": "Structural Causal Models: definitions",
    "section": "Not identified vs estimand",
    "text": "Not identified vs estimand"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#seeing-is-not-doing",
    "href": "lectures/day2-scms/lec3-scms-outline.html#seeing-is-not-doing",
    "title": "Structural Causal Models: definitions",
    "section": "Seeing is not doing",
    "text": "Seeing is not doing"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#counterfactuals",
    "href": "lectures/day2-scms/lec3-scms-outline.html#counterfactuals",
    "title": "Structural Causal Models: definitions",
    "section": "Counterfactuals",
    "text": "Counterfactuals"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#take-it-one-level-higher-counterfactuals",
    "href": "lectures/day2-scms/lec3-scms-outline.html#take-it-one-level-higher-counterfactuals",
    "title": "Structural Causal Models: definitions",
    "section": "Take it one level higher: counterfactuals",
    "text": "Take it one level higher: counterfactuals"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#adam-versus-zoe",
    "href": "lectures/day2-scms/lec3-scms-outline.html#adam-versus-zoe",
    "title": "Structural Causal Models: definitions",
    "section": "Adam versus Zoe",
    "text": "Adam versus Zoe"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#computing-counterfactuals-with-scms",
    "href": "lectures/day2-scms/lec3-scms-outline.html#computing-counterfactuals-with-scms",
    "title": "Structural Causal Models: definitions",
    "section": "Computing counterfactuals with SCMs",
    "text": "Computing counterfactuals with SCMs"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#computing-counterfactuals",
    "href": "lectures/day2-scms/lec3-scms-outline.html#computing-counterfactuals",
    "title": "Structural Causal Models: definitions",
    "section": "Computing counterfactuals",
    "text": "Computing counterfactuals"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#pearls-causal-hierarchy-of-questions",
    "href": "lectures/day2-scms/lec3-scms-outline.html#pearls-causal-hierarchy-of-questions",
    "title": "Structural Causal Models: definitions",
    "section": "Pearl’s Causal Hierarchy (of questions)",
    "text": "Pearl’s Causal Hierarchy (of questions)"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#where-do-we-get-this-knowledge-from",
    "href": "lectures/day2-scms/lec3-scms-outline.html#where-do-we-get-this-knowledge-from",
    "title": "Structural Causal Models: definitions",
    "section": "Where do we get this knowledge from?",
    "text": "Where do we get this knowledge from?"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#not-covered-but-also-possible",
    "href": "lectures/day2-scms/lec3-scms-outline.html#not-covered-but-also-possible",
    "title": "Structural Causal Models: definitions",
    "section": "Not covered but also possible:",
    "text": "Not covered but also possible:"
  },
  {
    "objectID": "lectures/day2-scms/lec3-scms-outline.html#references-dag-recap",
    "href": "lectures/day2-scms/lec3-scms-outline.html#references-dag-recap",
    "title": "Structural Causal Models: definitions",
    "section": "References DAG-recap",
    "text": "References DAG-recap"
  }
]