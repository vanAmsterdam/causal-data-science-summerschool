---
title: Causal Directed Acylic Graphs
subtitle: introduction
author: Wouter van Amsterdam
date: 2024-08-06
format: 
    #beamer:
        #aspectratio: 169
        #logo: umcu_blue.png
    #html:
        #toc: true
        #toc-depth: 2
        #number-sections: true
    revealjs:
        toc: false
        #theme: [default, umcu.scss]
        incremental: true
        width: 1920
        height: 1080
        logo: umcu_blue.png
        center: true
        include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
execute:
    warning: false
    message: false
categories:
    - DAG
    - day2
bibliography: bibliography.bib
---

```{r}
#| label: setup
#| echo: false
#| execute: true

suppressMessages({
  library(purrr)
  library(data.table)
  library(ggplot2); theme_set(theme_bw())
  library(knitr)
})
```

<!--

Qs:
1. what knowledge of probability to expect? e.g. conditioning

--->

# Day 2 intro: Causal Directed Acyclic Graphs and Structural Causal Models

## Today's lectures

- introduce new framework based on
  - causal Directed Acyclic Graphs (DAGs)
  - Structral Causal Models (SCMs)
- counterfactuals and Pearl's Causal Hierarchy of questions
- lectures will follow Pearl's book Causality @pearlCausality2009, specifically chapters 3 (DAGs) and 7 (SCMs)

## Causal inference frameworks

### What are they for?

#### Mathematical language to
  - define *causal* quantities
  - express *assumptions*
  - derive how to *estimate* causal effects

## Causal inference frameworks

### Why learn more than one?

<!--todo: add hyperlink to day 1 materials-->

- On day 1 we learned about the Potential Outcomes framework
    - Defines causal effects in terms of (averages of) *individual potential outcomes*
    - Estimation requires assumptions of (conditional) exchangeability and positivity / overlap and consistency
- There isn't only 1 way to think about causality, find one that '*clicks*'
- Now we will learn another framework: *Structural Causal Models* and *causal graphs*
    - causal relations and manipulations of *variables*
    - Developed by different people initially - Judea Pearl, Peter Spirtes, Clark Glymour
    - SCM approach is broader in that it can define more different types of causal questions
- Equivalence: given the same data and assumptions, get the same estimates

## Lecture 1 & 2 topics {background-image="1920_1080.png" background-size="contain"}

- motivating examples for DAGs
- what are DAGs
- causal inference with DAGs
  - what is an intervention
  - structures: confounding, mediation, colliders
  - d-separation
  - back-door criterion
 
## practical 1

  - drawing and using dags (what to condition on); daggity
  - same data, different dags, different answers

# Motivating examples

## Example task: are hospital deliveries good for babies? {#sec-example-delivery}

::: {.r-stack}

![](figs/delivery1.png)

![](figs/delivery2.png){.fragment}

![](figs/delivery.png){.fragment}

:::

## Example task: are hospital deliveries good for babies?

- You're a data scientist in a children's hospital
- Have data on
  - delivery location (home or hospital)
  - neonatal outcomes (good or bad)
  - pregnancy risk (high or low)
- Question: do hospital deliveries result in better outcomes for babies?

. . .

```{r}
#| label: simdeliveries

# t=0: home, t=1: hospital
# z=0: low risk, z=1: high risk
# y=1: good outcome
dnames = list(location=c('home', 'hospital'), risk=c('low', 'high'))
pos_tz <- matrix(c(
  c(0.9,  0.5), # y|t=0,z=0,1
  c(0.95, 0.8)  # y|t=1,z=0,1
), nrow=2, byrow=T,
dimnames=dnames)

ps_tz <- matrix(c(
  c(0.72, 0.08), # p(t=0,z=0,1)
  c(0.02, 0.18)  # p(t=1,z=0,1)
), nrow=2, byrow=T, dimnames=dnames)

# p(t,z) under do(t)
ps_do0 <- matrix(c( 
  c(0.8, 0.2), 
  c(0.0, 0.0)  
), nrow=2, byrow=T)

ps_do1 <- matrix(c( 
  c(0.0, 0.0), 
  c(0.8, 0.2) 
), nrow=2, byrow=T)


eys  <- rowSums(pos_tz * ps_tz) / rowSums(ps_tz) # E y|t
dots <- c(sum(pos_tz * ps_do0), sum(pos_tz*ps_do1))

n = 1000

ts  <- vector(mode="logical", length=0)
zs  <- vector(mode="logical", length=0)
py0s <- vector(mode="numeric", length=0)
py1s <- vector(mode="numeric", length=0)
ys  <- vector(mode="logical", length=0)

ntots <- n * ps_tz

for (t in c(0,1)) {
  for (z in c(0,1)) {
    ntz <- n *ps_tz[t+1,z+1]
    ts  <- c(ts, rep(t, ntz))
    zs  <- c(zs, rep(z, ntz))
    py <- pos_tz[t+1,z+1]
    y <- c(rep(0,
               round(ntz * (1-py))), # <- round should not be needed here but found a bug
           rep(1,
               round(ntz*py))) 
    ys <- c(ys, y)
    py0s <- c(py0s, rep(pos_tz[1, z+1], ntz))
    py1s <- c(py1s, rep(pos_tz[2, z+1], ntz))
  }
}
# ys <- ifelse(ts, y1s, y0s)
df <- data.table(
  location=ts,
  risk=zs,
  outcome=ys,
  py0=py0s,
  py1=py1s)

# head(df)

# kable(eys, col.names="tips")
#pander::pander(ftable(eys))

# pander::pander(ftable(pos_tz), emphasize.strong.rows=c(1), emphasize.strong.cols=c(1))

ntots <- n * ps_tz
nys <- ntots * pos_tz

strs <- paste0(nys, " / ", ntots, " = ", 100*pos_tz, "%")
str_mat <- matrix(strs, ncol=2, dimnames=dnames)
# kable(t(str_mat))
# pander::pander(ftable(t(str_mat)), emphasize.strong.rows=c(1), emphasize.strong.cols=c(1))

ntotsm <- rowSums(ntots)
nysm <- rowSums(nys)
strsm <- paste0(nysm, " / ", ntotsm, " = ", 100*eys, "%")

tab_tots <- rbind(ntots, ntotsm)


# tab <- df[, list(good=sum(outcome==1), bad=sum(outcome==0), frac_good=mean(outcome)), by=c("risk", "location")]


```

. . . 


```{r}
#| label: deliveryplot
#| output: fragment
#| include: false

df[, ni:=1.]

ggplot(df, aes(x=1, fill=factor(outcome))) +
  geom_bar(stat="count", position="stack") + 
  # geom_bar(aes(y=..count../sum(..count..))) + 
  facet_grid(risk~location)
```


## Observed data

|      |      | location |          |
|------|------|---------:|---------:|
|      |      | home     | hospital |
| risk | low  | `r str_mat[1,1]` | `r str_mat[2,1]` |
|      | high | `r str_mat[1,2]` | `r str_mat[2,2]` |

- better outcomes for babies delivered in the hospital for *both risk groups*

## Observed data

|      |      | location |          |
|------|------|---------:|---------:|
|      |      | home     | hospital |
| risk | low  | `r str_mat[1,1]` | `r str_mat[2,1]` |
|      | high | `r str_mat[1,2]` | `r str_mat[2,2]` |
|      |      |                  |                  |
|      | *marginal* | `r strsm[1]` | `r strsm[2]` |

- better outcomes for babies delivered in the hospital for *both risk groups*
- but not better *marginal* ('overall')
- how is this possible? (a.k.a. *simpsons paradox*)
- what is the correct way to estimate the effect of delivery location?

## New question: hernia

- for a patient with a hernia, will they be able to walk sooner when recovering at home or when recovering in a hospital?

::: {.r-stack}

![](figs/delivery-locations.png){.fragment}

![](figs/delivery-backpain.png){.fragment}

:::

## Observed data 2

|      |      | location |          |
|------|------|---------:|---------:|
|      |      | home     | hospital |
| bedrest | no | `r str_mat[1,1]` | `r str_mat[2,1]` |
|      | yes | `r str_mat[1,2]` | `r str_mat[2,2]` |
|      |      |                  |                  |
|      | *marginal* | `r strsm[1]` | `r strsm[2]` |

- more bed rest in hospital
- what is the correct way to estimate the effect of location?

## How to unravel this?

- we got two questions with exactly the same data
- in one example, 'stratified analysis' seemed best
- in the other example, 'marginal analysis' seemed best
- with *Directed Acyclic Graphs* we can make our decision

## Causal Directed Acyclic Graphs

### diagram that represents our assumptions on causal relations

1. nodes are variables
2. arrows (directed edges) point from cause to effect

![Directed Acyclic Graph](_tikzs/dag-fire0.png){#fig-dag-fire0 .fragment}

- when used to convey causal assumptions, DAGs are 'causal' DAGs
- this is not the only use of DAGs (see [day 4](../day4-causal-predictions/lec1.html))

## Making DAGs for our examples:

### The pregnancy DAG

![](_tikzs/dag-delivery.png){#fig-dag-delivery1 height="40%"}

- assumptions:
    - women with high risk of bad neonatal outcomes (`pregnancy risk`) are referred to the hospital for delivery
    - hospital deliveries lead to better outcomes for babies as more emergency treatments possible
    - both `pregnancy risk` and `hospital delivery` cause `neonatal outcome`
- the *other variable* `pregnancy risk` is a common cause of the treatment (`hospital delivery`) and the outcome (this is what's called a confounder)

## Making DAGs for our examples:

### The hernia DAG

![](_tikzs/dag-hernia.png){#fig-dag-hernia height="40%"}

- assumptions:
    - patients admitted to the hospital keep more `bed rest` than those who remain at home
    - `bed rest` leads to lower recovery times thus less walking patients after 1 week
- the *other variable* `bed rest` is a *mediator* between the treatment (`hospitalized`) and the outcome

## Causal DAGs to the rescue

- the *other variable* was:
    - a **common cause** of the treatment and outcome in the pregnancy example
    - a **mediator** between the treatment and the outcome in the hernia example, 
- using our background knowledge we could see *something* is different about these examples
- next: ground this in causal theory and see implications for analysis

# Some math background: probabilites and assumptions

## Why math???

:::{layout="[60,40]"}

- why not?
- need probability for estimation
- need conditional independence for causal inference
- need to understand 'strength' of assumptions


![oh no math](figs/why math.png)

:::


## Marginal, Joint and Conditional probabilites

Probability statements about *random events* $A$ and $B$

$A=1\to$ patient dies; $B=1\to$ has cancer

::: {layout="[60,40]" layout-valign="center"}

| statement | interpretation |
|----------|-----------------------------------|
| $P(A)$   | *marginal* probability that event $A$ occurs |
| $P(B)$   | *marginal* probability that event $B$ occurs |

::::{.r-stack}

:::::{.fragment .fade-in-then-out}
|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    | 5    | 5     | 10  |
|   | has no cancer | 10   | 80    | 90  |
|   |               | 15   | 85    | 100 |
:::::

:::::{.fragment .fade-in-then-out}
|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    | | | |
|   | has no cancer | | | |
|   |               | 15   | 85    | 100 |
$P(A=1) = 15 / 100$
:::::

:::::{.fragment .fade-in-then-out}
|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    |     |      | 10  |
|   | has no cancer |    |     | 90  |
|   |               |    |     | 100 |
$P(B=1) = 10 / 100$
:::::

::::

:::

## Marginal, Joint and Conditional probabilites

Probability statements about *random events* $A$ and $B$:

::: {layout="[60,40]" layout-valign="center"}

| statement | interpretation |
|----------|-----------------------------------|
| $P(A)$   | *marginal* probability that event $A$ occurs |
| $P(A,B)$ | *joint* probability of $A$ and $B$  |

::::{.r-stack}

:::::{.fragment .fade-in-then-out}
|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    | 5    | 5     | 10  |
|   | has no cancer | 10   | 80    | 90  |
|   |               | 15   | 85    | 100 |
:::::

:::::{.fragment .fade-in-then-out}
|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    | 5    | | |
|   | has no cancer | | | |
|   |               | | | 100 |

$P(B=1,A=1) = 5 / 100$
:::::


::::

:::

## Marginal, Joint and Conditional probabilites

Probability statements about *random events* $A$ and $B$:

::: {layout="[60,40]" layout-valign="center"}

| statement | interpretation |
|----------|-----------------------------------|
| $P(A)$   | *marginal* probability that event $A$ occurs |
| $P(A,B)$ | *joint* probability of $A$ and $B$  |
| $P(A|B)$ | *conditional* probability of $A$ given $B$  |

::::{.r-stack}

:::::{.fragment .fade-in-then-out}
|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    | 5    | 5     | 10  |
|   | has no cancer | 10   | 80    | 90  |
|   |               | 15   | 85    | 100 |

[- *marginal* $P(A=1) = 15/100$]{.non-incremental}

:::::

:::::{.fragment .fade-in-then-out}
|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    | 5    | 5     | 10  |
|   | has no cancer |    |     |   |
|   |               |    |     |  |

[- *marginal* $P(A=1) = 15/100$]{.non-incremental}

[- *conditional* $P(A=1|B=1) = 5 / 10$]{.non-incremental}

:::::

::::

:::

## Probability rules and identities

:::{layout="[60,40]"}

| statement | interpretation |
|----------|-----------------------------------|
| $P(A) = \sum_{b} P(A,B=b)$ | marginal is sum over joint |

:::::{.fragment}

|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    | 5    |     |   |
|   | has no cancer | 10   |     |   |
|   |               | 15   |     | 100 |

\begin{align}
    P(A=1) &= P(A=1,B=0) + P(A=1,B=1) \\
           &= 5/100 + 10/100 \\
           & = 15/100
\end{align}


:::::

:::

## Probability rules and identities

:::{layout="[60,40]"}

| statement | interpretation |
|----------|-----------------------------------|
| $P(A) = \sum_{b} P(A,B=b)$ | marginal is sum over joint |
| $P(A,B) = P(A|B)P(B)$ | product rule | 

:::::{.fragment}
|   |               | A    |       |     |
|---|---------------|------|-------|-----|
|   |               | dies | lives |     |
| B | has cancer    | 5    |     | 10 |
|   | has no cancer |    |     |   |
|   |               |    |     | 100 |

\begin{align}
    P(A=1,B=1) &= P(A=1|B=1)P(B=1) \\
               &= 5/10 * 10/100 \\
               & = 5/100
\end{align}


:::::

:::

## Probability rules and identities

| statement | interpretation |
|----------|-----------------------------------|
| $P(A) = \sum_{b} P(A,B=b)$ | marginal is sum over joint |
| $P(A,B) = P(A|B)P(B)$ | product rule | 
| $P(A|B) = \frac{P(A,B)}{P(B)}$ | conditional is joint over marginal (follows from product rule)| 
| $P(A|C) = \sum_{b} P(A|B=b,C)P(B=b|C)$ | total expectation (consequence of marginal vs joint and product rule) |

## Marginal and conditional independence:

| statement | interpretation |
|----------|-----------------------------------|
| $P(A,B) = P(A)P(B)$ | (marginal) independence of $A$ and $B$| 

- knowing $A$ has no information on what to expect of $B$
- If I roll a die, the result of that die ($A$) has no information on the weather in the Netherlands ($B$)

## Marginal and conditional independence:

| statement | interpretation |
|----------|-----------------------------------|
| $P(A,B) = P(A)P(B)$ | (marginal) independence of $A$ and $B$| 
| $P(A,B|C) = P(A|C)P(B|C)$ | conditional independence of $A$ and $B$ given $C$| 

- $C$ has all the information that is shared between $A$ and $B$

## Conditional Independence in an example

::: {layout="[70,30]" layout-valign="center"}

- Charlie calls Alice and reads her script $C$, then she calls Bob and reads him the same
- A week later we ask Alice to repeat the story Charlie told her, she remembered $A$, a noisy version of $C$
- We ask Bob the same, he recounts $B$, a different noisy version of $C$
- Are $A$ and $B$ independent? No! $P(A,B) \neq P(A)P(B)$
  - If we learn $A$ from Alice, we can get a good guess about $B$ from Bob
- If we knew the $C$, would hearing $A$ give use more information about $B$?
  - No, because all the shared information between $A$ and $B$ is explained by $C$, so:
  - $P(A,B|C) = P(A|C)P(B|C)$
- Variables can be marginally dependent but conditionally independent

![ABC](figs/ABC.png){width="100%"}

:::

## Assumption parlance {#sec-assumptions}

- necessary assumption:
  - A **must** hold for B to be true
- sufficient assumption:
  - B is always true when A holds
- strong assumption:
  - requires *strong* evidence, we'd rather not make these
- weak assumption:
  - requires *weak* evidence
- strong vs weak assumption are judged on relative terms
  - if assumption A is sufficient for B, B cannot be a stronger assumption that A

# DAG definitions

## DAGs convey two types of assumptions:

### causal direction and conditional independence

1. causal direction: what causes what?


::: {layout-ncol=2}

![DAG 1](_tikzs/dag-floor1.png){#fig-dag-floor1}

![DAG 2](_tikzs/dag-floor2.png){.fragment}

:::

- read @fig-dag-floor1 as
  - `sprinkler on` **may** (or may not) cause `wet floor`
  - `wet floor` **cannot** cause `sprinkler on`

## DAGs convey two types of assumptions:

### causal direction and conditional independence

1. conditional indepence (e.g. exclusion of influence / information)

::: {layout-ncol=3}

![DAG 1](_tikzs/dag-fire1){#fig-dag-fire1 height=400}

![DAG 2](_tikzs/dag-fire2){#fig-dag-fire2 height=400}

![DAG 3](_tikzs/dag-fire3){#fig-dag-fire3 height=400}

:::

- @fig-dag-fire1 says `fire` can **only** cause `wet floor` through `sprinkler on`
- @fig-dag-fire2 says *there may be other ways through which `fire` causes `wet floor`*
  - @fig-dag-fire2 is thus a *weaker* assumption than @fig-dag-fire1
- @fig-dag-fire3 is also compatible with @fig-dag-fire2


## DAGs are 'non-parametric'

### They relay what variable 'listens' to what, but not in what way

::: {layout="[30,70]" layout-valign="center"}

![DAG](_tikzs/dag-nonparametric.png)

```{r}
#| label: fig-dag-nonparametric
#| output: fragment
#| fig-cap: Three datasets with the same DAG

set.seed(12345)
n = 1e3

f1 <- function(t, x, u) t + 0.5 * (x - pi) + u
f2 <- function(t, x, u) t + sin(x) + u
f3 <- function(t, x, u) t * sin(x) - (1-t) * sin(x) + u

df <- data.table(
  x = runif(n, 0, 2*pi),
  t = rbinom(n, 1, 0.5),
  u = rnorm(n, 0, .1)
)

df[, `:=`(
  y1 = f1(t, x, u),
  y2 = f2(t, x, u),
  y3 = f3(t, x, u)
)]

dfm <- melt(df, measure.vars=c('y1', 'y2', 'y3'),
            variable.name="f", value.name="y")

ggplot(dfm, aes(x=x, y=y, col=factor(t))) + 
  geom_point() + 
  facet_grid(~f)
```

:::

1. $Y = T + 0.5 (X - \pi) + \epsilon$ (linear)
2. $Y = T + \sin(X) + \epsilon$ (non-linear additive)
3. $Y = T * \sin(X) - (1-T) \sin(x) + \epsilon$ (non-linear + interaction)

## DAGs are 'non-parametric'

### They relay what variable 'listens' to what, but not in what way

::: {layout="[30,70]" layout-valign="center"}

![DAG](_tikzs/dag-nonparametric.png)

- this DAG says $Y$ is a function of $X,T$ and external noise $U_Y$, or:
- $Y = f_Y(X,T,U_Y)$
- in the [next lecture](./lec3-scms.html) we'll talk more about these 'structural equations'

:::

## DAGs imply a causal factorization of the joint distribution

::: {layout="[30,70]"}

![observational data](_tikzs/dag-obs.png){#fig-obs width=50%}

\begin{align}
    P(Y,T,Z,W) &= P(Y|T,Z,W)P(T,Z,W) \\
               &\class{fragment}{= P(Y|T,Z)P(T,Z,W)} \\
               &\class{fragment}{= P(Y|T,Z)P(T|Z,W)P(Z,W)} \\
               &\class{fragment}{= P(Y|T,Z)P(T|Z,W)P(Z)P(W)}
\end{align}

:::

If this looks complicated: just follow the arrows

## The DAG definition of an intervention {#sec-def-intervention}

assume this is our DAG for a situation and we want to learn the effect $T$ has on $Y$

- this is denoted $P(Y|\text{do}(T))$
- in the graph, [intervening]{.fg} on variable $T$ means removing all incoming arrows
- this assumes such a *modular* intervention is possible: i.e. leave everything else unaltered

::: {layout-ncol=2}

![observational data](_tikzs/dag-obs.png){#fig-obs width=50%}

![intervened DAG](_tikzs/dag-intervened.png){#fig-intervened .fragment width=50%}

:::

- which means $T$ does not *listen* to other variables anymore, but is set at a particular value, like in an experiment

## Intervention as graph surgery - changed distribution

::: {layout-ncol=2}

![observational data](_tikzs/dag1-obs.png){#fig-obs width=50%}

![intervened DAG](_tikzs/dag1-intervened.png){#fig-intervened width=50%}

:::

::: {layout-ncol=2}

::::{.fragment}
\begin{align}
      P_{\text{obs}}(Y,T,Z) &= P(Y|T,Z)\color{red}{P(T|Z)}P(Z) \\
        P_{\text{obs}}(Y|T) &= \sum_{z} P(Y|T,Z=z)P(Z=z|T)
\end{align}

::::

::::{.fragment}
\begin{align}
      P_{\text{int}}(Y,T,Z) &= P(Y|T,Z)\color{green}{P(T)}P(Z) \\
        P_{\text{int}}(Y|T) &= \sum_{z} P(Y|T,Z=z)P(Z=z|T) \\
               &\class{fragment}{= \sum_{z} P(Y|T,Z=z)\color{green}{P(Z)}} \\
               &\class{fragment}{= P(Y|\text{do}(T))}
\end{align}
::::

:::

## Intervention as graph surgery - changed distribution

::: {layout-ncol=2}

:::: {#int}
![observational data](_tikzs/dag1-obs.png){#fig-obs width=50%}

$$P_{\text{obs}}(Y|T) = \sum_{z} P(Y|T,Z=z)\color{red}{P(Z=z|T)}$$
::::

:::: {#int}
![intervened DAG](_tikzs/dag1-intervened.png){#fig-intervened width=50%}

$$P_{\text{int}}(Y|T) = \sum_{z} P(Y|T,Z=z)\color{green}{P(Z=z)}$$ {#eq-estimand}
::::

:::

- in $P_{\text{obs}}$, $P(Z|T) \color{red}{\neq} P(Z)$
- in $P_{\text{int}}$, $P(Z|T) \color{green}{\neq} P(Z)$
- thereby $P_{\text{obs}}(Y|T) \neq P_{\text{int}}(P(Y|T)) = P(Y|\text{do}(T))$
- **seeing is not doing**
- **looking at @eq-estimand, we can compute these from $P_{\text{obs}}$!** (this is what is called an *estimand*)

## Back to example 1

::: {layout-ncol=2}

![DAG](_tikzs/dag-delivery.png)


|      |      | location |          |
|------|------|---------:|---------:|
|      |      | home     | hospital |
| risk | low  | `r str_mat[1,1]` | `r str_mat[2,1]` |
|      | high | `r str_mat[1,2]` | `r str_mat[2,2]` |
|      |      |                  |                  |
|      | *marginal* | `r strsm[1]` | `r strsm[2]` |

:::

- estimand: $P(\text{outcome}|\text{do}(\text{location})) = \sum_{\text{risk}} P(\text{outcome}|\text{location},\text{risk})P(\text{risk})$
- $P(\text{risk}=\text{low})=74\%$

::: {.fragment}
\begin{align}
 P(\text{outcome}|\text{do}(\text{hospital})) &= 95 * 0.74 + 80 * 0.26 = 91.1\% \\
     P(\text{outcome}|\text{do}(\text{home})) &= 90 * 0.74 + 50 * 0.26 = 79.6\%
\end{align}
:::

- **conclusion**: sending all deliveries to the hospital leads to better outcomes

## Back to example 2

::: {layout-ncol=2}

![DAG](_tikzs/dag-hernia)

- removing all arrows going in to $T$ results in the same DAG
- so $P(Y|T) = P(Y|\text{do}(T))$
- i.e. use the marginals

:::



## The gist of observational causal inference

is to take data we have to make inferences about data from a different distribution (i.e. the intervened-on distribution)

:::{.columns}

::::{.column width="20%"}

![observational data: data we have](_tikzs/dag-obs.png){#fig-obs}

![intervened DAG: what we want to know](_tikzs/dag-intervened.png){#fig-intervened}

::::

::::{.column width="80%"}

- causal inference frameworks provide a language to express assumptions
- based on these assumptions, the framework tell us whether such an inference is possible
    - this is often referred to as *is the effect identified*
- and provide formulas for how to do so based on the observed data distribution (*estimands*)
- (one could say this is essentially assumption-based extrapolation, some researchers think this entire enterprise is anti-scientific)
- not yet said: *how* to do statistical inference to estimate the estimand (much can still go wrong here)
  - can also be part of identification, see [the following lecture on SCMs](./lec3-scms.html)

::::

:::

# DAG rules

## Basic DAG patterns: chain

:::{layout="[30,70]"}

![chain / mediation](_tikzs/dag-chain.png){#fig-chain width="100%"}

- $M$ *mediates* effect of $X$ on $Y$
- $X \perp Y | M$
- do not want to adjust for $M$ when estimating total effect of $X$ on $Y$

:::

## Basic DAG patterns: fork

:::{layout="[30,70]"}

![fork / confounder](_tikzs/dag-fork.png){#fig-fork width="100%"}

- $Z$ *causes* both $X$ and $Y$ (common cause / confounder)
- $X \perp Y | Z$
- $Z \to X$ is a *back-door*: a path between $X$ and $Y$ that starts with an arrow into $X$
- typically want to adjust for $Z$ (see [later @sec-backdoor])

:::

## Basic DAG patterns: collider

:::{layout="[30,70]"}

![collider](_tikzs/dag-collider){#fig-collider width="100%"}

- $X$ and $Y$ *both cause* $Z$
- $X \perp Y$ (but *NOT* when conditioning on $Z$)
- often do not want to condition on $Z$ as this induces a correlation between $X$ and $Y$

:::

## Collider bias - Tinder

:::{layout="[30,70]"}

::: {#fig-collider}

![collider](_tikzs/dag-collider){#fig-collider width="100%"}

\begin{align}
    \text{intelligent} &\sim U[0,1] \\
    \text{attractive}  &\sim U[0,1] \\
    \text{on tinder}   &= I_{\text{intelligent} + \text{attractive} < 1}
\end{align}


:::

```{r}
#| label: fig-tinder
#| output: fragment
#| fig-width: 12

intelligent = runif(n)
attractive = runif(n)
s = intelligent + attractive > 1.
df <- data.frame(intelligent, attractive, s)
df$s <- factor(df$s)

ggplot(df, aes(x=intelligent, y=attractive)) +
  geom_point(aes(alpha=s, col=s)) + 
  # stat_smooth(method='lm', col='black', se=F, formula=y~x) +
  # stat_smooth(method='lm', se=F, formula=y~x, col='black') +
  stat_smooth(data=df[df$s==T,], method='lm', col='darkred', formula=y~x, se=F) + 
  scale_color_manual(
    breaks=c(F,T), values=c('gray', 'darkgreen'),
    labels=c('on tinder', 'not on tinder'),
    guide=guide_legend(
      title=''
      )
    ) + 
  scale_alpha_manual(breaks=c(F,T), values=c(0.15, 0.2), guide=NULL) + 
  theme(axis.ticks=element_blank(), axis.text=element_blank()) + 
  coord_equal()
```

:::

## Conditioning on a collider creates dependence of its parents 

- may not be too visible: doing an analysis in a selected subgroup is a form of ('invisible') conditioning)
- e.g. when selecting only patients in the hospital
    - being admitted to the hospital is a collider (has many different causes, e.g. traffic accident or fever)
    - usually only one of these is the reason for hospital admission
    - the causes for hospital admission now seem anti-correlated
- collider conditioning *might* be an explanation for the *obsesity paradox* (i.e. obesity is correlated with better outcomes in diverse medical settings) [e.g. @banackObesityParadoxMay2017]

## When life gets complicated / real

![Figure 4 in @debertinSynthesizingSubjectmatterExpertise2024](figs/dag-murray.jpg)

## d-separation (directional-separation)

![paths](_tikzs/path1.png)

- a *path* is a set of nodes connected by edges ($x \ldots y$)
- a *directed-path* is a path with a constant direction ($x \dots t$)
- an *unblocked-path* is a path without a collider ($t \ldots y$)
- a *blocked-path* is a path with a collider ($s,t, u$)
- *d(irectional)-separation* of $x,y$ means there is no unblocked path between them

## d-separation when conditioning

![paths with conditioning variables $r$, $t$](_tikzs/path1z.png)

- conditioning on variable: 
  - when variable is a collider: *opens a path* ($t$ opens $s,t,u$ etc.)
  - otherwise: *blocks a path* (e.g. $r$ blocks $x,r,s$)
- conditioning *set* $Z=\{r,t\}$: set of conditioning variables

## The back-door criterion and adjustment {#sec-backdoor}

**Definition 3.3.1 (Back-Door) (for pairs of variables)**

A set of variables $Z$ satisfies the *back-door* criterion relative to an ordered pair of variables $(X,Y)$ in a DAG if:

1. no node in $Z$ is a descendant of $X$ *(e.g. mediatiors)*
2. $Z$ blocks every path between $X$ and $Y$ that contains an arrow into $X$

:::{.fragment}

**Theorem 3.2.2 (Back-Door Adjustment)**

If a set of variables $Z$ satisfies the back-door criterion relative to $(X,Y)$, then the causal effect of $X$ on $Y$ is identifiable and is given by the formula

$$P(y|\text{do}(x)) = \sum_z P(y|x,z)P(z)$$ {#eq-backdooradjustment}

:::

## Did we see this equation before?

- Yes! When computing the effect of hospital deliveries on neonatal outcomes @eq-estimand
- DAGs tell us what to adjust for
- automatic algorithms tell use whether an estimand exists and what it is
- nice paper on this: Crash course in good and bad controls [@cinelliCrashCourseGood2022]


## References

