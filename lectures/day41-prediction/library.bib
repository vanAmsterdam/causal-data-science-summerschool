@article{alaaMachineLearningGuide2021,
  title = {Machine Learning to Guide the Use of Adjuvant Therapies for Breast Cancer},
  author = {Alaa, Ahmed M. and Gurdasani, Deepti and Harris, Adrian L. and Rashbass, Jem and family=Schaar, given=Mihaela, prefix=van der, useprefix=true},
  date = {2021-06-24},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  pages = {1--11},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10/gk6bh7},
  abstract = {Accurate prediction of the individualized survival benefit of adjuvant therapy is key to making informed therapeutic decisions for patients with early invasive breast cancer. Machine learning technologies can enable accurate prognostication of patient outcomes under different treatment options by modelling complex interactions between risk factors in a data-driven fashion. Here, we use an automated and interpretable machine learning algorithm to develop a breast cancer prognostication and treatment benefit prediction model—Adjutorium—using data from large-scale cohorts of nearly one million women captured in the national cancer registries of the United Kingdom and the United States. We trained and internally validated the Adjutorium model on 395,862 patients from the UK National Cancer Registration and Analysis Service (NCRAS), and then externally validated the model among 571,635 patients from the US Surveillance, Epidemiology, and End Results (SEER) programme. Adjutorium exhibited significantly improved accuracy compared to the major prognostic tool in current clinical use (PREDICT v2.1) in both internal and external validation. Importantly, our model substantially improved accuracy in specific subgroups known to be under-served by existing models. Adjutorium is currently implemented as a web-based decision support tool (https://vanderschaar-lab.com/adjutorium/) to aid decisions on adjuvant therapy in women with early breast cancer, and can be publicly accessed by patients and clinicians worldwide.},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Breast cancer;Prognosis\\
Subject\_term\_id: breast-cancer;prognosis}
}

@article{candidodosreisUpdatedPREDICTBreast2017,
  title = {An Updated {{PREDICT}} Breast Cancer Prognostication and Treatment Benefit Prediction Model with Independent Validation},
  author = {Candido dos Reis, Francisco J. and Wishart, Gordon C. and Dicks, Ed M. and Greenberg, David and Rashbass, Jem and Schmidt, Marjanka K. and family=Broek, given=Alexandra J., prefix=van den, useprefix=true and Ellis, Ian O. and Green, Andrew and Rakha, Emad and Maishman, Tom and Eccles, Diana M. and Pharoah, Paul D. P.},
  date = {2017-12},
  journaltitle = {Breast Cancer Research},
  shortjournal = {Breast Cancer Res},
  volume = {19},
  number = {1},
  pages = {58},
  issn = {1465-542X},
  doi = {10/gbhgpq},
  abstract = {Background: PREDICT is a breast cancer prognostic and treatment benefit model implemented online. The overall fit of the model has been good in multiple independent case series, but PREDICT has been shown to underestimate breast cancer specific mortality in women diagnosed under the age of 40. Another limitation is the use of discrete categories for tumour size and node status resulting in ‘step’ changes in risk estimates on moving between categories. We have refitted the PREDICT prognostic model using the original cohort of cases from East Anglia with updated survival time in order to take into account age at diagnosis and to smooth out the survival function for tumour size and node status. Methods: Multivariable Cox regression models were used to fit separate models for ER negative and ER positive disease. Continuous variables were fitted using fractional polynomials and a smoothed baseline hazard was obtained by regressing the baseline cumulative hazard for each patients against time using fractional polynomials. The fit of the prognostic models were then tested in three independent data sets that had also been used to validate the original version of PREDICT. Results: In the model fitting data, after adjusting for other prognostic variables, there is an increase in risk of breast cancer specific mortality in younger and older patients with ER positive disease, with a substantial increase in risk for women diagnosed before the age of 35. In ER negative disease the risk increases slightly with age. The association between breast cancer specific mortality and both tumour size and number of positive nodes was non-linear with a more marked increase in risk with increasing size and increasing number of nodes in ER positive disease. The overall calibration and discrimination of the new version of PREDICT (v2) was good and comparable to that of the previous version in both model development and validation data sets. However, the calibration of v2 improved over v1 in patients diagnosed under the age of 40. Conclusions: The PREDICT v2 is an improved prognostication and treatment benefit model compared with v1. The online version should continue to aid clinical decision making in women with early breast cancer.},
  langid = {english},
  annotation = {80 citations (Crossref) [2021-08-06]}
}

@article{collinsTRIPODAIStatement2024,
  title = {{{TRIPOD}}+{{AI}} Statement: Updated Guidance for Reporting Clinical Prediction Models That Use Regression or Machine Learning Methods},
  shorttitle = {{{TRIPOD}}+{{AI}} Statement},
  author = {Collins, Gary S. and Moons, Karel G. M. and Dhiman, Paula and Riley, Richard D. and Beam, Andrew L. and Calster, Ben Van and Ghassemi, Marzyeh and Liu, Xiaoxuan and Reitsma, Johannes B. and family=Smeden, given=Maarten, prefix=van, useprefix=false and Boulesteix, Anne-Laure and Camaradou, Jennifer Catherine and Celi, Leo Anthony and Denaxas, Spiros and Denniston, Alastair K. and Glocker, Ben and Golub, Robert M. and Harvey, Hugh and Heinze, Georg and Hoffman, Michael M. and Kengne, André Pascal and Lam, Emily and Lee, Naomi and Loder, Elizabeth W. and Maier-Hein, Lena and Mateen, Bilal A. and McCradden, Melissa D. and Oakden-Rayner, Lauren and Ordish, Johan and Parnell, Richard and Rose, Sherri and Singh, Karandeep and Wynants, Laure and Logullo, Patricia},
  date = {2024-04-16},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  volume = {385},
  pages = {e078378},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2023-078378},
  abstract = {{$<$}p{$>$}The TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis) statement was published in 2015 to provide the minimum reporting recommendations for studies developing or evaluating the performance of a prediction model. Methodological advances in the field of prediction have since included the widespread use of artificial intelligence (AI) powered by machine learning methods to develop prediction models. An update to the TRIPOD statement is thus needed. TRIPOD+AI provides harmonised guidance for reporting prediction model studies, irrespective of whether regression modelling or machine learning methods have been used. The new checklist supersedes the TRIPOD 2015 checklist, which should no longer be used. This article describes the development of TRIPOD+AI and presents the expanded 27 item checklist with more detailed explanation of each reporting recommendation, and the TRIPOD+AI for Abstracts checklist. TRIPOD+AI aims to promote the complete, accurate, and transparent reporting of studies that develop a prediction model or evaluate its performance. Complete reporting will facilitate study appraisal, model evaluation, and model implementation.{$<$}/p{$>$}},
  langid = {english}
}

@article{cooperEvaluationMachinelearningMethods1997,
  title = {An Evaluation of Machine-Learning Methods for Predicting Pneumonia Mortality},
  author = {Cooper, Gregory F. and Aliferis, Constantin F. and Ambrosino, Richard and Aronis, John and Buchanan, Bruce G. and Caruana, Richard and Fine, Michael J. and Glymour, Clark and Gordon, Geoffrey and Hanusa, Barbara H. and Janosky, Janine E. and Meek, Christopher and Mitchell, Tom and Richardson, Thomas and Spirtes, Peter},
  date = {1997-02-01},
  journaltitle = {Artificial Intelligence in Medicine},
  shortjournal = {Artificial Intelligence in Medicine},
  volume = {9},
  number = {2},
  pages = {107--138},
  issn = {0933-3657},
  doi = {10.1016/S0933-3657(96)00367-3},
  abstract = {This paper describes the application of eight statistical and machine-learning methods to derive computer models for predicting mortality of hospital patients with pneumonia from their findings at initial presentation. The eight models were each constructed based on 9847 patient cases and they were each evaluated on 4352 additional cases. The primary evaluation metric was the error in predicted survival as a function of the fraction of patients predicted to survive. This metric is useful in assessing a model's potential to assist a clinician in deciding whether to treat a given patient in the hospital or at home. We examined the error rates of the models when predicting that a given fraction of patients will survive. We examined survival fractions between 0.1 and 0.6. Over this range, each model's predictive error rate was within 1\% of the error rate of every other model. When predicting that approximately 30\% of the patients will survive, all the models have an error rate of less than 1.5\%. The models are distinguished more by the number of variables and parameters that they contain than by their error rates; these differences suggest which models may be the most amenable to future implementation as paper-based guidelines.},
  keywords = {Clinical databases,Computer-based prediction,Machine learning,Pneumonia}
}

@article{dahabrehCausalInferenceEffects2024,
  title = {Causal {{Inference About}} the {{Effects}} of {{Interventions From Observational Studies}} in {{Medical Journals}}},
  author = {Dahabreh, Issa J. and Bibbins-Domingo, Kirsten},
  date = {2024-06-04},
  journaltitle = {JAMA},
  shortjournal = {JAMA},
  volume = {331},
  number = {21},
  pages = {1845--1853},
  issn = {0098-7484},
  doi = {10.1001/jama.2024.7741},
  abstract = {Many medical journals, including JAMA, restrict the use of causal language to the reporting of randomized clinical trials. Although well-conducted randomized clinical trials remain the preferred approach for answering causal questions, methods for observational studies have advanced such that causal interpretations of the results of well-conducted observational studies may be possible when strong assumptions hold. Furthermore, observational studies may be the only practical source of information for answering some questions about the causal effects of medical or policy interventions, can support the study of interventions in populations and settings that reflect practice, and can help identify interventions for further experimental investigation. Identifying opportunities for the appropriate use of causal language when describing observational studies is important for communication in medical journals.A structured approach to whether and how causal language may be used when describing observational studies would enhance the communication of research goals, support the assessment of assumptions and design and analytic choices, and allow for more clear and accurate interpretation of results. Building on the extensive literature on causal inference across diverse disciplines, we suggest a framework for observational studies that aim to provide evidence about the causal effects of interventions based on 6 core questions: what is the causal question; what quantity would, if known, answer the causal question; what is the study design; what causal assumptions are being made; how can the observed data be used to answer the causal question in principle and in practice; and is a causal interpretation of the analyses tenable?Adoption of the proposed framework to identify when causal interpretation is appropriate in observational studies promises to facilitate better communication between authors, reviewers, editors, and readers. Practical implementation will require cooperation between editors, authors, and reviewers to operationalize the framework and evaluate its effect on the reporting of empirical research.}
}

@article{flanaginWhatDoesProposed2024,
  title = {What {{Does}} the {{Proposed Causal Inference Framework}} for {{Observational Studies Mean}} for {{JAMA}} and the {{JAMA Network Journals}}?},
  author = {Flanagin, Annette and Lewis, Roger J. and Muth, Christopher C. and Curfman, Gregory},
  date = {2024-06-04},
  journaltitle = {JAMA},
  shortjournal = {JAMA},
  volume = {331},
  number = {21},
  pages = {1812--1813},
  issn = {0098-7484},
  doi = {10.1001/jama.2024.8107},
  abstract = {The Special Communication “Causal Inferences About the Effects of Interventions From Observational Studies in Medical Journals,” published in this issue of JAMA, provides a rationale and framework for considering causal inference from observational studies published by medical journals. Our intent is to invite discussion of this framework, explore its application in the context of specific study designs, and actively examine how this framework could be implemented and used by authors, peer reviewers, and editors of medical journals, including JAMA and the journals of the JAMA Network. Our overarching goal is to ensure that findings from observational designs may be appropriately interpreted in thoughtful and circumspect manners and applied by readers, other researchers, and clinicians, with the ultimate goal of improving patient care and public and global health.}
}

@inproceedings{hartfordDeepIVFlexible2017,
  title = {Deep {{IV}}: {{A Flexible Approach}} for {{Counterfactual Prediction}}},
  shorttitle = {Deep {{IV}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
  date = {2017-07-17},
  pages = {1414--1423},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/hartford17a.html},
  urldate = {2021-09-18},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}

@book{hernanCausalInferenceWhat2020,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hernan, Miguel A and Robins, James M},
  date = {2020},
  langid = {english}
}

@article{hernanCWordScientificEuphemisms2018,
  title = {The {{C-Word}}: {{Scientific Euphemisms Do Not Improve Causal Inference From Observational Data}}},
  shorttitle = {The {{C-Word}}},
  author = {Hernán, Miguel A.},
  date = {2018-05},
  journaltitle = {American Journal of Public Health},
  shortjournal = {Am J Public Health},
  volume = {108},
  number = {5},
  eprint = {29565659},
  eprinttype = {pmid},
  pages = {616--619},
  issn = {0090-0036},
  doi = {10.2105/AJPH.2018.304337},
  abstract = {Causal inference is a core task of science. However, authors and editors often refrain from explicitly acknowledging the causal goal of research projects; they refer to causal effect estimates as associational estimates., This commentary argues that using the term “causal” is necessary to improve the quality of observational research., Specifically, being explicit about the causal objective of a study reduces ambiguity in the scientific question, errors in the data analysis, and excesses in the interpretation of the results.},
  pmcid = {PMC5888052}
}

@article{hildenPrognosisMedicineAnalysis1987,
  title = {Prognosis in Medicine: {{An}} Analysis of Its Meaning and Rôles},
  shorttitle = {Prognosis in Medicine},
  author = {Hilden, Jørgen and Habbema, J. Dik F.},
  date = {1987-10-01},
  journaltitle = {Theoretical Medicine},
  shortjournal = {Theor Med Bioeth},
  volume = {8},
  number = {3},
  pages = {349--365},
  issn = {1573-1200},
  doi = {10.1007/BF00489469},
  abstract = {The medical concept of prognosis is analysed into its basic constituents: patient data, medical intervention, outcome, utilities and probabilities; and sources of utility and probability values are discussed. Prognosis cannot be divorced from contemplated medical action, nor from action to be taken by the patient in response to prognostication. Regrettably, the usual decision-theoretic approach ignores this latter aspect. Elicitation of utilities, decision contemplation and prognostic counselling interweave, diagnostics playing a subsidiary role in decision-oriented clinical practice. At times the doctor has grounds for withholding information. As this is known to the patient, prognostic counselling becomes a conflict-prone and rationality-thwarting activity. The meaning of standard phrases such as “prognosis of a disease”, “the prognosis of this patient”, “the prognosis is unknown”, is examined.},
  langid = {english},
  keywords = {Clinical trial,Medical decision-making,Physician-patient relations,Professional jargon (medicine),Prognosis,Utility theory}
}

@online{InstructionsAuthorsJAMA,
  title = {Instructions for {{Authors}} | {{JAMA}} | {{JAMA Network}}},
  url = {https://jamanetwork.com/journals/jama/pages/instructions-for-authors},
  urldate = {2021-08-03}
}

@article{karmaliBloodPressureloweringTreatment2018,
  title = {Blood Pressure-Lowering Treatment Strategies Based on Cardiovascular Risk versus Blood Pressure: {{A}} Meta-Analysis of Individual Participant Data},
  shorttitle = {Blood Pressure-Lowering Treatment Strategies Based on Cardiovascular Risk versus Blood Pressure},
  author = {Karmali, Kunal N. and Lloyd-Jones, Donald M. and family=Leeuw, given=Joep, prefix=van der, useprefix=false and Jr, David C. Goff and Yusuf, Salim and Zanchetti, Alberto and Glasziou, Paul and Jackson, Rodney and Woodward, Mark and Rodgers, Anthony and Neal, Bruce C. and Berge, Eivind and Teo, Koon and Davis, Barry R. and Chalmers, John and Pepine, Carl and Rahimi, Kazem and Sundström, Johan and Collaboration, on behalf of the Blood Pressure Lowering Treatment Trialists’},
  date = {2018-03-20},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {15},
  number = {3},
  pages = {e1002538},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1002538},
  abstract = {Background Clinical practice guidelines have traditionally recommended blood pressure treatment based primarily on blood pressure thresholds. In contrast, using predicted cardiovascular risk has been advocated as a more effective strategy to guide treatment decisions for cardiovascular disease (CVD) prevention. We aimed to compare outcomes from a blood pressure-lowering treatment strategy based on predicted cardiovascular risk with one based on systolic blood pressure (SBP) level. Methods and findings We used individual participant data from the Blood Pressure Lowering Treatment Trialists’ Collaboration (BPLTTC) from 1995 to 2013. Trials randomly assigned participants to either blood pressure-lowering drugs versus placebo or more intensive versus less intensive blood pressure-lowering regimens. We estimated 5-y risk of CVD events using a multivariable Weibull model previously developed in this dataset. We compared the two strategies at specific SBP thresholds and across the spectrum of risk and blood pressure levels studied in BPLTTC trials. The primary outcome was number of CVD events avoided per persons treated. We included data from 11 trials (47,872 participants). During a median of 4.0 y of follow-up, 3,566 participants (7.5\%) experienced a major cardiovascular event. Areas under the curve comparing the two treatment strategies throughout the range of possible thresholds for CVD risk and SBP demonstrated that, on average, a greater number of CVD events would be avoided for a given number of persons treated with the CVD risk strategy compared with the SBP strategy (area under the curve 0.71 [95\% confidence interval (CI) 0.70–0.72] for the CVD risk strategy versus 0.54 [95\% CI 0.53–0.55] for the SBP strategy). Compared with treating everyone with SBP ≥ 150 mmHg, a CVD risk strategy would require treatment of 29\% (95\% CI 26\%–31\%) fewer persons to prevent the same number of events or would prevent 16\% (95\% CI 14\%–18\%) more events for the same number of persons treated. Compared with treating everyone with SBP ≥ 140 mmHg, a CVD risk strategy would require treatment of 3.8\% (95\% CI 12.5\% fewer to 7.2\% more) fewer persons to prevent the same number of events or would prevent 3.1\% (95\% CI 1.5\%–5.0\%) more events for the same number of persons treated, although the former estimate was not statistically significant. In subgroup analyses, the CVD risk strategy did not appear to be more beneficial than the SBP strategy in patients with diabetes mellitus or established CVD. Conclusions A blood pressure-lowering treatment strategy based on predicted cardiovascular risk is more effective than one based on blood pressure levels alone across a range of thresholds. These results support using cardiovascular risk assessment to guide blood pressure treatment decision-making in moderate- to high-risk individuals, particularly for primary prevention.},
  langid = {english},
  keywords = {ACE inhibitor therapy,Aged,Antihypertensive Agents,Blood pressure,Blood Pressure,Blood Pressure Determination,Cardiovascular disease risk,Cardiovascular diseases,Cardiovascular Diseases,Cardiovascular therapy,Diabetes mellitus,Female,Humans,Hypertension,Kaplan-Meier Estimate,Male,Medical risk factors,Middle Aged,Practice Guidelines as Topic,Primary Prevention,Randomized Controlled Trials as Topic,Risk Assessment,Risk Factors,Stroke,Treatment guidelines,Treatment Outcome}
}

@online{keoghPredictionInterventionsEvaluation2024,
  title = {Prediction under Interventions: Evaluation of Counterfactual Performance Using Longitudinal Observational Data},
  shorttitle = {Prediction under Interventions},
  author = {Keogh, Ruth H. and family=Geloven, given=Nan, prefix=van, useprefix=true},
  date = {2024-01-10},
  eprint = {2304.10005},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2304.10005},
  abstract = {Predictions under interventions are estimates of what a person's risk of an outcome would be if they were to follow a particular treatment strategy, given their individual characteristics. Such predictions can give important input to medical decision making. However, evaluating predictive performance of interventional predictions is challenging. Standard ways of evaluating predictive performance do not apply when using observational data, because prediction under interventions involves obtaining predictions of the outcome under conditions that are different to those that are observed for a subset of individuals in the validation dataset. This work describes methods for evaluating counterfactual performance of predictions under interventions for time-to-event outcomes. This means we aim to assess how well predictions would match the validation data if all individuals had followed the treatment strategy under which predictions are made. We focus on counterfactual performance evaluation using longitudinal observational data, and under treatment strategies that involve sustaining a particular treatment regime over time. We introduce an estimation approach using artificial censoring and inverse probability weighting which involves creating a validation dataset that mimics the treatment strategy under which predictions are made. We extend measures of calibration, discrimination (c-index and cumulative/dynamic AUCt) and overall prediction error (Brier score) to allow assessment of counterfactual performance. The methods are evaluated using a simulation study, including scenarios in which the methods should detect poor performance. Applying our methods in the context of liver transplantation shows that our procedure allows quantification of the performance of predictions supporting crucial decisions on organ allocation.},
  pubstate = {prepublished},
  keywords = {Statistics - Methodology}
}

@article{miaoIdentifyingCausalEffects2018,
  title = {Identifying Causal Effects with Proxy Variables of an Unmeasured Confounder},
  author = {Miao, Wang and Geng, Zhi and Tchetgen Tchetgen, Eric J},
  date = {2018-12-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {105},
  number = {4},
  pages = {987--993},
  issn = {0006-3444},
  doi = {10.1093/biomet/asy038},
  abstract = {We consider a causal effect that is confounded by an unobserved variable, but for which observed proxy variables of the confounder are available. We show that with at least two independent proxy variables satisfying a certain rank condition, the causal effect can be nonparametrically identified, even if the measurement error mechanism, i.e., the conditional distribution of the proxies given the confounder, may not be identified. Our result generalizes the identification strategy of Kuroki \&amp; Pearl (2014), which rests on identification of the measurement error mechanism. When only one proxy for the confounder is available, or when the required rank condition is not met, we develop a strategy for testing the null hypothesis of no causal effect.}
}

@book{pearlBookWhyNew2018,
  title = {The {{Book}} of {{Why}}: {{The New Science}} of {{Cause}} and {{Effect}}},
  shorttitle = {The {{Book}} of {{Why}}},
  author = {Pearl, Judea and Mackenzie, Dana},
  date = {2018-05-15},
  edition = {1st edition},
  publisher = {Basic Books},
  location = {New York},
  abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence "Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality -- the study of cause and effect -- on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
  isbn = {978-0-465-09760-9},
  langid = {english},
  pagetotal = {432}
}

@book{pearlCausality2009,
  title = {Causality},
  author = {Pearl, Judea},
  date = {2009-09},
  publisher = {Cambridge University Press},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
  langid = {english}
}

@unpublished{puliGeneralControlFunctions2021,
  title = {General {{Control Functions}} for {{Causal Effect Estimation}} from {{Instrumental Variables}}},
  author = {Puli, Aahlad Manas and Ranganath, Rajesh},
  date = {2021-02-02},
  eprint = {1907.03451},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.03451},
  urldate = {2021-09-18},
  abstract = {Causal effect estimation relies on separating the variation in the outcome into parts due to the treatment and due to the confounders. To achieve this separation, practitioners often use external sources of randomness that only influence the treatment called instrumental variables (IVs). We study variables constructed from treatment and IV that help estimate effects, called control functions. We characterize general control functions for effect estimation in a meta-identification result. Then, we show that structural assumptions on the treatment process allow the construction of general control functions, thereby guaranteeing identification. To construct general control functions and estimate effects, we develop the general control function method (GCFN). GCFN's first stage called variational decoupling (VDE) constructs general control functions by recovering the residual variation in the treatment given the IV. Using VDE's control function, GCFN's second stage estimates effects via regression. Further, we develop semi-supervised GCFN to construct general control functions using subsets of data that have both IV and confounders observed as supervision; this needs no structural treatment process assumptions. We evaluate GCFN on low and high dimensional simulated data and on recovering the causal effect of slave export on modern community trust.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{shalitEstimatingIndividualTreatment2017,
  title = {Estimating Individual Treatment Effect: Generalization Bounds and Algorithms},
  shorttitle = {Estimating Individual Treatment Effect},
  author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
  date = {2017-05-16},
  eprint = {1606.03976},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.03976},
  urldate = {2021-09-23},
  abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{vanamsterdamAlgorithmsActionImproving2024,
  title = {From Algorithms to Action: Improving Patient Care Requires Causality},
  shorttitle = {From Algorithms to Action},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and family=Jong, given=Pim A., prefix=de, useprefix=false and Verhoeff, Joost J. C. and Leiner, Tim and Ranganath, Rajesh},
  date = {2024},
  journaltitle = {BMC Medical Informatics and Decision Making},
  volume = {24},
  number = {1},
  doi = {10.1186/s12911-024-02513-3},
  abstract = {In cancer research there is much interest in building and validating outcome prediction models to support treatment decisions. However, because most outcome prediction models are developed and validated without regard to the causal aspects of treatment decision making, many published outcome prediction models may cause harm when used for decision making, despite being found accurate in validation studies. Guidelines on prediction model validation and the checklist for risk model endorsement by the American Joint Committee on Cancer do not protect against prediction models that are accurate during development and validation but harmful when used for decision making. We explain why this is the case and how to build and validate models that are useful for decision making.},
  langid = {english}
}

@online{vanamsterdamConditionalAverageTreatment2022,
  title = {Conditional Average Treatment Effect Estimation with Treatment Offset Models},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and Ranganath, Rajesh},
  date = {2022-04-29},
  eprint = {2204.13975},
  eprinttype = {arXiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2204.13975},
  urldate = {2022-06-29},
  abstract = {Treatment effect estimates are often available from randomized controlled trials as a single average treatment effect for a certain patient population. Estimates of the conditional average treatment effect (CATE) are more useful for individualized treatment decision making, but randomized trials are often too small to estimate the CATE. There are several examples in medical literature where the assumption of a known constant relative treatment effect (e.g. an odds-ratio) is used to estimate CATE models from large observational datasets. One approach to estimating these CATE models is by using the relative treatment effect as an offset, while estimating the covariate-specific baseline risk. Whether this is a valid approach in the presence of unobserved confounding is unknown. We demonstrate for a simple example that offset models do not recover the true CATE in the presence of unobserved confounding. We then explore the magnitude of this bias in numerical experiments. For virtually all plausible confounding magnitudes, estimating the CATE using offset models is more accurate than assuming a single absolute treatment effect whenever there is sufficient variation in the baseline risk. Next, we observe that the odds-ratios reported in randomized controlled trials are not the odds-ratios that are needed in offset models because trials often report the marginal odds-ratio. We introduce a constraint to better use marginal odds-ratios from randomized controlled trials and find that the newly introduced constrained offset models have lower bias than standard offset models. Finally, we highlight directions for future research for exploiting the assumption of a constant relative treatment effect with offset models.},
  pubstate = {prepublished},
  keywords = {Statistics - Methodology}
}

@article{vanamsterdamConditionalAverageTreatment2023,
  title = {Conditional Average Treatment Effect Estimation with Marginally Constrained Models},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and Ranganath, Rajesh},
  date = {2023-08-29},
  journaltitle = {Journal of Causal Inference},
  volume = {11},
  number = {1},
  pages = {20220027},
  issn = {2193-3685},
  doi = {10.1515/jci-2022-0027},
  abstract = {Abstract                            Treatment effect estimates are often available from randomized controlled trials as a single               average treatment effect               for a certain patient population. Estimates of the               conditional average treatment effect               (CATE) are more useful for individualized treatment decision-making, but randomized trials are often too small to estimate the CATE. Examples in medical literature make use of the               relative               treatment effect (e.g. an odds ratio) reported by randomized trials to estimate the CATE using large observational datasets. One approach to estimating these CATE models is by using the relative treatment effect as an               offset               , while estimating the covariate-specific untreated risk. We observe that the odds ratios reported in randomized controlled trials are not the odds ratios that are needed in offset models because trials often report the               marginal               odds ratio. We introduce a constraint or a regularizer to better use marginal odds ratios from randomized controlled trials and find that under the standard observational causal inference assumptions, this approach provides a consistent estimate of the CATE. Next, we show that the offset approach is not valid for CATE estimation in the presence of unobserved confounding. We study if the offset assumption and the marginal constraint lead to better approximations of the CATE relative to the alternative of using the average treatment effect estimate from the randomized trial. We empirically show that when the underlying CATE has sufficient variation, the constraint and offset approaches lead to closer approximations to the CATE.},
  langid = {english}
}

@article{vanamsterdamIndividualTreatmentEffect2022,
  title = {Individual Treatment Effect Estimation in the Presence of Unobserved Confounding Using Proxies: A Cohort Study in Stage {{III}} Non-Small Cell Lung Cancer},
  shorttitle = {Individual Treatment Effect Estimation in the Presence of Unobserved Confounding Using Proxies},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and Verhoeff, Joost J. C. and Harlianto, Netanja I. and Bartholomeus, Gijs A. and Puli, Aahlad Manas and family=Jong, given=Pim A., prefix=de, useprefix=true and Leiner, Tim and family=Lindert, given=Anne S. R., prefix=van, useprefix=true and Eijkemans, Marinus J. C. and Ranganath, Rajesh},
  date = {2022-04-07},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {5848},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-09775-9},
  abstract = {Randomized Controlled Trials (RCT) are the gold standard for estimating treatment effects but some important situations in cancer care require treatment effect estimates from observational data. We developed “Proxy based individual treatment effect modeling in cancer” (PROTECT) to estimate treatment effects from observational data when there are unobserved confounders, but proxy measurements of these confounders exist. We identified an unobserved confounder in observational cancer research: overall fitness. Proxy measurements of overall fitness exist like performance score, but the fitness as observed by the treating physician is unavailable for research. PROTECT reconstructs the distribution of the unobserved confounder based on these proxy measurements to estimate the treatment effect. PROTECT was applied to an observational cohort of 504 stage III non-small cell lung cancer (NSCLC) patients, treated with concurrent chemoradiation or sequential chemoradiation. Whereas conventional confounding adjustment methods seemed to overestimate the treatment effect, PROTECT provided credible treatment effect estimates.},
  issue = {1},
  langid = {english},
  keywords = {Lung cancer,Outcomes research,Predictive markers,Prognostic markers,Statistics}
}

@online{vanamsterdamWhenAccuratePrediction2024,
  title = {When Accurate Prediction Models Yield Harmful Self-Fulfilling Prophecies},
  author = {family=Amsterdam, given=Wouter A. C., prefix=van, useprefix=true and family=Geloven, given=Nan, prefix=van, useprefix=true and Krijthe, Jesse H. and Ranganath, Rajesh and Ciná, Giovanni},
  date = {2024-02-08},
  eprint = {2312.01210},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2312.01210},
  abstract = {Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach. Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model. Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment are useless for decision making as they made no change in the data distribution. Discussion: Our results point to the need to revise standard practices for validation, deployment and evaluation of prediction models that are used in medical decisions. Conclusion: Outcome prediction models can yield harmful self-fulfilling prophecies when used for decision making, a new perspective on prediction model development, deployment and monitoring is needed.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology}
}

@article{waldFittingStraightLines1940,
  title = {The {{Fitting}} of {{Straight Lines}} If {{Both Variables}} Are {{Subject}} to {{Error}}},
  author = {Wald, Abraham},
  date = {1940-09},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {11},
  number = {3},
  pages = {284--300},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177731868},
  abstract = {The Annals of Mathematical Statistics}
}

@article{xuPredictionCardiovascularDisease2021a,
  title = {Prediction of {{Cardiovascular Disease Risk Accounting}} for {{Future Initiation}} of {{Statin Treatment}}},
  author = {Xu, Zhe and Arnold, Matthew and Stevens, David and Kaptoge, Stephen and Pennells, Lisa and Sweeting, Michael J and Barrett, Jessica and Di Angelantonio, Emanuele and Wood, Angela M},
  date = {2021-10-01},
  journaltitle = {American Journal of Epidemiology},
  volume = {190},
  number = {10},
  pages = {2000--2014},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwab031},
  abstract = {Abstract             Cardiovascular disease (CVD) risk-prediction models are used to identify high-risk individuals and guide statin initiation. However, these models are usually derived from individuals who might initiate statins during follow-up. We present a simple approach to address statin initiation to predict “statin-naive” CVD risk. We analyzed primary care data (2004–2017) from the UK Clinical Practice Research Datalink for 1,678,727 individuals (aged 40–85 years) without CVD or statin treatment history at study entry. We derived age- and sex-specific prediction models including conventional risk factors and a time-dependent effect of statin initiation constrained to 25\% risk reduction (from trial results). We compared predictive performance and measures of public-health impact (e.g., number needed to screen to prevent 1 event) against models ignoring statin initiation. During a median follow-up of 8.9 years, 103,163 individuals developed CVD. In models accounting for (versus ignoring) statin initiation, 10-year CVD risk predictions were slightly higher; predictive performance was moderately improved. However, few individuals were reclassified to a high-risk threshold, resulting in negligible improvements in number needed to screen to prevent 1 event. In conclusion, incorporating statin effects from trial results into risk-prediction models enables statin-naive CVD risk estimation and provides moderate gains in predictive ability but had a limited impact on treatment decision-making under current guidelines in this population.},
  langid = {english}
}
