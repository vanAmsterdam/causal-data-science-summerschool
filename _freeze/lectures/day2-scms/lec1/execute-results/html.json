{
  "hash": "3417b7e75a41d4bdf968e496a3fb1940",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Causal Directed Acylic Graphs\nsubtitle: introduction\nauthor: Wouter van Amsterdam\ndate: 2024-08-06\nformat: \n    #beamer:\n        #aspectratio: 169\n        #logo: umcu_blue.png\n    #html:\n        #toc: true\n        #toc-depth: 2\n        #number-sections: true\n    revealjs:\n        toc: false\n        #theme: [default, umcu.scss]\n        incremental: true\n        width: 1920\n        height: 1080\n        logo: umcu_blue.png\n        center: true\n        include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\nexecute:\n    warning: false\n    message: false\ncategories:\n    - DAG\n    - day2\nbibliography: bibliography.bib\n---\n\n::: {.cell execute='true'}\n\n:::\n\n\n\n<!--\n\nQs:\n1. what knowledge of probability to expect? e.g. conditioning\n\n--->\n\n# Day 2 intro: Causal Directed Acyclic Graphs and Structural Causal Models\n\n## Today's lectures\n\n- introduce new framework based on\n  - causal Directed Acyclic Graphs (DAGs)\n  - Structral Causal Models (SCMs)\n- counterfactuals and Pearl's Causal Hierarchy of questions\n- lectures will follow Pearl's book Causality @pearlCausality2009, specifically chapters 3 (DAGs) and 7 (SCMs)\n\n## Causal inference frameworks\n\n### What are they for?\n\n#### Mathematical language to\n  - define *causal* quantities\n  - express *assumptions*\n  - derive how to *estimate* causal effects\n\n## Causal inference frameworks\n\n### Why learn more than one?\n\n<!--todo: add hyperlink to day 1 materials-->\n\n- On day 1 we learned about the Potential Outcomes framework\n    - Defines causal effects in terms of (averages of) *individual potential outcomes*\n    - Estimation requires assumptions of (conditional) exchangeability and positivity / overlap and consistency\n- There isn't only 1 way to think about causality, find one that '*clicks*'\n- Now we will learn another framework: *Structural Causal Models* and *causal graphs*\n    - causal relations and manipulations of *variables*\n    - Developed by different people initially - Judea Pearl, Peter Spirtes, Clark Glymour\n    - SCM approach is broader in that it can define more different types of causal questions\n- Equivalence: given the same data and assumptions, get the same estimates\n\n<!--## Lecture 1 & 2 topics {background-image=\"1920_1080.png\" background-size=\"contain\"}-->\n## Lecture 1 & 2 topics\n\n- motivating examples for DAGs\n- what are DAGs\n- causal inference with DAGs\n  - what is an intervention\n  - structures: confounding, mediation, colliders\n  - d-separation\n  - back-door criterion\n \n## practical 1\n\n  - drawing and using dags (what to condition on); daggity\n  - same data, different dags, different answers\n\n# Motivating examples\n\n## Example task: are hospital deliveries good for babies? {#sec-example-delivery}\n\n::: {.r-stack}\n\n![](figs/delivery1.png)\n\n![](figs/delivery2.png){.fragment}\n\n![](figs/delivery.png){.fragment}\n\n:::\n\n## Example task: are hospital deliveries good for babies?\n\n- You're a data scientist in a children's hospital\n- Have data on\n  - delivery location (home or hospital)\n  - neonatal outcomes (good or bad)\n  - pregnancy risk (high or low)\n- Question: do hospital deliveries result in better outcomes for babies?\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# t=0: home, t=1: hospital\n# z=0: low risk, z=1: high risk\n# y=1: good outcome\ndnames = list(location=c('home', 'hospital'), risk=c('low', 'high'))\npos_tz <- matrix(c(\n  c(0.9,  0.5), # y|t=0,z=0,1\n  c(0.95, 0.8)  # y|t=1,z=0,1\n), nrow=2, byrow=T,\ndimnames=dnames)\n\nps_tz <- matrix(c(\n  c(0.72, 0.08), # p(t=0,z=0,1)\n  c(0.02, 0.18)  # p(t=1,z=0,1)\n), nrow=2, byrow=T, dimnames=dnames)\n\n# p(t,z) under do(t)\nps_do0 <- matrix(c( \n  c(0.8, 0.2), \n  c(0.0, 0.0)  \n), nrow=2, byrow=T)\n\nps_do1 <- matrix(c( \n  c(0.0, 0.0), \n  c(0.8, 0.2) \n), nrow=2, byrow=T)\n\n\neys  <- rowSums(pos_tz * ps_tz) / rowSums(ps_tz) # E y|t\ndots <- c(sum(pos_tz * ps_do0), sum(pos_tz*ps_do1))\n\nn = 1000\n\nts  <- vector(mode=\"logical\", length=0)\nzs  <- vector(mode=\"logical\", length=0)\npy0s <- vector(mode=\"numeric\", length=0)\npy1s <- vector(mode=\"numeric\", length=0)\nys  <- vector(mode=\"logical\", length=0)\n\nntots <- n * ps_tz\n\nfor (t in c(0,1)) {\n  for (z in c(0,1)) {\n    ntz <- n *ps_tz[t+1,z+1]\n    ts  <- c(ts, rep(t, ntz))\n    zs  <- c(zs, rep(z, ntz))\n    py <- pos_tz[t+1,z+1]\n    y <- c(rep(0,\n               round(ntz * (1-py))), # <- round should not be needed here but found a bug\n           rep(1,\n               round(ntz*py))) \n    ys <- c(ys, y)\n    py0s <- c(py0s, rep(pos_tz[1, z+1], ntz))\n    py1s <- c(py1s, rep(pos_tz[2, z+1], ntz))\n  }\n}\n# ys <- ifelse(ts, y1s, y0s)\ndf <- data.table(\n  location=ts,\n  risk=zs,\n  outcome=ys,\n  py0=py0s,\n  py1=py1s)\n\n# head(df)\n\n# kable(eys, col.names=\"tips\")\n#pander::pander(ftable(eys))\n\n# pander::pander(ftable(pos_tz), emphasize.strong.rows=c(1), emphasize.strong.cols=c(1))\n\nntots <- n * ps_tz\nnys <- ntots * pos_tz\n\nstrs <- paste0(nys, \" / \", ntots, \" = \", 100*pos_tz, \"%\")\nstr_mat <- matrix(strs, ncol=2, dimnames=dnames)\n# kable(t(str_mat))\n# pander::pander(ftable(t(str_mat)), emphasize.strong.rows=c(1), emphasize.strong.cols=c(1))\n\nntotsm <- rowSums(ntots)\nnysm <- rowSums(nys)\nstrsm <- paste0(nysm, \" / \", ntotsm, \" = \", 100*eys, \"%\")\n\ntab_tots <- rbind(ntots, ntotsm)\n\n\n# tab <- df[, list(good=sum(outcome==1), bad=sum(outcome==0), frac_good=mean(outcome)), by=c(\"risk\", \"location\")]\n```\n:::\n\n\n\n. . . \n\n\n\n\n\n\n\n\n\n## Observed data\n\n|      |      | location |          |\n|------|------|---------:|---------:|\n|      |      | home     | hospital |\n| risk | low  | 648 / 720 = 90% | 19 / 20 = 95% |\n|      | high | 40 / 80 = 50% | 144 / 180 = 80% |\n\n- better outcomes for babies delivered in the hospital for *both risk groups*\n\n## Observed data\n\n|      |      | location |          |\n|------|------|---------:|---------:|\n|      |      | home     | hospital |\n| risk | low  | 648 / 720 = 90% | 19 / 20 = 95% |\n|      | high | 40 / 80 = 50% | 144 / 180 = 80% |\n|      |      |                  |                  |\n|      | *marginal* | 688 / 800 = 86% | 163 / 200 = 81.5% |\n\n- better outcomes for babies delivered in the hospital for *both risk groups*\n- but not better *marginal* ('overall')\n- how is this possible? (a.k.a. *simpsons paradox*)\n- what is the correct way to estimate the effect of delivery location?\n\n## New question: hernia\n\n- for a patient with a hernia, will they be able to walk sooner when recovering at home or when recovering in a hospital?\n\n::: {.r-stack}\n\n![](figs/delivery-locations.png){.fragment}\n\n![](figs/delivery-backpain.png){.fragment}\n\n:::\n\n## Observed data 2\n\n|      |      | location |          |\n|------|------|---------:|---------:|\n|      |      | home     | hospital |\n| bedrest | no | 648 / 720 = 90% | 19 / 20 = 95% |\n|      | yes | 40 / 80 = 50% | 144 / 180 = 80% |\n|      |      |                  |                  |\n|      | *marginal* | 688 / 800 = 86% | 163 / 200 = 81.5% |\n\n- more bed rest in hospital\n- what is the correct way to estimate the effect of location?\n\n## How to unravel this?\n\n- we got two questions with exactly the same data\n- in one example, 'stratified analysis' seemed best\n- in the other example, 'marginal analysis' seemed best\n- with *Directed Acyclic Graphs* we can make our decision\n\n## Causal Directed Acyclic Graphs\n\n### diagram that represents our assumptions on causal relations\n\n1. nodes are variables\n2. arrows (directed edges) point from cause to effect\n\n![Directed Acyclic Graph](_tikzs/dag-fire0.png){#fig-dag-fire0 .fragment}\n\n- when used to convey causal assumptions, DAGs are 'causal' DAGs\n- this is not the only use of DAGs (see [day 4](../day4-causal-predictions/lec1.html))\n\n## Making DAGs for our examples:\n\n### The pregnancy DAG\n\n![](_tikzs/dag-delivery.png){#fig-dag-delivery1 height=\"40%\"}\n\n- assumptions:\n    - women with high risk of bad neonatal outcomes (`pregnancy risk`) are referred to the hospital for delivery\n    - hospital deliveries lead to better outcomes for babies as more emergency treatments possible\n    - both `pregnancy risk` and `hospital delivery` cause `neonatal outcome`\n- the *other variable* `pregnancy risk` is a common cause of the treatment (`hospital delivery`) and the outcome (this is what's called a confounder)\n\n## Making DAGs for our examples:\n\n### The hernia DAG\n\n![](_tikzs/dag-hernia.png){#fig-dag-hernia height=\"40%\"}\n\n- assumptions:\n    - patients admitted to the hospital keep more `bed rest` than those who remain at home\n    - `bed rest` leads to lower recovery times thus less walking patients after 1 week\n- the *other variable* `bed rest` is a *mediator* between the treatment (`hospitalized`) and the outcome\n\n## Causal DAGs to the rescue\n\n- the *other variable* was:\n    - a **common cause** of the treatment and outcome in the pregnancy example\n    - a **mediator** between the treatment and the outcome in the hernia example, \n- using our background knowledge we could see *something* is different about these examples\n- next: ground this in causal theory and see implications for analysis\n\n# Some math background: probabilites and assumptions\n\n## Why math???\n\n:::{layout=\"[60,40]\"}\n\n- why not?\n- need probability for estimation\n- need conditional independence for causal inference\n- need to understand 'strength' of assumptions\n\n\n![oh no math](figs/why math.png)\n\n:::\n\n\n## Marginal, Joint and Conditional probabilites\n\nProbability statements about *random events* $A$ and $B$\n\n$A=1\\to$ patient dies; $B=1\\to$ has cancer\n\n::: {layout=\"[60,40]\" layout-valign=\"center\"}\n\n| statement | interpretation |\n|----------|-----------------------------------|\n| $P(A)$   | *marginal* probability that event $A$ occurs |\n| $P(B)$   | *marginal* probability that event $B$ occurs |\n\n::::{.r-stack}\n\n:::::{.fragment .fade-in-then-out}\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    | 5    | 5     | 10  |\n|   | has no cancer | 10   | 80    | 90  |\n|   |               | 15   | 85    | 100 |\n:::::\n\n:::::{.fragment .fade-in-then-out}\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    | | | |\n|   | has no cancer | | | |\n|   |               | 15   | 85    | 100 |\n$P(A=1) = 15 / 100$\n:::::\n\n:::::{.fragment .fade-in-then-out}\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    |     |      | 10  |\n|   | has no cancer |    |     | 90  |\n|   |               |    |     | 100 |\n$P(B=1) = 10 / 100$\n:::::\n\n::::\n\n:::\n\n## Marginal, Joint and Conditional probabilites\n\nProbability statements about *random events* $A$ and $B$:\n\n::: {layout=\"[60,40]\" layout-valign=\"center\"}\n\n| statement | interpretation |\n|----------|-----------------------------------|\n| $P(A)$   | *marginal* probability that event $A$ occurs |\n| $P(A,B)$ | *joint* probability of $A$ and $B$  |\n\n::::{.r-stack}\n\n:::::{.fragment .fade-in-then-out}\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    | 5    | 5     | 10  |\n|   | has no cancer | 10   | 80    | 90  |\n|   |               | 15   | 85    | 100 |\n:::::\n\n:::::{.fragment .fade-in-then-out}\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    | 5    | | |\n|   | has no cancer | | | |\n|   |               | | | 100 |\n\n$P(B=1,A=1) = 5 / 100$\n:::::\n\n\n::::\n\n:::\n\n## Marginal, Joint and Conditional probabilites\n\nProbability statements about *random events* $A$ and $B$:\n\n::: {layout=\"[60,40]\" layout-valign=\"center\"}\n\n| statement | interpretation |\n|----------|-----------------------------------|\n| $P(A)$   | *marginal* probability that event $A$ occurs |\n| $P(A,B)$ | *joint* probability of $A$ and $B$  |\n| $P(A|B)$ | *conditional* probability of $A$ given $B$  |\n\n::::{.r-stack}\n\n:::::{.fragment .fade-in-then-out}\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    | 5    | 5     | 10  |\n|   | has no cancer | 10   | 80    | 90  |\n|   |               | 15   | 85    | 100 |\n\n[- *marginal* $P(A=1) = 15/100$]{.non-incremental}\n\n:::::\n\n:::::{.fragment .fade-in-then-out}\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    | 5    | 5     | 10  |\n|   | has no cancer |    |     |   |\n|   |               |    |     |  |\n\n[- *marginal* $P(A=1) = 15/100$]{.non-incremental}\n\n[- *conditional* $P(A=1|B=1) = 5 / 10$]{.non-incremental}\n\n:::::\n\n::::\n\n:::\n\n## Probability rules and identities\n\n:::{layout=\"[60,40]\"}\n\n| statement | interpretation |\n|----------|-----------------------------------|\n| $P(A) = \\sum_{b} P(A,B=b)$ | marginal is sum over joint |\n\n:::::{.fragment}\n\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    | 5    |     |   |\n|   | has no cancer | 10   |     |   |\n|   |               | 15   |     | 100 |\n\n\\begin{align}\n    P(A=1) &= P(A=1,B=0) + P(A=1,B=1) \\\\\n           &= 5/100 + 10/100 \\\\\n           & = 15/100\n\\end{align}\n\n\n:::::\n\n:::\n\n## Probability rules and identities\n\n:::{layout=\"[60,40]\"}\n\n| statement | interpretation |\n|----------|-----------------------------------|\n| $P(A) = \\sum_{b} P(A,B=b)$ | marginal is sum over joint |\n| $P(A,B) = P(A|B)P(B)$ | product rule | \n\n:::::{.fragment}\n|   |               | A    |       |     |\n|---|---------------|------|-------|-----|\n|   |               | dies | lives |     |\n| B | has cancer    | 5    |     | 10 |\n|   | has no cancer |    |     |   |\n|   |               |    |     | 100 |\n\n\\begin{align}\n    P(A=1,B=1) &= P(A=1|B=1)P(B=1) \\\\\n               &= 5/10 * 10/100 \\\\\n               & = 5/100\n\\end{align}\n\n\n:::::\n\n:::\n\n## Probability rules and identities\n\n| statement | interpretation |\n|----------|-----------------------------------|\n| $P(A) = \\sum_{b} P(A,B=b)$ | marginal is sum over joint |\n| $P(A,B) = P(A|B)P(B)$ | product rule | \n| $P(A|B) = \\frac{P(A,B)}{P(B)}$ | conditional is joint over marginal (follows from product rule)| \n| $P(A|C) = \\sum_{b} P(A|B=b,C)P(B=b|C)$ | total expectation (consequence of marginal vs joint and product rule) |\n\n## Marginal and conditional independence:\n\n| statement | interpretation |\n|----------|-----------------------------------|\n| $P(A,B) = P(A)P(B)$ | (marginal) independence of $A$ and $B$| \n\n- knowing $A$ has no information on what to expect of $B$\n- If I roll a die, the result of that die ($A$) has no information on the weather in the Netherlands ($B$)\n\n## Marginal and conditional independence:\n\n| statement | interpretation |\n|----------|-----------------------------------|\n| $P(A,B) = P(A)P(B)$ | (marginal) independence of $A$ and $B$| \n| $P(A,B|C) = P(A|C)P(B|C)$ | conditional independence of $A$ and $B$ given $C$| \n\n- $C$ has all the information that is shared between $A$ and $B$\n\n## Conditional Independence in an example\n\n::: {layout=\"[70,30]\" layout-valign=\"center\"}\n\n- Charlie calls Alice and reads her script $C$, then she calls Bob and reads him the same\n- A week later we ask Alice to repeat the story Charlie told her, she remembered $A$, a noisy version of $C$\n- We ask Bob the same, he recounts $B$, a different noisy version of $C$\n- Are $A$ and $B$ independent? No! $P(A,B) \\neq P(A)P(B)$\n  - If we learn $A$ from Alice, we can get a good guess about $B$ from Bob\n- If we knew the $C$, would hearing $A$ give use more information about $B$?\n  - No, because all the shared information between $A$ and $B$ is explained by $C$, so:\n  - $P(A,B|C) = P(A|C)P(B|C)$\n- Variables can be marginally dependent but conditionally independent\n\n![ABC](figs/ABC.png){width=\"100%\"}\n\n:::\n\n## Assumption parlance {#sec-assumptions}\n\n- necessary assumption:\n  - A **must** hold for B to be true\n- sufficient assumption:\n  - B is always true when A holds\n- strong assumption:\n  - requires *strong* evidence, we'd rather not make these\n- weak assumption:\n  - requires *weak* evidence\n- strong vs weak assumption are judged on relative terms\n  - if assumption A is sufficient for B, B cannot be a stronger assumption that A\n\n# DAG definitions\n\n## DAGs convey two types of assumptions:\n\n### causal direction and conditional independence\n\n1. causal direction: what causes what?\n\n\n::: {layout-ncol=2}\n\n![DAG 1](_tikzs/dag-floor1.png){#fig-dag-floor1}\n\n![DAG 2](_tikzs/dag-floor2.png){.fragment}\n\n:::\n\n- read @fig-dag-floor1 as\n  - `sprinkler on` **may** (or may not) cause `wet floor`\n  - `wet floor` **cannot** cause `sprinkler on`\n\n## DAGs convey two types of assumptions:\n\n### causal direction and conditional independence\n\n1. conditional indepence (e.g. exclusion of influence / information)\n\n::: {layout-ncol=3}\n\n![DAG 1](_tikzs/dag-fire1){#fig-dag-fire1 height=400}\n\n![DAG 2](_tikzs/dag-fire2){#fig-dag-fire2 height=400}\n\n![DAG 3](_tikzs/dag-fire3){#fig-dag-fire3 height=400}\n\n:::\n\n- @fig-dag-fire1 says `fire` can **only** cause `wet floor` through `sprinkler on`\n- @fig-dag-fire2 says *there may be other ways through which `fire` causes `wet floor`*\n  - @fig-dag-fire2 is thus a *weaker* assumption than @fig-dag-fire1\n- @fig-dag-fire3 is also compatible with @fig-dag-fire2\n\n\n## DAGs are 'non-parametric'\n\n### They relay what variable 'listens' to what, but not in what way\n\n::: {layout=\"[30,70]\" layout-valign=\"center\"}\n\n![DAG](_tikzs/dag-nonparametric.png)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nn = 1e3\n\nf1 <- function(t, x, u) t + 0.5 * (x - pi) + u\nf2 <- function(t, x, u) t + sin(x) + u\nf3 <- function(t, x, u) t * sin(x) - (1-t) * sin(x) + u\n\ndf <- data.table(\n  x = runif(n, 0, 2*pi),\n  t = rbinom(n, 1, 0.5),\n  u = rnorm(n, 0, .1)\n)\n\ndf[, `:=`(\n  y1 = f1(t, x, u),\n  y2 = f2(t, x, u),\n  y3 = f3(t, x, u)\n)]\n\ndfm <- melt(df, measure.vars=c('y1', 'y2', 'y3'),\n            variable.name=\"f\", value.name=\"y\")\n\nggplot(dfm, aes(x=x, y=y, col=factor(t))) + \n  geom_point() + \n  facet_grid(~f)\n```\n\n::: {.cell-output-display}\n![Three datasets with the same DAG](lec1_files/figure-html/fig-dag-nonparametric-1.png){#fig-dag-nonparametric width=672}\n:::\n:::\n\n\n\n:::\n\n1. $Y = T + 0.5 (X - \\pi) + \\epsilon$ (linear)\n2. $Y = T + \\sin(X) + \\epsilon$ (non-linear additive)\n3. $Y = T * \\sin(X) - (1-T) \\sin(x) + \\epsilon$ (non-linear + interaction)\n\n## DAGs are 'non-parametric'\n\n### They relay what variable 'listens' to what, but not in what way\n\n::: {layout=\"[30,70]\" layout-valign=\"center\"}\n\n![DAG](_tikzs/dag-nonparametric.png)\n\n- this DAG says $Y$ is a function of $X,T$ and external noise $U_Y$, or:\n- $Y = f_Y(X,T,U_Y)$\n- in the [next lecture](./lec3-scms.html) we'll talk more about these 'structural equations'\n\n:::\n\n## DAGs imply a causal factorization of the joint distribution\n\n::: {layout=\"[30,70]\"}\n\n![observational data](_tikzs/dag-obs.png){#fig-obs width=50%}\n\n\\begin{align}\n    P(Y,T,Z,W) &= P(Y|T,Z,W)P(T,Z,W) \\\\\n               &\\class{fragment}{= P(Y|T,Z)P(T,Z,W)} \\\\\n               &\\class{fragment}{= P(Y|T,Z)P(T|Z,W)P(Z,W)} \\\\\n               &\\class{fragment}{= P(Y|T,Z)P(T|Z,W)P(Z)P(W)}\n\\end{align}\n\n:::\n\nIf this looks complicated: just follow the arrows\n\n## The DAG definition of an intervention {#sec-def-intervention}\n\nassume this is our DAG for a situation and we want to learn the effect $T$ has on $Y$\n\n- this is denoted $P(Y|\\text{do}(T))$\n- in the graph, [intervening]{.fg} on variable $T$ means removing all incoming arrows\n- this assumes such a *modular* intervention is possible: i.e. leave everything else unaltered\n\n::: {layout-ncol=2}\n\n![observational data](_tikzs/dag-obs.png){#fig-obs width=50%}\n\n![intervened DAG](_tikzs/dag-intervened.png){#fig-intervened .fragment width=50%}\n\n:::\n\n- which means $T$ does not *listen* to other variables anymore, but is set at a particular value, like in an experiment\n\n## Intervention as graph surgery - changed distribution\n\n::: {layout-ncol=2}\n\n![observational data](_tikzs/dag1-obs.png){#fig-obs width=50%}\n\n![intervened DAG](_tikzs/dag1-intervened.png){#fig-intervened width=50%}\n\n:::\n\n::: {layout-ncol=2}\n\n::::{.fragment}\n\\begin{align}\n      P_{\\text{obs}}(Y,T,Z) &= P(Y|T,Z)\\color{red}{P(T|Z)}P(Z) \\\\\n        P_{\\text{obs}}(Y|T) &= \\sum_{z} P(Y|T,Z=z)P(Z=z|T)\n\\end{align}\n\n::::\n\n::::{.fragment}\n\\begin{align}\n      P_{\\text{int}}(Y,T,Z) &= P(Y|T,Z)\\color{green}{P(T)}P(Z) \\\\\n        P_{\\text{int}}(Y|T) &= \\sum_{z} P(Y|T,Z=z)P(Z=z|T) \\\\\n               &\\class{fragment}{= \\sum_{z} P(Y|T,Z=z)\\color{green}{P(Z)}} \\\\\n               &\\class{fragment}{= P(Y|\\text{do}(T))}\n\\end{align}\n::::\n\n:::\n\n## Intervention as graph surgery - changed distribution\n\n::: {layout-ncol=2}\n\n:::: {#int}\n![observational data](_tikzs/dag1-obs.png){#fig-obs width=50%}\n\n$$P_{\\text{obs}}(Y|T) = \\sum_{z} P(Y|T,Z=z)\\color{red}{P(Z=z|T)}$$\n::::\n\n:::: {#int}\n![intervened DAG](_tikzs/dag1-intervened.png){#fig-intervened width=50%}\n\n$$P_{\\text{int}}(Y|T) = \\sum_{z} P(Y|T,Z=z)\\color{green}{P(Z=z)}$$ {#eq-estimand}\n::::\n\n:::\n\n- in $P_{\\text{obs}}$, $P(Z|T) \\color{red}{\\neq} P(Z)$\n- in $P_{\\text{int}}$, $P(Z|T) \\color{green}{\\neq} P(Z)$\n- thereby $P_{\\text{obs}}(Y|T) \\neq P_{\\text{int}}(P(Y|T)) = P(Y|\\text{do}(T))$\n- **seeing is not doing**\n- **looking at @eq-estimand, we can compute these from $P_{\\text{obs}}$!** (this is what is called an *estimand*)\n\n## Back to example 1\n\n::: {layout-ncol=2}\n\n![DAG](_tikzs/dag-delivery.png)\n\n\n|      |      | location |          |\n|------|------|---------:|---------:|\n|      |      | home     | hospital |\n| risk | low  | 648 / 720 = 90% | 19 / 20 = 95% |\n|      | high | 40 / 80 = 50% | 144 / 180 = 80% |\n|      |      |                  |                  |\n|      | *marginal* | 688 / 800 = 86% | 163 / 200 = 81.5% |\n\n:::\n\n- estimand: $P(\\text{outcome}|\\text{do}(\\text{location})) = \\sum_{\\text{risk}} P(\\text{outcome}|\\text{location},\\text{risk})P(\\text{risk})$\n- $P(\\text{risk}=\\text{low})=74\\%$\n\n::: {.fragment}\n\\begin{align}\n P(\\text{outcome}|\\text{do}(\\text{hospital})) &= 95 * 0.74 + 80 * 0.26 = 91.1\\% \\\\\n     P(\\text{outcome}|\\text{do}(\\text{home})) &= 90 * 0.74 + 50 * 0.26 = 79.6\\%\n\\end{align}\n:::\n\n- **conclusion**: sending all deliveries to the hospital leads to better outcomes\n\n## Back to example 2\n\n::: {layout-ncol=2}\n\n![DAG](_tikzs/dag-hernia)\n\n- removing all arrows going in to $T$ results in the same DAG\n- so $P(Y|T) = P(Y|\\text{do}(T))$\n- i.e. use the marginals\n\n:::\n\n\n\n## The gist of observational causal inference\n\nis to take data we have to make inferences about data from a different distribution (i.e. the intervened-on distribution)\n\n:::{.columns}\n\n::::{.column width=\"20%\"}\n\n![observational data: data we have](_tikzs/dag-obs.png){#fig-obs}\n\n![intervened DAG: what we want to know](_tikzs/dag-intervened.png){#fig-intervened}\n\n::::\n\n::::{.column width=\"80%\"}\n\n- causal inference frameworks provide a language to express assumptions\n- based on these assumptions, the framework tell us whether such an inference is possible\n    - this is often referred to as *is the effect identified*\n- and provide formula(s) for how to do so based on the observed data distribution (*estimand(s)*)\n- (one could say this is essentially assumption-based extrapolation, some researchers think this entire enterprise is anti-scientific)\n- not yet said: *how* to do statistical inference to estimate the estimand (much can still go wrong here)\n  - can also be part of identification, see [the following lecture on SCMs](./lec3-scms.html)\n\n::::\n\n:::\n\n# DAG rules\n\n## Basic DAG patterns: chain\n\n:::{layout=\"[30,70]\"}\n\n![chain / mediation](_tikzs/dag-chain.png){#fig-chain width=\"100%\"}\n\n- $M$ *mediates* effect of $X$ on $Y$\n- $X \\perp Y | M$\n- do not want to adjust for $M$ when estimating total effect of $X$ on $Y$\n\n:::\n\n## Basic DAG patterns: fork\n\n:::{layout=\"[30,70]\"}\n\n![fork / confounder](_tikzs/dag-fork.png){#fig-fork width=\"100%\"}\n\n- $Z$ *causes* both $X$ and $Y$ (common cause / confounder)\n- $X \\perp Y | Z$\n- $Z \\to X$ is a *back-door*: a path between $X$ and $Y$ that starts with an arrow into $X$\n- typically want to adjust for $Z$ (see [later @sec-backdoor])\n\n:::\n\n## Basic DAG patterns: collider\n\n:::{layout=\"[30,70]\"}\n\n![collider](_tikzs/dag-collider){#fig-collider width=\"100%\"}\n\n- $X$ and $Y$ *both cause* $Z$\n- $X \\perp Y$ (but *NOT* when conditioning on $Z$)\n- often do not want to condition on $Z$ as this induces a correlation between $X$ and $Y$\n\n:::\n\n## Collider bias - Tinder\n\n:::{layout=\"[30,70]\"}\n\n::: {#fig-collider}\n\n![collider](_tikzs/dag-collider){#fig-collider width=\"100%\"}\n\n\\begin{align}\n    \\text{intelligent} &\\sim U[0,1] \\\\\n    \\text{attractive}  &\\sim U[0,1] \\\\\n    \\text{on tinder}   &= I_{\\text{intelligent} + \\text{attractive} < 1}\n\\end{align}\n\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintelligent = runif(n)\nattractive = runif(n)\ns = intelligent + attractive > 1.\ndf <- data.frame(intelligent, attractive, s)\ndf$s <- factor(df$s)\n\nggplot(df, aes(x=intelligent, y=attractive)) +\n  geom_point(aes(alpha=s, col=s)) + \n  # stat_smooth(method='lm', col='black', se=F, formula=y~x) +\n  # stat_smooth(method='lm', se=F, formula=y~x, col='black') +\n  stat_smooth(data=df[df$s==T,], method='lm', col='darkred', formula=y~x, se=F) + \n  scale_color_manual(\n    breaks=c(F,T), values=c('gray', 'darkgreen'),\n    labels=c('on tinder', 'not on tinder'),\n    guide=guide_legend(\n      title=''\n      )\n    ) + \n  scale_alpha_manual(breaks=c(F,T), values=c(0.15, 0.2), guide=NULL) + \n  theme(axis.ticks=element_blank(), axis.text=element_blank()) + \n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](lec1_files/figure-html/fig-tinder-1.png){#fig-tinder width=1152}\n:::\n:::\n\n\n\n:::\n\n## Conditioning on a collider creates dependence of its parents \n\n- may not be too visible: doing an analysis in a selected subgroup is a form of ('invisible') conditioning)\n- e.g. when selecting only patients in the hospital\n    - being admitted to the hospital is a collider (has many different causes, e.g. traffic accident or fever)\n    - usually only one of these is the reason for hospital admission\n    - the causes for hospital admission now seem anti-correlated\n- collider conditioning *might* be an explanation for the *obsesity paradox* (i.e. obesity is correlated with better outcomes in diverse medical settings) [e.g. @banackObesityParadoxMay2017]\n\n<!--- nice paper on this: Crash course in good and bad controls [@cinelliCrashCourseGood2022]-->\n\n## When life gets complicated / real\n\n![Bogie, James; Fleming, Michael; Cullen, Breda; Mackay, Daniel; Pell, Jill P. (2021). Full directed acyclic graph.. PLOS ONE. Figure. https://doi.org/10.1371/journal.pone.0249258.s003](figs/bigdag.jpg)\n\n## d-separation (directional-separation)\n\n![paths](_tikzs/path1.png)\n\n- a *path* is a set of nodes connected by edges ($x \\ldots y$)\n- a *directed-path* is a path with a constant direction ($x \\dots t$)\n- an *unblocked-path* is a path without a collider ($t \\ldots y$)\n- a *blocked-path* is a path with a collider ($s,t, u$)\n- *d(irectional)-separation* of $x,y$ means there is no unblocked path between them\n\n## d-separation when conditioning\n\n![paths with conditioning variables $r$, $t$](_tikzs/path1z.png)\n\n- conditioning on variable: \n  - when variable is a collider: *opens a path* ($t$ opens $s,t,u$ etc.)\n  - otherwise: *blocks a path* (e.g. $r$ blocks $x,r,s$)\n- conditioning *set* $Z=\\{r,t\\}$: set of conditioning variables\n\n## The back-door criterion and adjustment {#sec-backdoor}\n\n**Definition 3.3.1 (Back-Door) (for pairs of variables)**\n\nA set of variables $Z$ satisfies the *back-door* criterion relative to an ordered pair of variables $(X,Y)$ in a DAG if:\n\n1. no node in $Z$ is a descendant of $X$ *(e.g. mediatiors)*\n2. $Z$ blocks every path between $X$ and $Y$ that contains an arrow into $X$\n\n:::{.fragment}\n\n**Theorem 3.2.2 (Back-Door Adjustment)**\n\nIf a set of variables $Z$ satisfies the back-door criterion relative to $(X,Y)$, then the causal effect of $X$ on $Y$ is identifiable and is given by the formula\n\n$$P(y|\\text{do}(x)) = \\sum_z P(y|x,z)P(z)$$ {#eq-backdooradjustment}\n\n:::\n\n## Did we see this equation before?\n\n- Yes! When computing the effect of hospital deliveries on neonatal outcomes @eq-estimand\n- DAGs tell us what to adjust for\n- automatic algorithms tell use whether an estimand exists and what it is\n\n## How about positivity\n\n- backdoor adjustment with $z$ requires computing $P(y|x,z)$\n- by the product rule:\n\n  $$P(y|x,z) = \\frac{P(y,x,z)}{P(x,z)}$$ \n- this division is only defined when $P(x,z) > 0$\n- which is the same as the positivity assumption from Day 1 in Potential Outcomes\n\n\n\n## References\n\n",
    "supporting": [
      "lec1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}