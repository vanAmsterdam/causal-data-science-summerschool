{
  "hash": "301e33e5c873673ddf414571d063e58e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Adjustment Sets and Approaches - and limitations / critiques\nauthor: Wouter van Amsterdam\ndate: 2024-08-06\nformat: \n    #latex:\n        #documentclass: beamer\n        #lof: true\n        #lot: true\n    #html:\n        #toc: true\n        #toc-depth: 2\n        #number-sections: true\n    revealjs:\n        theme: umcu.scss\n        incremental: true\n        width: 1600\n        height: 900\n        logo: umcu_blue.png\n        center: true\nexecute:\n    warning: false\n    message: false\ncategories:\n    - SCM\n    - DAG\n    - day2\nbibliography: bibliography.bib\n---\n\n::: {.cell execute='true'}\n\n:::\n\n\n\n\n<!--\n\nQs:\n\n1. what knowledge of probability to expect? e.g. conditioning\n\n--->\n\n# Adjustment sets and approaches\n\n## How to find adjustment sets?\n\n- adjustment sets:\n    - the back-door criterion states that any set $Z$ that blocks all backdoor paths from $X$ to $Y$ is a sufficient adjustment set for causal effect estimation of $P(Y|\\text{do}(X))$ using the backdoor formula.\n    - how do we find these sufficient sets?\n    - what if there are multiple?\n- adjustment: how to do this?\n  - stratification\n  - what is regression adjustment?\n  - T-learner vs S-learner\n\n## Valid adjustment sets\n\n::: {layout=\"[70,30]\"}\n\n- in general:\n    - $PA_T$ (the direct parents of treatment $T$: $Z_1$) are a valid adjustment set\n    - $PA_Y$ (the direct parents of outcome $Y$: $Z_2$) are a valid adjustment set\n- in this case:\n    - $W$ is a valid adjustment set\n![dag](_tikzs/house.png)\n\n:::\n\n## Valid adjustment sets: picking one\n\n- websites like [dagitty.net](https://dagitty.net) and [causalfusion.net](https://causalfusion.net) provide user-friendly interfaces for creating and exporting DAGs, in addition:\n  - valid adjustment sets (if they exist)\n  - testable conditional indepdencies\n\n---\n\n::: {.r-stack}\n\n![daggity.net](figs/daggity.png)\n\n![causalfusion.net](figs/causalfusion.png){.fragment}\n\n:::\n\n# How to do adjustment\n\n## What not to do\n\n1. do univariable pre-screening against outcome (and / or treatment)\n  - this should maybe never be done\n  - especially not in the context of causal inference\n\n## Adjustment formula\n\n$$P(y|\\text{do}(x)) = \\sum_z P(y|x,z)P(z)$$\n\n- entails summing over all possible values of $Z$\n- say $Z$ is 5 categorical variables with each 3 categories, this means $4^5=1024$ estimates of:\n  - $P(y|x,z)$ for each value of $x$\n- what if $Z$ is continuous?\n- in practice, researchers rely on smoothness assumptions (e.g. regression) to estimate $P(Y|x,z)$ with a parametric model\n- this assumption *can* be based on substantive causal knowledge, but often seems inspired rather pragmatism or necessity\n- misspecification of this estimator leads to biased results (even if you know all the confounders)\n\n## Target queries\n\n- up to now we've worked exclusively with $P(y|\\text{do}(t))$: the probability of observing outcome $y$ when setting treatment $T$ to $t$\n- this is not typically what is of most interest, say there are two treatment options $T \\in \\{0,1\\}$ (control and 'treatment')\n  1. *average treatment effect*\n     $$\\text{ATE} = E[y|\\text{do}(t=1)] - E[y|\\text{do}(t=0)]$$\n  2. *conditional average treatment effect*\n     $$\\text{CATE} = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]$$\n  3. *prediction-under-intervention* $P(y|\\text{do}(t),w)$ (more on this on [day 4](../day4-causal-predictions/lec1.html))\n- these can be computed from $P(y|\\text{do}(t),w)$\n\n## The simplest case: linear regression\n\n- assume the following structural causal model ($z$ is confounder, $u$ is exogenous noise):\n  $$f_y(t,z,u) = \\beta_t t + \\beta_z z + \\beta_u u$$\n- then:\n  \\begin{align}\n    \\text{ATE} &= E[Y|\\text{do}(t=1)] - E[Y|\\text{do}(t=0)] \\\\\n               &\\class{fragment}{= E_{z,u}[\\beta_t * 1+ \\beta_z z + \\beta_u u] - E_{z,u}[\\beta_t * 0 + \\beta_z z + \\beta_u u]} \\\\\n               &\\class{fragment}{= \\beta_t + E_{z,u}[\\beta_z z + \\beta_u u] - E_{z,u}[\\beta_z z + \\beta_u u]} \\\\\n               &\\class{fragment}{= \\beta_t}\n  \\end{align}\n\n- i.e. the ATE collapses to the the regression parameter for $t$ in a linear regression model of $y$ on $t,z$\n\n## General estimators for the ATE and the CATE (meta-learners) {#sec-metalearners}\n\n- denote $\\tau(w) = E[y|\\text{do}(t=1),w] - E[y|\\text{do}(t=0),w]$\n- T-learner: model $T=0$ and $T=1$ separately (e.g. regression separetely for treated and untreated):\n  \\begin{align}\n    \\mu_0(w) &= E[Y|\\text{do}(T=0),W=w] \\\\\n    \\mu_1(w) &= E[Y|\\text{do}(T=1),W=w] \\\\\n    \\tau(w)  &= \\mu_1(w) - \\mu_0(w)\n  \\end{align}\n- S-learner: use $T$ as just another feature (assuming $W$ is a sufficient set)\n  \\begin{align}\n    \\mu(t,w) &= E[Y|T=t,W=w] \\\\\n    \\tau(w)  &= \\mu(1,w) - \\mu(0,w)\n  \\end{align}\n- (many other variants combinations): this is a whole literature)\n\n## Intuitive way-pointers:\n\n- where does the complexity come from? \n  a. variance in outcome under control: $E[y|\\text{do}(T=0),w]$\n  b. variance CATE: $\\tau(w)$ (in statistics: *interaction* between treatment and covariate)\n\n## Where does the variance come from? {.non-incremental}\n\n::: {layout=\"[30,70]\" layout-valign=\"center\"}\n\n![DAG](_tikzs/dag-nonparametric.png)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nn = 1e3\n\nf1 <- function(t, x, u) t + 0.5 * (x - pi) + u\nf2 <- function(t, x, u) t + sin(x) + u\nf3 <- function(t, x, u) t * sin(x) - (1-t) * sin(x) + u\n\ndf <- data.table(\n  x = runif(n, 0, 2*pi),\n  t = rbinom(n, 1, 0.5),\n  u = rnorm(n, 0, .1)\n)\n\ndf[, `:=`(\n  y1 = f1(t, x, u),\n  y2 = f2(t, x, u),\n  y3 = f3(t, x, u)\n)]\n\ndfm <- melt(df, measure.vars=c('y1', 'y2', 'y3'),\n            variable.name=\"f\", value.name=\"y\")\n\nggplot(dfm, aes(x=x, y=y, col=factor(t))) + \n  geom_point() + \n  facet_grid(~f)\n```\n\n::: {.cell-output-display}\n![Three datasets with the same DAG](lec4_files/figure-html/fig-dag-nonparametric-1.png){#fig-dag-nonparametric width=672}\n:::\n:::\n\n\n\n:::\n\n1. $Y = T + 0.5 (X - \\pi) + \\epsilon$ (linear)\n2. $Y = T + \\sin(X) + \\epsilon$ (non-linear additive)\n3. $Y = T * \\sin(X) - (1-T) \\sin(x) + \\epsilon$ (non-linear + interaction)\n\n \n# Limitations of DAGs and SCMs\n\n## Making DAGs\n\n- how do you get a DAG? up to now we assumed we had one\n- based on prior evidence, expert knowledge\n- \"no causes in, no causes out\"\n\n## A003024: The death of DAGs?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na003024 <- c(1,1,3,25,543,29281,3781503,1138779265,\n783702329343,1213442454842881,4175098976430598143,\n31603459396418917607425,\n521939651343829405020504063,\n18676600744432035186664816926721,\n1439428141044398334941790719839535103)\na_years <- a003024 / 31556952 # n seconds in gregorian calendar year\na_days <- a_years * 365.2425 # n seconds in gregorian calendar year\nadf <- data.table(days=a_days, years=a_years, seconds=a003024)\nadf[, hours:=days*24]\nadf[, n_nodes:=.I-1]\nadf[, an_hour:=hours > 1.]\nadf[, a_day:=days > 1.]\nadf[, a_year:=years > 1.]\nadf[, human_species:=years > 3e5]\nadf[, age_universe:=years > 13.787e9]\nadf[n_nodes>1, list(n_nodes, n_dags=seconds, an_hour, a_day, a_year, human_species, age_universe)]\n```\n:::\n\n\n\nThe number of possible DAGs grows super-exponentially in the number of nodes\n\n| n_nodes | n_dags              | time at 1 sec / DAG |\n|---------|---------------------|---------------------|\n| 1       | 1                   |                     |\n| 2       | 3                   |                     |\n| 3       | 25                  |                     |\n| 4       | 543                 |                     |\n| 5       | 29281               | [> an hour]{.fragment}           |\n| 6       | 3781503             | [> a day]{.fragment}             |\n| 7       | 1138779265          | [> a year]{.fragment}            |\n| 8       | 783702329343        |                     |\n| 9       | 1213442454842881    | [> human species]{.fragment}     |\n| 10      | 4175098976430598143 | [> age of universe]{.fragment}   |\n\n## Do we need to consider all DAGs?\n\n- a single sufficient set suffices\n- adjusting for all direct causes of the treatment or all direct causes of the outcome are always sufficent sets\n- can we judge these without specifying all covariate-covariate relationships?\n- potential approach:\n  - put all potential confounders in a cluster [e.g @anandCausalEffectIdentification2023]\n  - ignore covariate-covariate relationships in that cluster\n  - what happens when (partial) missing data?\n\n# SCM vs potential outcomes\n\n- definition of causal effect\n  - PO: averages of individual potential outcomes\n  - SCM: submodel or mutilated DAG\n- both require positivity \n- d-separation implies conditional independence (exchangeability)\n\n## References\n\n",
    "supporting": [
      "lec4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}